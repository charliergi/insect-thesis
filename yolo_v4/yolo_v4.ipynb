{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLT YOLO v4 example usecase\n",
    "\n",
    "This notebook shows an example usecase of YOLO v4 object detection using Transfer Learning Toolkit.\n",
    "\n",
    "0. [Set up env variables](#head-0)\n",
    "1. [Prepare dataset and pre-trained model](#head-1) <br>\n",
    "    1.1 [Download pre-trained model](#head-1-1) <br>\n",
    "2. [Provide training specification](#head-2)\n",
    "3. [Run TLT training](#head-3)\n",
    "4. [Evaluate trained models](#head-4)\n",
    "5. [Prune trained models](#head-5)\n",
    "6. [Retrain pruned models](#head-6)\n",
    "7. [Evaluate retrained model](#head-7)\n",
    "8. [Visualize inferences](#head-8)\n",
    "9. [Deploy](#head-9)\n",
    "10. [Verify deployed model](#head-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables <a class=\"anchor\" id=\"head-0\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please replace the variable with your key.\n",
      "env: GPU_INDEX=0\n",
      "env: KEY=tlt\n",
      "env: USER_EXPERIMENT_DIR=/workspace/tlt-experiments/insect-thesis\n",
      "env: PRETRAINED_DIR=/workspace/tlt-experiments/insect-thesis/data/faster_rcnn\n",
      "env: DATA_DOWNLOAD_DIR=/workspace/tlt-experiments/insect-thesis/data/kitti/final-test\n",
      "env: SPECS_DIR=./specs\n"
     ]
    }
   ],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "print(\"Please replace the variable with your key.\")\n",
    "%env GPU_INDEX=0\n",
    "%env KEY=tlt\n",
    "%set_env USER_EXPERIMENT_DIR=/workspace/tlt-experiments/insect-thesis\n",
    "%set_env PRETRAINED_DIR=/workspace/tlt-experiments/insect-thesis/data/faster_rcnn\n",
    "%set_env DATA_DOWNLOAD_DIR=/workspace/tlt-experiments/insect-thesis/data/kitti/final-test\n",
    "%env SPECS_DIR=./specs\n",
    "!mkdir -p $DATA_DOWNLOAD_DIR/yolo_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using the KITTI detection dataset for the tutorial. To find more details please visit\n",
    " http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d. Please download the KITTI detection images (http://www.cvlibs.net/download.php?file=data_object_image_2.zip) and labels (http://www.cvlibs.net/download.php?file=data_object_label_2.zip) to $DATA_DOWNLOAD_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset is present\n",
    "!mkdir -p $DATA_DOWNLOAD_DIR\n",
    "!if [ ! -f $DATA_DOWNLOAD_DIR/data_object_image_2.zip ]; then echo 'Image zip file not found, please download.'; else echo 'Found Image zip file.';fi\n",
    "!if [ ! -f $DATA_DOWNLOAD_DIR/data_object_label_2.zip ]; then echo 'Label zip file not found, please download.'; else echo 'Found Labels zip file.';fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack \n",
    "!unzip -u $DATA_DOWNLOAD_DIR/data_object_image_2.zip -d $DATA_DOWNLOAD_DIR\n",
    "!unzip -u $DATA_DOWNLOAD_DIR/data_object_label_2.zip -d $DATA_DOWNLOAD_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "!ls -l $DATA_DOWNLOAD_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 11611 samples in KITTI training dataset\r\n",
      "10450 for train and 1161 for val\r\n"
     ]
    }
   ],
   "source": [
    "# Generate val dataset out of training dataset\n",
    "!python3.6 generate_val_dataset.py --input_image_dir=$DATA_DOWNLOAD_DIR/kitti/final-test/train/images \\\n",
    "                                   --input_label_dir=$DATA_DOWNLOAD_DIR/kitti/final-test/train/labels \\\n",
    "                                   --output_dir=$DATA_DOWNLOAD_DIR/kitti/final-test/val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if you have your own dataset already in a volume (or folder), you can mount the volume on `DATA_DOWNLOAD_DIR` (or create a soft link). Below shows an example:\n",
    "```bash\n",
    "# if your dataset is in /dev/sdc1\n",
    "mount /dev/sdc1 $DATA_DOWNLOAD_DIR\n",
    "\n",
    "# if your dataset is in folder /var/dataset\n",
    "ln -sf /var/dataset $DATA_DOWNLOAD_DIR\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Start optimization iteration: 1\n",
      "Start optimization iteration: 11\n",
      "Start optimization iteration: 21\n",
      "Start optimization iteration: 31\n",
      "Start optimization iteration: 41\n",
      "Start optimization iteration: 51\n",
      "Start optimization iteration: 61\n",
      "Start optimization iteration: 71\n",
      "Start optimization iteration: 81\n",
      "Start optimization iteration: 91\n",
      "Start optimization iteration: 101\n",
      "Start optimization iteration: 111\n",
      "Start optimization iteration: 121\n",
      "Start optimization iteration: 131\n",
      "Start optimization iteration: 141\n",
      "Start optimization iteration: 151\n",
      "Start optimization iteration: 161\n",
      "Please use following anchor sizes in YOLO config:\n",
      "(56.00, 66.00)\n",
      "(131.00, 147.00)\n",
      "(223.00, 203.00)\n",
      "(170.91, 293.00)\n",
      "(327.96, 250.89)\n",
      "(239.27, 367.87)\n",
      "(316.00, 406.06)\n",
      "(406.47, 324.00)\n",
      "(431.00, 433.10)\n"
     ]
    }
   ],
   "source": [
    "# If you use your own dataset, you will need to run the code below to generate the best anchor shape\n",
    "\n",
    "!yolo_v4 kmeans -l $DATA_DOWNLOAD_DIR/kitti/final-test/train/labels \\\n",
    "                 -i $DATA_DOWNLOAD_DIR/kitti/final-test/train/images \\\n",
    "                 -n 9 \\\n",
    "                 -x 512 \\\n",
    "                 -y 512\n",
    "\n",
    "# The anchor shape generated by this script is sorted. Write the first 3 into small_anchor_shape in the config\n",
    "# file. Write middle 3 into mid_anchor_shape. Write last 3 into big_anchor_shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download pre-trained model <a class=\"anchor\" id=\"head-1-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tlt_pretrained_object_detection:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $USER_EXPERIMENT_DIR/pretrained_resnet18/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tlt_pretrained_object_detection:resnet18 --dest $USER_EXPERIMENT_DIR/pretrained_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $USER_EXPERIMENT_DIR/pretrained_resnet18/tlt_pretrained_object_detection_vresnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Provide training specification <a class=\"anchor\" id=\"head-2\"></a>\n",
    "* Augmentation parameters for on-the-fly data augmentation\n",
    "* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc.\n",
    "* Whether to use quantization aware training (QAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide pretrained model path\n",
    "!sed -i 's,EXPERIMENT_DIR,'\"$USER_EXPERIMENT_DIR\"',' $SPECS_DIR/yolo_v4_train_resnet18_kitti.txt\n",
    "\n",
    "# To enable QAT training on sample spec file, uncomment following lines\n",
    "# !sed -i \"s/enable_qat: false/enable_qat: true/g\" $SPECS_DIR/yolo_train_resnet18_kitti.txt\n",
    "# !sed -i \"s/enable_qat: false/enable_qat: true/g\" $SPECS_DIR/yolo_retrain_resnet18_kitti.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the sample spec file disables QAT training. You can force non-QAT training by running lines below\n",
    "# !sed -i \"s/enable_qat: true/enable_qat: false/g\" $SPECS_DIR/yolo_train_resnet18_kitti.txt\n",
    "# !sed -i \"s/enable_qat: true/enable_qat: false/g\" $SPECS_DIR/yolo_retrain_resnet18_kitti.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed: 42\r\n",
      "yolov4_config {\r\n",
      "  big_anchor_shape: \"[(316.00, 406.06),(406.47, 324.00),(431.00, 433.10)]\"\r\n",
      "  mid_anchor_shape: \"[(170.91, 293.00),(327.96, 250.89),(239.27, 367.87)]\"\r\n",
      "  small_anchor_shape: \"[(56.00, 66.00),(131.00, 147.00),(223.00, 203.00)]\"\r\n",
      "  box_matching_iou: 0.25\r\n",
      "  arch: \"resnet\"\r\n",
      "  nlayers: 34\r\n",
      "  arch_conv_blocks: 2\r\n",
      "  loss_loc_weight: 0.8\r\n",
      "  loss_neg_obj_weights: 100.0\r\n",
      "  loss_class_weights: 0.5\r\n",
      "  label_smoothing: 0.0\r\n",
      "  big_grid_xy_extend: 0.05\r\n",
      "  mid_grid_xy_extend: 0.1\r\n",
      "  small_grid_xy_extend: 0.2\r\n",
      "  freeze_bn: false\r\n",
      "  #freeze_blocks: 0\r\n",
      "  force_relu: false\r\n",
      "}\r\n",
      "training_config {\r\n",
      "  batch_size_per_gpu: 4\r\n",
      "  num_epochs: 20\r\n",
      "  enable_qat: false\r\n",
      "  checkpoint_interval: 1\r\n",
      "  learning_rate {\r\n",
      "    soft_start_cosine_annealing_schedule {\r\n",
      "      min_learning_rate: 1e-7\r\n",
      "      max_learning_rate: 1e-4\r\n",
      "      soft_start: 0.3\r\n",
      "    }\r\n",
      "  }\r\n",
      "  regularizer {\r\n",
      "    type: L1\r\n",
      "    weight: 3e-5\r\n",
      "  }\r\n",
      "  optimizer {\r\n",
      "    adam {\r\n",
      "      epsilon: 1e-7\r\n",
      "      beta1: 0.9\r\n",
      "      beta2: 0.999\r\n",
      "      amsgrad: false\r\n",
      "    }\r\n",
      "  }\r\n",
      "  pretrain_model_path: \"/workspace/tlt-experiments/insect-thesis/data/faster_rcnn/resnet_34.hdf5\"\r\n",
      "  #resume_model_path: \"/workspace/tlt-experiments/insect-thesis/experiment_dir_unpruned/weights/yolov4_resnet34_epoch_010.tlt\"\r\n",
      "  \r\n",
      "}\r\n",
      "eval_config {\r\n",
      "  average_precision_mode: SAMPLE\r\n",
      "  batch_size: 1\r\n",
      "  matching_iou_threshold: 0.5\r\n",
      "}\r\n",
      "nms_config {\r\n",
      "  confidence_threshold: 0.001\r\n",
      "  clustering_iou_threshold: 0.5\r\n",
      "  top_k: 200\r\n",
      "}\r\n",
      "augmentation_config {\r\n",
      "  hue: 0.1\r\n",
      "  saturation: 1.5\r\n",
      "  exposure:1.5\r\n",
      "  vertical_flip:0\r\n",
      "  horizontal_flip: 0.5\r\n",
      "  jitter: 0.3\r\n",
      "  output_width: 512\r\n",
      "  output_height: 512\r\n",
      "  randomize_input_shape_period: 0\r\n",
      "  mosaic_prob: 0.5\r\n",
      "  mosaic_min_ratio:0.2\r\n",
      "}\r\n",
      "dataset_config {\r\n",
      "  data_sources: {\r\n",
      "      label_directory_path: \"/workspace/tlt-experiments/insect-thesis/data/kitti/final-test/train/labels\"\r\n",
      "      image_directory_path: \"/workspace/tlt-experiments/insect-thesis/data/kitti/final-test/train/images\"\r\n",
      "  }\r\n",
      "  include_difficult_in_training: true\r\n",
      "  target_class_mapping {\r\n",
      "    key: 'noctuidae'\r\n",
      "    value: 'noctuidae'\r\n",
      "    }\r\n",
      "    target_class_mapping {\r\n",
      "    key: 'geometridae'\r\n",
      "    value: 'geometridae'\r\n",
      "    }\r\n",
      "    target_class_mapping {\r\n",
      "    key: 'coleoptera'\r\n",
      "    value: 'coleoptera'\r\n",
      "    }\r\n",
      "    target_class_mapping {\r\n",
      "    key: 'diptera'\r\n",
      "    value: 'diptera'\r\n",
      "    }\r\n",
      "    target_class_mapping {\r\n",
      "    key: 'odonata'\r\n",
      "    value: 'odonata'\r\n",
      "    }\r\n",
      "    target_class_mapping {\r\n",
      "    key: 'orthoptera'\r\n",
      "    value: 'orthoptera'\r\n",
      "    }\r\n",
      "    target_class_mapping {\r\n",
      "    key: 'hemiptera'\r\n",
      "    value: 'hemiptera'\r\n",
      "    }\r\n",
      "    target_class_mapping {\r\n",
      "    key: 'hymenoptera'\r\n",
      "    value: 'hymenoptera'\r\n",
      "    }\r\n",
      "    target_class_mapping {\r\n",
      "    key: 'trichoptera'\r\n",
      "    value: 'trichoptera'\r\n",
      "    }\r\n",
      "  \r\n",
      "  validation_data_sources: {\r\n",
      "      label_directory_path: \"/workspace/tlt-experiments/insect-thesis/data/kitti/final-test/val/labels\"\r\n",
      "      image_directory_path: \"/workspace/tlt-experiments/insect-thesis/data/kitti/final-test/val/images\"\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat $SPECS_DIR/yolo_v4_train_resnet34_kitti.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run TLT training <a class=\"anchor\" id=\"head-3\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "* WARNING: training will take several hours or one day to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $DATA_DOWNLOAD_DIR/yolo_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LOGS=logs-yolov4-resnet34.txt\n"
     ]
    }
   ],
   "source": [
    "%env LOGS=logs-yolov4-resnet34.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $SPECS_DIR/yolo_v4_train_resnet34_kitti.txt >> $DATA_DOWNLOAD_DIR/kitti/final-test/$LOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $DATA_DOWNLOAD_DIR/yolo_v4/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darknet_19.hdf5    resnet_10.hdf5  resnet_34.hdf5\r\n",
      "mobilenet_v2.hdf5  resnet_18.hdf5  resnet_50.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "!ls /workspace/tlt-experiments/insect-thesis/data/faster_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\n",
      "Epoch 1/20\n",
      " 100/9797 [..............................] - ETA: 47:55 - loss: 98.3373\n",
      " 200/9797 [..............................] - ETA: 35:20 - loss: 98.1847\n",
      " 300/9797 [..............................] - ETA: 30:59 - loss: 98.0083\n",
      " 400/9797 [>.............................] - ETA: 28:42 - loss: 97.7916\n",
      " 500/9797 [>.............................] - ETA: 27:15 - loss: 97.5406\n",
      " 600/9797 [>.............................] - ETA: 26:10 - loss: 97.2578\n",
      " 700/9797 [=>............................] - ETA: 25:23 - loss: 96.9453\n",
      " 800/9797 [=>............................] - ETA: 24:42 - loss: 96.6025\n",
      " 900/9797 [=>............................] - ETA: 24:07 - loss: 96.2309\n",
      "1000/9797 [==>...........................] - ETA: 23:36 - loss: 95.8276\n",
      "1100/9797 [==>...........................] - ETA: 23:08 - loss: 95.3943\n",
      "1200/9797 [==>...........................] - ETA: 22:43 - loss: 94.9125\n",
      "1300/9797 [==>...........................] - ETA: 22:18 - loss: 94.3855\n",
      "1400/9797 [===>..........................] - ETA: 21:56 - loss: 93.7986\n",
      "1500/9797 [===>..........................] - ETA: 21:34 - loss: 93.1574\n",
      "1600/9797 [===>..........................] - ETA: 21:13 - loss: 92.4597\n",
      "1700/9797 [====>.........................] - ETA: 20:52 - loss: 91.7057\n",
      "1800/9797 [====>.........................] - ETA: 20:32 - loss: 90.8981\n",
      "1900/9797 [====>.........................] - ETA: 20:13 - loss: 90.0144\n",
      "2000/9797 [=====>........................] - ETA: 19:54 - loss: 89.0412\n",
      "2100/9797 [=====>........................] - ETA: 19:35 - loss: 87.9126\n",
      "2200/9797 [=====>........................] - ETA: 19:17 - loss: 86.6141\n",
      "2300/9797 [======>.......................] - ETA: 18:59 - loss: 85.2073\n",
      "2400/9797 [======>.......................] - ETA: 18:42 - loss: 83.7752\n",
      "2500/9797 [======>.......................] - ETA: 18:25 - loss: 82.3444\n",
      "2600/9797 [======>.......................] - ETA: 18:08 - loss: 80.9345\n",
      "2700/9797 [=======>......................] - ETA: 17:51 - loss: 79.5678\n",
      "2800/9797 [=======>......................] - ETA: 17:34 - loss: 78.2528\n",
      "2900/9797 [=======>......................] - ETA: 17:18 - loss: 76.9833\n",
      "3000/9797 [========>.....................] - ETA: 17:01 - loss: 75.7574\n",
      "3100/9797 [========>.....................] - ETA: 16:45 - loss: 74.5828\n",
      "3200/9797 [========>.....................] - ETA: 16:29 - loss: 73.4482\n",
      "3300/9797 [=========>....................] - ETA: 16:13 - loss: 72.3579\n",
      "3400/9797 [=========>....................] - ETA: 15:57 - loss: 71.3071\n",
      "3500/9797 [=========>....................] - ETA: 15:41 - loss: 70.2987\n",
      "3600/9797 [==========>...................] - ETA: 15:25 - loss: 69.3261\n",
      "3700/9797 [==========>...................] - ETA: 15:09 - loss: 68.3878\n",
      "3800/9797 [==========>...................] - ETA: 14:53 - loss: 67.4814\n",
      "3900/9797 [==========>...................] - ETA: 14:38 - loss: 66.6083\n",
      "4000/9797 [===========>..................] - ETA: 14:22 - loss: 65.7636\n",
      "4100/9797 [===========>..................] - ETA: 14:07 - loss: 64.9475\n",
      "4200/9797 [===========>..................] - ETA: 13:51 - loss: 64.1585\n",
      "4300/9797 [============>.................] - ETA: 13:36 - loss: 63.3933\n",
      "4400/9797 [============>.................] - ETA: 13:20 - loss: 62.6530\n",
      "4500/9797 [============>.................] - ETA: 13:05 - loss: 61.9368\n",
      "4600/9797 [=============>................] - ETA: 12:50 - loss: 61.2416\n",
      "4700/9797 [=============>................] - ETA: 12:34 - loss: 60.5664\n",
      "4800/9797 [=============>................] - ETA: 12:19 - loss: 59.9093\n",
      "4900/9797 [==============>...............] - ETA: 12:04 - loss: 59.2719\n",
      "5000/9797 [==============>...............] - ETA: 11:49 - loss: 58.6522\n",
      "5100/9797 [==============>...............] - ETA: 11:34 - loss: 58.0485\n",
      "5200/9797 [==============>...............] - ETA: 11:18 - loss: 57.4601\n",
      "5300/9797 [===============>..............] - ETA: 11:03 - loss: 56.8880\n",
      "5400/9797 [===============>..............] - ETA: 10:48 - loss: 56.3314\n",
      "5500/9797 [===============>..............] - ETA: 10:33 - loss: 55.7878\n",
      "5600/9797 [================>.............] - ETA: 10:18 - loss: 55.2585\n",
      "5700/9797 [================>.............] - ETA: 10:03 - loss: 54.7411\n",
      "5800/9797 [================>.............] - ETA: 9:48 - loss: 54.2353\n",
      "5900/9797 [=================>............] - ETA: 9:33 - loss: 53.7415\n",
      "6000/9797 [=================>............] - ETA: 9:18 - loss: 53.2599\n",
      "6100/9797 [=================>............] - ETA: 9:03 - loss: 52.7881\n",
      "6200/9797 [=================>............] - ETA: 8:48 - loss: 52.3268\n",
      "6300/9797 [==================>...........] - ETA: 8:33 - loss: 51.8761\n",
      "6400/9797 [==================>...........] - ETA: 8:19 - loss: 51.4344\n",
      "6500/9797 [==================>...........] - ETA: 8:04 - loss: 51.0028\n",
      "6600/9797 [===================>..........] - ETA: 7:49 - loss: 50.5797\n",
      "6700/9797 [===================>..........] - ETA: 7:34 - loss: 50.1643\n",
      "6800/9797 [===================>..........] - ETA: 7:19 - loss: 49.7583\n",
      "6900/9797 [====================>.........] - ETA: 7:04 - loss: 49.3614\n",
      "7000/9797 [====================>.........] - ETA: 6:50 - loss: 48.9715\n",
      "7100/9797 [====================>.........] - ETA: 6:35 - loss: 48.5894\n",
      "7200/9797 [=====================>........] - ETA: 6:20 - loss: 48.2142\n",
      "7300/9797 [=====================>........] - ETA: 6:05 - loss: 47.8471\n",
      "7400/9797 [=====================>........] - ETA: 5:51 - loss: 47.4855\n",
      "7500/9797 [=====================>........] - ETA: 5:36 - loss: 47.1318\n",
      "7600/9797 [======================>.......] - ETA: 5:21 - loss: 46.7839\n",
      "7700/9797 [======================>.......] - ETA: 5:06 - loss: 46.4424\n",
      "7800/9797 [======================>.......] - ETA: 4:52 - loss: 46.1078\n",
      "7900/9797 [=======================>......] - ETA: 4:37 - loss: 45.7781\n",
      "8000/9797 [=======================>......] - ETA: 4:22 - loss: 45.4551\n",
      "8100/9797 [=======================>......] - ETA: 4:08 - loss: 45.1363\n",
      "8200/9797 [========================>.....] - ETA: 3:53 - loss: 44.8234\n",
      "8300/9797 [========================>.....] - ETA: 3:38 - loss: 44.5160\n",
      "8400/9797 [========================>.....] - ETA: 3:24 - loss: 44.2139\n",
      "8500/9797 [=========================>....] - ETA: 3:09 - loss: 43.9171\n",
      "8600/9797 [=========================>....] - ETA: 2:54 - loss: 43.6252\n",
      "8700/9797 [=========================>....] - ETA: 2:40 - loss: 43.3383\n",
      "8800/9797 [=========================>....] - ETA: 2:25 - loss: 43.0562\n",
      "8900/9797 [==========================>...] - ETA: 2:10 - loss: 42.7785\n",
      "9000/9797 [==========================>...] - ETA: 1:56 - loss: 42.5056\n",
      "9100/9797 [==========================>...] - ETA: 1:41 - loss: 42.2377\n",
      "9200/9797 [===========================>..] - ETA: 1:27 - loss: 41.9733\n",
      "9300/9797 [===========================>..] - ETA: 1:12 - loss: 41.7138\n",
      "9400/9797 [===========================>..] - ETA: 57s - loss: 41.4574\n",
      "9500/9797 [============================>.] - ETA: 43s - loss: 41.2055\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 40.9576\n",
      "9700/9797 [============================>.] - ETA: 14s - loss: 40.7135\n",
      "Epoch 00001: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_001.tlt\n",
      "coleoptera    AP    0.28965\n",
      "diptera       AP    0.25492\n",
      "geometridae   AP    0.56967\n",
      "hemiptera     AP    0.38153\n",
      "hymenoptera   AP    0.28093\n",
      "noctuidae     AP    0.52177\n",
      "odonata       AP    0.55519\n",
      "orthoptera    AP    0.479\n",
      "trichoptera   AP    0.6042\n",
      "              mAP   0.43743\n",
      "Validation loss: 5.086988793582504\n",
      "Epoch 2/20\n",
      " 100/9797 [..............................] - ETA: 23:15 - loss: 17.0309\n",
      " 200/9797 [..............................] - ETA: 22:55 - loss: 16.9989\n",
      " 300/9797 [..............................] - ETA: 22:39 - loss: 16.9591\n",
      " 400/9797 [>.............................] - ETA: 22:25 - loss: 16.9160\n",
      " 500/9797 [>.............................] - ETA: 22:11 - loss: 16.8614\n",
      " 600/9797 [>.............................] - ETA: 21:57 - loss: 16.8013\n",
      " 700/9797 [=>............................] - ETA: 21:43 - loss: 16.7555\n",
      " 800/9797 [=>............................] - ETA: 21:29 - loss: 16.7060\n",
      " 900/9797 [=>............................] - ETA: 21:14 - loss: 16.6589\n",
      "1000/9797 [==>...........................] - ETA: 21:00 - loss: 16.6109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100/9797 [==>...........................] - ETA: 20:46 - loss: 16.5649\n",
      "1200/9797 [==>...........................] - ETA: 20:31 - loss: 16.5220\n",
      "1300/9797 [==>...........................] - ETA: 20:17 - loss: 16.4766\n",
      "1400/9797 [===>..........................] - ETA: 20:02 - loss: 16.4328\n",
      "1500/9797 [===>..........................] - ETA: 19:48 - loss: 16.3932\n",
      "1600/9797 [===>..........................] - ETA: 19:33 - loss: 16.3515\n",
      "1700/9797 [====>.........................] - ETA: 19:19 - loss: 16.3111\n",
      "1800/9797 [====>.........................] - ETA: 19:04 - loss: 16.2716\n",
      "1900/9797 [====>.........................] - ETA: 18:50 - loss: 16.2333\n",
      "2000/9797 [=====>........................] - ETA: 18:36 - loss: 16.1972\n",
      "2100/9797 [=====>........................] - ETA: 18:22 - loss: 16.1574\n",
      "2200/9797 [=====>........................] - ETA: 18:07 - loss: 16.1176\n",
      "2300/9797 [======>.......................] - ETA: 17:53 - loss: 16.0796\n",
      "2400/9797 [======>.......................] - ETA: 17:39 - loss: 16.0400\n",
      "2500/9797 [======>.......................] - ETA: 17:24 - loss: 16.0078\n",
      "2600/9797 [======>.......................] - ETA: 17:10 - loss: 15.9733\n",
      "2700/9797 [=======>......................] - ETA: 16:56 - loss: 15.9386\n",
      "2800/9797 [=======>......................] - ETA: 16:42 - loss: 15.9035\n",
      "2900/9797 [=======>......................] - ETA: 16:27 - loss: 15.8705\n",
      "3000/9797 [========>.....................] - ETA: 16:13 - loss: 15.8385\n",
      "3100/9797 [========>.....................] - ETA: 15:59 - loss: 15.8028\n",
      "3200/9797 [========>.....................] - ETA: 15:45 - loss: 15.7687\n",
      "3300/9797 [=========>....................] - ETA: 15:30 - loss: 15.7347\n",
      "3400/9797 [=========>....................] - ETA: 15:16 - loss: 15.7028\n",
      "3500/9797 [=========>....................] - ETA: 15:02 - loss: 15.6673\n",
      "3600/9797 [==========>...................] - ETA: 14:47 - loss: 15.6353\n",
      "3700/9797 [==========>...................] - ETA: 14:33 - loss: 15.6071\n",
      "3800/9797 [==========>...................] - ETA: 14:19 - loss: 15.5758\n",
      "3900/9797 [==========>...................] - ETA: 14:04 - loss: 15.5441\n",
      "4000/9797 [===========>..................] - ETA: 13:50 - loss: 15.5128\n",
      "4100/9797 [===========>..................] - ETA: 13:36 - loss: 15.4807\n",
      "4200/9797 [===========>..................] - ETA: 13:21 - loss: 15.4514\n",
      "4300/9797 [============>.................] - ETA: 13:07 - loss: 15.4215\n",
      "4400/9797 [============>.................] - ETA: 12:53 - loss: 15.3928\n",
      "4500/9797 [============>.................] - ETA: 12:38 - loss: 15.3647\n",
      "4600/9797 [=============>................] - ETA: 12:24 - loss: 15.3354\n",
      "4700/9797 [=============>................] - ETA: 12:10 - loss: 15.3076\n",
      "4800/9797 [=============>................] - ETA: 11:55 - loss: 15.2808\n",
      "4900/9797 [==============>...............] - ETA: 11:41 - loss: 15.2529\n",
      "5000/9797 [==============>...............] - ETA: 11:27 - loss: 15.2256\n",
      "5100/9797 [==============>...............] - ETA: 11:12 - loss: 15.1975\n",
      "5200/9797 [==============>...............] - ETA: 10:58 - loss: 15.1707\n",
      "5300/9797 [===============>..............] - ETA: 10:44 - loss: 15.1433\n",
      "5400/9797 [===============>..............] - ETA: 10:29 - loss: 15.1177\n",
      "5500/9797 [===============>..............] - ETA: 10:15 - loss: 15.0908\n",
      "5600/9797 [================>.............] - ETA: 10:01 - loss: 15.0654\n",
      "5700/9797 [================>.............] - ETA: 9:46 - loss: 15.0406\n",
      "5800/9797 [================>.............] - ETA: 9:32 - loss: 15.0155\n",
      "5900/9797 [=================>............] - ETA: 9:18 - loss: 14.9916\n",
      "6000/9797 [=================>............] - ETA: 9:04 - loss: 14.9660\n",
      "6100/9797 [=================>............] - ETA: 8:49 - loss: 14.9419\n",
      "6200/9797 [=================>............] - ETA: 8:35 - loss: 14.9174\n",
      "6300/9797 [==================>...........] - ETA: 8:21 - loss: 14.8945\n",
      "6400/9797 [==================>...........] - ETA: 8:06 - loss: 14.8707\n",
      "6500/9797 [==================>...........] - ETA: 7:52 - loss: 14.8448\n",
      "6600/9797 [===================>..........] - ETA: 7:38 - loss: 14.8195\n",
      "6700/9797 [===================>..........] - ETA: 7:23 - loss: 14.7958\n",
      "6800/9797 [===================>..........] - ETA: 7:09 - loss: 14.7724\n",
      "6900/9797 [====================>.........] - ETA: 6:55 - loss: 14.7498\n",
      "7000/9797 [====================>.........] - ETA: 6:40 - loss: 14.7267\n",
      "7100/9797 [====================>.........] - ETA: 6:26 - loss: 14.7042\n",
      "7200/9797 [=====================>........] - ETA: 6:12 - loss: 14.6815\n",
      "7300/9797 [=====================>........] - ETA: 5:57 - loss: 14.6583\n",
      "7400/9797 [=====================>........] - ETA: 5:43 - loss: 14.6362\n",
      "7500/9797 [=====================>........] - ETA: 5:29 - loss: 14.6132\n",
      "7600/9797 [======================>.......] - ETA: 5:14 - loss: 14.5911\n",
      "7700/9797 [======================>.......] - ETA: 5:00 - loss: 14.5699\n",
      "7800/9797 [======================>.......] - ETA: 4:46 - loss: 14.5472\n",
      "7900/9797 [=======================>......] - ETA: 4:31 - loss: 14.5259\n",
      "8000/9797 [=======================>......] - ETA: 4:17 - loss: 14.5054\n",
      "8100/9797 [=======================>......] - ETA: 4:03 - loss: 14.4843\n",
      "8200/9797 [========================>.....] - ETA: 3:48 - loss: 14.4628\n",
      "8300/9797 [========================>.....] - ETA: 3:34 - loss: 14.4416\n",
      "8400/9797 [========================>.....] - ETA: 3:20 - loss: 14.4201\n",
      "8500/9797 [=========================>....] - ETA: 3:05 - loss: 14.3978\n",
      "8600/9797 [=========================>....] - ETA: 2:51 - loss: 14.3764\n",
      "8700/9797 [=========================>....] - ETA: 2:37 - loss: 14.3557\n",
      "8800/9797 [=========================>....] - ETA: 2:22 - loss: 14.3351\n",
      "8900/9797 [==========================>...] - ETA: 2:08 - loss: 14.3149\n",
      "9000/9797 [==========================>...] - ETA: 1:54 - loss: 14.2943\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 14.2738\n",
      "9200/9797 [===========================>..] - ETA: 1:25 - loss: 14.2543\n",
      "9300/9797 [===========================>..] - ETA: 1:11 - loss: 14.2340\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 14.2144\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 14.1942\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 14.1737\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 14.1535\n",
      "Epoch 00002: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_002.tlt\n",
      "coleoptera    AP    0.61981\n",
      "diptera       AP    0.62226\n",
      "geometridae   AP    0.80607\n",
      "hemiptera     AP    0.65858\n",
      "hymenoptera   AP    0.59026\n",
      "noctuidae     AP    0.81423\n",
      "odonata       AP    0.6767\n",
      "orthoptera    AP    0.72763\n",
      "trichoptera   AP    0.69571\n",
      "              mAP   0.69014\n",
      "Validation loss: 2.5013627186930942\n",
      "Epoch 3/20\n",
      " 100/9797 [..............................] - ETA: 23:03 - loss: 12.1573\n",
      " 200/9797 [..............................] - ETA: 22:46 - loss: 12.1392\n",
      " 300/9797 [..............................] - ETA: 22:32 - loss: 12.1124\n",
      " 400/9797 [>.............................] - ETA: 22:18 - loss: 12.1012\n",
      " 500/9797 [>.............................] - ETA: 22:04 - loss: 12.0927\n",
      " 600/9797 [>.............................] - ETA: 21:49 - loss: 12.0825\n",
      " 700/9797 [=>............................] - ETA: 21:35 - loss: 12.0603\n",
      " 800/9797 [=>............................] - ETA: 21:21 - loss: 12.0388\n",
      " 900/9797 [=>............................] - ETA: 21:07 - loss: 12.0155\n",
      "1000/9797 [==>...........................] - ETA: 20:53 - loss: 11.9964\n",
      "1100/9797 [==>...........................] - ETA: 20:39 - loss: 11.9873\n",
      "1200/9797 [==>...........................] - ETA: 20:26 - loss: 11.9751\n",
      "1300/9797 [==>...........................] - ETA: 20:12 - loss: 11.9544\n",
      "1400/9797 [===>..........................] - ETA: 19:57 - loss: 11.9404\n",
      "1500/9797 [===>..........................] - ETA: 19:43 - loss: 11.9263\n",
      "1600/9797 [===>..........................] - ETA: 19:29 - loss: 11.9109\n",
      "1700/9797 [====>.........................] - ETA: 19:15 - loss: 11.8944\n",
      "1800/9797 [====>.........................] - ETA: 19:01 - loss: 11.8819\n",
      "1900/9797 [====>.........................] - ETA: 18:47 - loss: 11.8706\n",
      "2000/9797 [=====>........................] - ETA: 18:33 - loss: 11.8560\n",
      "2100/9797 [=====>........................] - ETA: 18:19 - loss: 11.8389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/9797 [=====>........................] - ETA: 18:05 - loss: 11.8239\n",
      "2300/9797 [======>.......................] - ETA: 17:50 - loss: 11.8055\n",
      "2400/9797 [======>.......................] - ETA: 17:36 - loss: 11.7909\n",
      "2500/9797 [======>.......................] - ETA: 17:21 - loss: 11.7773\n",
      "2600/9797 [======>.......................] - ETA: 17:07 - loss: 11.7629\n",
      "2700/9797 [=======>......................] - ETA: 16:53 - loss: 11.7521\n",
      "2800/9797 [=======>......................] - ETA: 16:39 - loss: 11.7394\n",
      "2900/9797 [=======>......................] - ETA: 16:25 - loss: 11.7224\n",
      "3000/9797 [========>.....................] - ETA: 16:10 - loss: 11.7071\n",
      "3100/9797 [========>.....................] - ETA: 15:56 - loss: 11.6933\n",
      "3200/9797 [========>.....................] - ETA: 15:42 - loss: 11.6778\n",
      "3300/9797 [=========>....................] - ETA: 15:27 - loss: 11.6625\n",
      "3400/9797 [=========>....................] - ETA: 15:13 - loss: 11.6509\n",
      "3500/9797 [=========>....................] - ETA: 14:59 - loss: 11.6356\n",
      "3600/9797 [==========>...................] - ETA: 14:45 - loss: 11.6194\n",
      "3700/9797 [==========>...................] - ETA: 14:30 - loss: 11.6055\n",
      "3800/9797 [==========>...................] - ETA: 14:16 - loss: 11.5887\n",
      "3900/9797 [==========>...................] - ETA: 14:02 - loss: 11.5762\n",
      "4000/9797 [===========>..................] - ETA: 13:48 - loss: 11.5622\n",
      "4100/9797 [===========>..................] - ETA: 13:34 - loss: 11.5473\n",
      "4200/9797 [===========>..................] - ETA: 13:19 - loss: 11.5346\n",
      "4300/9797 [============>.................] - ETA: 13:05 - loss: 11.5219\n",
      "4400/9797 [============>.................] - ETA: 12:51 - loss: 11.5071\n",
      "4500/9797 [============>.................] - ETA: 12:36 - loss: 11.4934\n",
      "4600/9797 [=============>................] - ETA: 12:22 - loss: 11.4806\n",
      "4700/9797 [=============>................] - ETA: 12:08 - loss: 11.4667\n",
      "4800/9797 [=============>................] - ETA: 11:54 - loss: 11.4533\n",
      "4900/9797 [==============>...............] - ETA: 11:39 - loss: 11.4376\n",
      "5000/9797 [==============>...............] - ETA: 11:25 - loss: 11.4253\n",
      "5100/9797 [==============>...............] - ETA: 11:11 - loss: 11.4136\n",
      "5200/9797 [==============>...............] - ETA: 10:56 - loss: 11.4011\n",
      "5300/9797 [===============>..............] - ETA: 10:42 - loss: 11.3875\n",
      "5400/9797 [===============>..............] - ETA: 10:28 - loss: 11.3742\n",
      "5500/9797 [===============>..............] - ETA: 10:14 - loss: 11.3601\n",
      "5600/9797 [================>.............] - ETA: 9:59 - loss: 11.3459\n",
      "5700/9797 [================>.............] - ETA: 9:45 - loss: 11.3330\n",
      "5800/9797 [================>.............] - ETA: 9:31 - loss: 11.3186\n",
      "5900/9797 [=================>............] - ETA: 9:16 - loss: 11.3056\n",
      "6000/9797 [=================>............] - ETA: 9:02 - loss: 11.2927\n",
      "6100/9797 [=================>............] - ETA: 8:48 - loss: 11.2786\n",
      "6200/9797 [=================>............] - ETA: 8:34 - loss: 11.2651\n",
      "6300/9797 [==================>...........] - ETA: 8:19 - loss: 11.2526\n",
      "6400/9797 [==================>...........] - ETA: 8:05 - loss: 11.2404\n",
      "6500/9797 [==================>...........] - ETA: 7:51 - loss: 11.2280\n",
      "6600/9797 [===================>..........] - ETA: 7:36 - loss: 11.2157\n",
      "6700/9797 [===================>..........] - ETA: 7:22 - loss: 11.2021\n",
      "6800/9797 [===================>..........] - ETA: 7:08 - loss: 11.1873\n",
      "6900/9797 [====================>.........] - ETA: 6:54 - loss: 11.1731\n",
      "7000/9797 [====================>.........] - ETA: 6:39 - loss: 11.1595\n",
      "7100/9797 [====================>.........] - ETA: 6:25 - loss: 11.1477\n",
      "7200/9797 [=====================>........] - ETA: 6:11 - loss: 11.1338\n",
      "7300/9797 [=====================>........] - ETA: 5:56 - loss: 11.1222\n",
      "7400/9797 [=====================>........] - ETA: 5:42 - loss: 11.1102\n",
      "7500/9797 [=====================>........] - ETA: 5:28 - loss: 11.0979\n",
      "7600/9797 [======================>.......] - ETA: 5:14 - loss: 11.0863\n",
      "7700/9797 [======================>.......] - ETA: 4:59 - loss: 11.0722\n",
      "7800/9797 [======================>.......] - ETA: 4:45 - loss: 11.0590\n",
      "7900/9797 [=======================>......] - ETA: 4:31 - loss: 11.0452\n",
      "8000/9797 [=======================>......] - ETA: 4:16 - loss: 11.0328\n",
      "8100/9797 [=======================>......] - ETA: 4:02 - loss: 11.0189\n",
      "8200/9797 [========================>.....] - ETA: 3:48 - loss: 11.0068\n",
      "8300/9797 [========================>.....] - ETA: 3:33 - loss: 10.9946\n",
      "8400/9797 [========================>.....] - ETA: 3:19 - loss: 10.9828\n",
      "8500/9797 [=========================>....] - ETA: 3:05 - loss: 10.9701\n",
      "8600/9797 [=========================>....] - ETA: 2:51 - loss: 10.9566\n",
      "8700/9797 [=========================>....] - ETA: 2:36 - loss: 10.9435\n",
      "8800/9797 [=========================>....] - ETA: 2:22 - loss: 10.9302\n",
      "8900/9797 [==========================>...] - ETA: 2:08 - loss: 10.9168\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 10.9042\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 10.8926\n",
      "9200/9797 [===========================>..] - ETA: 1:25 - loss: 10.8806\n",
      "9300/9797 [===========================>..] - ETA: 1:11 - loss: 10.8674\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 10.8542\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 10.8417\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 10.8303\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 10.8182\n",
      "Epoch 00003: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_003.tlt\n",
      "coleoptera    AP    0.70428\n",
      "diptera       AP    0.63326\n",
      "geometridae   AP    0.65924\n",
      "hemiptera     AP    0.71086\n",
      "hymenoptera   AP    0.64561\n",
      "noctuidae     AP    0.63252\n",
      "odonata       AP    0.75309\n",
      "orthoptera    AP    0.82116\n",
      "trichoptera   AP    0.85342\n",
      "              mAP   0.7126\n",
      "Validation loss: 2.3414843148959577\n",
      "Epoch 4/20\n",
      " 100/9797 [..............................] - ETA: 23:05 - loss: 9.4901\n",
      " 200/9797 [..............................] - ETA: 22:52 - loss: 9.5317\n",
      " 300/9797 [..............................] - ETA: 22:38 - loss: 9.5296\n",
      " 400/9797 [>.............................] - ETA: 22:23 - loss: 9.5343\n",
      " 500/9797 [>.............................] - ETA: 22:07 - loss: 9.5152\n",
      " 600/9797 [>.............................] - ETA: 21:53 - loss: 9.5127\n",
      " 700/9797 [=>............................] - ETA: 21:37 - loss: 9.4894\n",
      " 800/9797 [=>............................] - ETA: 21:22 - loss: 9.4801\n",
      " 900/9797 [=>............................] - ETA: 21:08 - loss: 9.4673\n",
      "1000/9797 [==>...........................] - ETA: 20:53 - loss: 9.4607\n",
      "1100/9797 [==>...........................] - ETA: 20:39 - loss: 9.4490\n",
      "1200/9797 [==>...........................] - ETA: 20:25 - loss: 9.4476\n",
      "1300/9797 [==>...........................] - ETA: 20:12 - loss: 9.4338\n",
      "1400/9797 [===>..........................] - ETA: 19:58 - loss: 9.4227\n",
      "1500/9797 [===>..........................] - ETA: 19:43 - loss: 9.4192\n",
      "1600/9797 [===>..........................] - ETA: 19:29 - loss: 9.4119\n",
      "1700/9797 [====>.........................] - ETA: 19:15 - loss: 9.3988\n",
      "1800/9797 [====>.........................] - ETA: 19:01 - loss: 9.3880\n",
      "1900/9797 [====>.........................] - ETA: 18:47 - loss: 9.3751\n",
      "2000/9797 [=====>........................] - ETA: 18:33 - loss: 9.3662\n",
      "2100/9797 [=====>........................] - ETA: 18:19 - loss: 9.3544\n",
      "2200/9797 [=====>........................] - ETA: 18:04 - loss: 9.3413\n",
      "2300/9797 [======>.......................] - ETA: 17:50 - loss: 9.3301\n",
      "2400/9797 [======>.......................] - ETA: 17:36 - loss: 9.3230\n",
      "2500/9797 [======>.......................] - ETA: 17:22 - loss: 9.3152\n",
      "2600/9797 [======>.......................] - ETA: 17:08 - loss: 9.3037\n",
      "2700/9797 [=======>......................] - ETA: 16:53 - loss: 9.2963\n",
      "2800/9797 [=======>......................] - ETA: 16:39 - loss: 9.2860\n",
      "2900/9797 [=======>......................] - ETA: 16:25 - loss: 9.2748\n",
      "3000/9797 [========>.....................] - ETA: 16:10 - loss: 9.2686\n",
      "3100/9797 [========>.....................] - ETA: 15:56 - loss: 9.2619\n",
      "3200/9797 [========>.....................] - ETA: 15:42 - loss: 9.2514\n",
      "3300/9797 [=========>....................] - ETA: 15:27 - loss: 9.2415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400/9797 [=========>....................] - ETA: 15:13 - loss: 9.2302\n",
      "3500/9797 [=========>....................] - ETA: 14:59 - loss: 9.2212\n",
      "3600/9797 [==========>...................] - ETA: 14:45 - loss: 9.2121\n",
      "3700/9797 [==========>...................] - ETA: 14:30 - loss: 9.2029\n",
      "3800/9797 [==========>...................] - ETA: 14:16 - loss: 9.1939\n",
      "3900/9797 [==========>...................] - ETA: 14:02 - loss: 9.1861\n",
      "4000/9797 [===========>..................] - ETA: 13:47 - loss: 9.1787\n",
      "4100/9797 [===========>..................] - ETA: 13:33 - loss: 9.1670\n",
      "4200/9797 [===========>..................] - ETA: 13:19 - loss: 9.1592\n",
      "4300/9797 [============>.................] - ETA: 13:05 - loss: 9.1516\n",
      "4400/9797 [============>.................] - ETA: 12:51 - loss: 9.1441\n",
      "4500/9797 [============>.................] - ETA: 12:36 - loss: 9.1348\n",
      "4600/9797 [=============>................] - ETA: 12:22 - loss: 9.1256\n",
      "4700/9797 [=============>................] - ETA: 12:08 - loss: 9.1176\n",
      "4800/9797 [=============>................] - ETA: 11:53 - loss: 9.1075\n",
      "4900/9797 [==============>...............] - ETA: 11:39 - loss: 9.0998\n",
      "5000/9797 [==============>...............] - ETA: 11:25 - loss: 9.0913\n",
      "5100/9797 [==============>...............] - ETA: 11:11 - loss: 9.0830\n",
      "5200/9797 [==============>...............] - ETA: 10:56 - loss: 9.0739\n",
      "5300/9797 [===============>..............] - ETA: 10:42 - loss: 9.0637\n",
      "5400/9797 [===============>..............] - ETA: 10:28 - loss: 9.0532\n",
      "5500/9797 [===============>..............] - ETA: 10:13 - loss: 9.0449\n",
      "5600/9797 [================>.............] - ETA: 9:59 - loss: 9.0381\n",
      "5700/9797 [================>.............] - ETA: 9:45 - loss: 9.0293\n",
      "5800/9797 [================>.............] - ETA: 9:31 - loss: 9.0214\n",
      "5900/9797 [=================>............] - ETA: 9:16 - loss: 9.0115\n",
      "6000/9797 [=================>............] - ETA: 9:02 - loss: 9.0012\n",
      "6100/9797 [=================>............] - ETA: 8:48 - loss: 8.9916\n",
      "6200/9797 [=================>............] - ETA: 8:33 - loss: 8.9820\n",
      "6300/9797 [==================>...........] - ETA: 8:19 - loss: 8.9730\n",
      "6400/9797 [==================>...........] - ETA: 8:05 - loss: 8.9640\n",
      "6500/9797 [==================>...........] - ETA: 7:51 - loss: 8.9572\n",
      "6600/9797 [===================>..........] - ETA: 7:36 - loss: 8.9497\n",
      "6700/9797 [===================>..........] - ETA: 7:22 - loss: 8.9410\n",
      "6800/9797 [===================>..........] - ETA: 7:08 - loss: 8.9322\n",
      "6900/9797 [====================>.........] - ETA: 6:54 - loss: 8.9246\n",
      "7000/9797 [====================>.........] - ETA: 6:39 - loss: 8.9151\n",
      "7100/9797 [====================>.........] - ETA: 6:25 - loss: 8.9076\n",
      "7200/9797 [=====================>........] - ETA: 6:11 - loss: 8.8988\n",
      "7300/9797 [=====================>........] - ETA: 5:56 - loss: 8.8903\n",
      "7400/9797 [=====================>........] - ETA: 5:42 - loss: 8.8813\n",
      "7500/9797 [=====================>........] - ETA: 5:28 - loss: 8.8728\n",
      "7600/9797 [======================>.......] - ETA: 5:14 - loss: 8.8645\n",
      "7700/9797 [======================>.......] - ETA: 4:59 - loss: 8.8546\n",
      "7800/9797 [======================>.......] - ETA: 4:45 - loss: 8.8452\n",
      "7900/9797 [=======================>......] - ETA: 4:31 - loss: 8.8378\n",
      "8000/9797 [=======================>......] - ETA: 4:16 - loss: 8.8303\n",
      "8100/9797 [=======================>......] - ETA: 4:02 - loss: 8.8211\n",
      "8200/9797 [========================>.....] - ETA: 3:48 - loss: 8.8116\n",
      "8300/9797 [========================>.....] - ETA: 3:33 - loss: 8.8024\n",
      "8400/9797 [========================>.....] - ETA: 3:19 - loss: 8.7932\n",
      "8500/9797 [=========================>....] - ETA: 3:05 - loss: 8.7837\n",
      "8600/9797 [=========================>....] - ETA: 2:51 - loss: 8.7762\n",
      "8700/9797 [=========================>....] - ETA: 2:36 - loss: 8.7670\n",
      "8800/9797 [=========================>....] - ETA: 2:22 - loss: 8.7576\n",
      "8900/9797 [==========================>...] - ETA: 2:08 - loss: 8.7507\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 8.7424\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 8.7344\n",
      "9200/9797 [===========================>..] - ETA: 1:25 - loss: 8.7259\n",
      "9300/9797 [===========================>..] - ETA: 1:11 - loss: 8.7177\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 8.7103\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 8.7023\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 8.6940\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 8.6855\n",
      "Epoch 00004: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_004.tlt\n",
      "coleoptera    AP    0.58418\n",
      "diptera       AP    0.70772\n",
      "geometridae   AP    0.84757\n",
      "hemiptera     AP    0.65083\n",
      "hymenoptera   AP    0.63513\n",
      "noctuidae     AP    0.85823\n",
      "odonata       AP    0.7254\n",
      "orthoptera    AP    0.76114\n",
      "trichoptera   AP    0.75351\n",
      "              mAP   0.72486\n",
      "Validation loss: 2.360789888979483\n",
      "Epoch 5/20\n",
      " 100/9797 [..............................] - ETA: 23:04 - loss: 7.9143\n",
      " 200/9797 [..............................] - ETA: 22:50 - loss: 7.8783\n",
      " 300/9797 [..............................] - ETA: 22:36 - loss: 7.8818\n",
      " 400/9797 [>.............................] - ETA: 22:22 - loss: 7.8564\n",
      " 500/9797 [>.............................] - ETA: 22:07 - loss: 7.8570\n",
      " 600/9797 [>.............................] - ETA: 21:53 - loss: 7.8649\n",
      " 700/9797 [=>............................] - ETA: 21:37 - loss: 7.8570\n",
      " 800/9797 [=>............................] - ETA: 21:24 - loss: 7.8511\n",
      " 900/9797 [=>............................] - ETA: 21:10 - loss: 7.8349\n",
      "1000/9797 [==>...........................] - ETA: 20:55 - loss: 7.8283\n",
      "1100/9797 [==>...........................] - ETA: 20:41 - loss: 7.8213\n",
      "1200/9797 [==>...........................] - ETA: 20:26 - loss: 7.8172\n",
      "1300/9797 [==>...........................] - ETA: 20:12 - loss: 7.8135\n",
      "1400/9797 [===>..........................] - ETA: 19:58 - loss: 7.8053\n",
      "1500/9797 [===>..........................] - ETA: 19:44 - loss: 7.7973\n",
      "1600/9797 [===>..........................] - ETA: 19:29 - loss: 7.7932\n",
      "1700/9797 [====>.........................] - ETA: 19:15 - loss: 7.7794\n",
      "1800/9797 [====>.........................] - ETA: 19:01 - loss: 7.7690\n",
      "1900/9797 [====>.........................] - ETA: 18:47 - loss: 7.7560\n",
      "2000/9797 [=====>........................] - ETA: 18:33 - loss: 7.7512\n",
      "2100/9797 [=====>........................] - ETA: 18:18 - loss: 7.7411\n",
      "2200/9797 [=====>........................] - ETA: 18:04 - loss: 7.7373\n",
      "2300/9797 [======>.......................] - ETA: 17:50 - loss: 7.7327\n",
      "2400/9797 [======>.......................] - ETA: 17:35 - loss: 7.7232\n",
      "2500/9797 [======>.......................] - ETA: 17:21 - loss: 7.7186\n",
      "2600/9797 [======>.......................] - ETA: 17:07 - loss: 7.7085\n",
      "2700/9797 [=======>......................] - ETA: 16:53 - loss: 7.7029\n",
      "2800/9797 [=======>......................] - ETA: 16:38 - loss: 7.6958\n",
      "2900/9797 [=======>......................] - ETA: 16:24 - loss: 7.6890\n",
      "3000/9797 [========>.....................] - ETA: 16:10 - loss: 7.6820\n",
      "3100/9797 [========>.....................] - ETA: 15:56 - loss: 7.6746\n",
      "3200/9797 [========>.....................] - ETA: 15:41 - loss: 7.6670\n",
      "3300/9797 [=========>....................] - ETA: 15:27 - loss: 7.6605\n",
      "3400/9797 [=========>....................] - ETA: 15:13 - loss: 7.6570\n",
      "3500/9797 [=========>....................] - ETA: 14:58 - loss: 7.6503\n",
      "3600/9797 [==========>...................] - ETA: 14:44 - loss: 7.6442\n",
      "3700/9797 [==========>...................] - ETA: 14:30 - loss: 7.6366\n",
      "3800/9797 [==========>...................] - ETA: 14:16 - loss: 7.6293\n",
      "3900/9797 [==========>...................] - ETA: 14:01 - loss: 7.6240\n",
      "4000/9797 [===========>..................] - ETA: 13:47 - loss: 7.6192\n",
      "4100/9797 [===========>..................] - ETA: 13:33 - loss: 7.6123\n",
      "4200/9797 [===========>..................] - ETA: 13:19 - loss: 7.6087\n",
      "4300/9797 [============>.................] - ETA: 13:04 - loss: 7.6035\n",
      "4400/9797 [============>.................] - ETA: 12:50 - loss: 7.5961\n",
      "4500/9797 [============>.................] - ETA: 12:36 - loss: 7.5918\n",
      "4600/9797 [=============>................] - ETA: 12:22 - loss: 7.5865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4700/9797 [=============>................] - ETA: 12:07 - loss: 7.5828\n",
      "4800/9797 [=============>................] - ETA: 11:53 - loss: 7.5785\n",
      "4900/9797 [==============>...............] - ETA: 11:39 - loss: 7.5760\n",
      "5000/9797 [==============>...............] - ETA: 11:25 - loss: 7.5710\n",
      "5100/9797 [==============>...............] - ETA: 11:10 - loss: 7.5656\n",
      "5200/9797 [==============>...............] - ETA: 10:56 - loss: 7.5590\n",
      "5300/9797 [===============>..............] - ETA: 10:42 - loss: 7.5535\n",
      "5400/9797 [===============>..............] - ETA: 10:28 - loss: 7.5489\n",
      "5500/9797 [===============>..............] - ETA: 10:13 - loss: 7.5415\n",
      "5600/9797 [================>.............] - ETA: 9:59 - loss: 7.5359\n",
      "5700/9797 [================>.............] - ETA: 9:45 - loss: 7.5321\n",
      "5800/9797 [================>.............] - ETA: 9:30 - loss: 7.5275\n",
      "5900/9797 [=================>............] - ETA: 9:16 - loss: 7.5213\n",
      "6000/9797 [=================>............] - ETA: 9:02 - loss: 7.5165\n",
      "6100/9797 [=================>............] - ETA: 8:47 - loss: 7.5092\n",
      "6200/9797 [=================>............] - ETA: 8:33 - loss: 7.5033\n",
      "6300/9797 [==================>...........] - ETA: 8:19 - loss: 7.4977\n",
      "6400/9797 [==================>...........] - ETA: 8:05 - loss: 7.4947\n",
      "6500/9797 [==================>...........] - ETA: 7:50 - loss: 7.4903\n",
      "6600/9797 [===================>..........] - ETA: 7:36 - loss: 7.4857\n",
      "6700/9797 [===================>..........] - ETA: 7:22 - loss: 7.4792\n",
      "6800/9797 [===================>..........] - ETA: 7:07 - loss: 7.4736\n",
      "6900/9797 [====================>.........] - ETA: 6:53 - loss: 7.4699\n",
      "7000/9797 [====================>.........] - ETA: 6:39 - loss: 7.4670\n",
      "7100/9797 [====================>.........] - ETA: 6:25 - loss: 7.4626\n",
      "7200/9797 [=====================>........] - ETA: 6:10 - loss: 7.4575\n",
      "7300/9797 [=====================>........] - ETA: 5:56 - loss: 7.4526\n",
      "7400/9797 [=====================>........] - ETA: 5:42 - loss: 7.4469\n",
      "7500/9797 [=====================>........] - ETA: 5:27 - loss: 7.4421\n",
      "7600/9797 [======================>.......] - ETA: 5:13 - loss: 7.4366\n",
      "7700/9797 [======================>.......] - ETA: 4:59 - loss: 7.4304\n",
      "7800/9797 [======================>.......] - ETA: 4:45 - loss: 7.4255\n",
      "7900/9797 [=======================>......] - ETA: 4:30 - loss: 7.4202\n",
      "8000/9797 [=======================>......] - ETA: 4:16 - loss: 7.4143\n",
      "8100/9797 [=======================>......] - ETA: 4:02 - loss: 7.4099\n",
      "8200/9797 [========================>.....] - ETA: 3:48 - loss: 7.4061\n",
      "8300/9797 [========================>.....] - ETA: 3:33 - loss: 7.4013\n",
      "8400/9797 [========================>.....] - ETA: 3:19 - loss: 7.3959\n",
      "8500/9797 [=========================>....] - ETA: 3:05 - loss: 7.3919\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 7.3865\n",
      "8700/9797 [=========================>....] - ETA: 2:36 - loss: 7.3798\n",
      "8800/9797 [=========================>....] - ETA: 2:22 - loss: 7.3743\n",
      "8900/9797 [==========================>...] - ETA: 2:08 - loss: 7.3685\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 7.3627\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 7.3572\n",
      "9200/9797 [===========================>..] - ETA: 1:25 - loss: 7.3520\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 7.3464\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 7.3417\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 7.3370\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 7.3319\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 7.3267\n",
      "Epoch 00005: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_005.tlt\n",
      "coleoptera    AP    0.66249\n",
      "diptera       AP    0.47357\n",
      "geometridae   AP    0.81302\n",
      "hemiptera     AP    0.65652\n",
      "hymenoptera   AP    0.63347\n",
      "noctuidae     AP    0.79236\n",
      "odonata       AP    0.81172\n",
      "orthoptera    AP    0.70003\n",
      "trichoptera   AP    0.79905\n",
      "              mAP   0.70469\n",
      "Validation loss: 2.439706393527393\n",
      "Epoch 6/20\n",
      " 100/9797 [..............................] - ETA: 23:02 - loss: 6.8959\n",
      " 200/9797 [..............................] - ETA: 22:44 - loss: 6.8774\n",
      " 300/9797 [..............................] - ETA: 22:29 - loss: 6.8612\n",
      " 400/9797 [>.............................] - ETA: 22:15 - loss: 6.8447\n",
      " 500/9797 [>.............................] - ETA: 22:00 - loss: 6.8051\n",
      " 600/9797 [>.............................] - ETA: 21:46 - loss: 6.8158\n",
      " 700/9797 [=>............................] - ETA: 21:33 - loss: 6.7853\n",
      " 800/9797 [=>............................] - ETA: 21:20 - loss: 6.7795\n",
      " 900/9797 [=>............................] - ETA: 21:06 - loss: 6.7745\n",
      "1000/9797 [==>...........................] - ETA: 20:52 - loss: 6.7771\n",
      "1100/9797 [==>...........................] - ETA: 20:38 - loss: 6.7742\n",
      "1200/9797 [==>...........................] - ETA: 20:24 - loss: 6.7718\n",
      "1300/9797 [==>...........................] - ETA: 20:10 - loss: 6.7692\n",
      "1400/9797 [===>..........................] - ETA: 19:56 - loss: 6.7653\n",
      "1500/9797 [===>..........................] - ETA: 19:42 - loss: 6.7756\n",
      "1600/9797 [===>..........................] - ETA: 19:28 - loss: 6.7834\n",
      "1700/9797 [====>.........................] - ETA: 19:14 - loss: 6.7795\n",
      "1800/9797 [====>.........................] - ETA: 19:00 - loss: 6.7766\n",
      "1900/9797 [====>.........................] - ETA: 18:46 - loss: 6.7758\n",
      "2000/9797 [=====>........................] - ETA: 18:31 - loss: 6.7705\n",
      "2100/9797 [=====>........................] - ETA: 18:17 - loss: 6.7706\n",
      "2200/9797 [=====>........................] - ETA: 18:03 - loss: 6.7660\n",
      "2300/9797 [======>.......................] - ETA: 17:49 - loss: 6.7584\n",
      "2400/9797 [======>.......................] - ETA: 17:34 - loss: 6.7523\n",
      "2500/9797 [======>.......................] - ETA: 17:20 - loss: 6.7526\n",
      "2600/9797 [======>.......................] - ETA: 17:06 - loss: 6.7456\n",
      "2700/9797 [=======>......................] - ETA: 16:52 - loss: 6.7430\n",
      "2800/9797 [=======>......................] - ETA: 16:37 - loss: 6.7403\n",
      "2900/9797 [=======>......................] - ETA: 16:23 - loss: 6.7351\n",
      "3000/9797 [========>.....................] - ETA: 16:09 - loss: 6.7320\n",
      "3100/9797 [========>.....................] - ETA: 15:55 - loss: 6.7271\n",
      "3200/9797 [========>.....................] - ETA: 15:41 - loss: 6.7214\n",
      "3300/9797 [=========>....................] - ETA: 15:27 - loss: 6.7213\n",
      "3400/9797 [=========>....................] - ETA: 15:13 - loss: 6.7191\n",
      "3500/9797 [=========>....................] - ETA: 14:58 - loss: 6.7160\n",
      "3600/9797 [==========>...................] - ETA: 14:44 - loss: 6.7098\n",
      "3700/9797 [==========>...................] - ETA: 14:30 - loss: 6.7040\n",
      "3800/9797 [==========>...................] - ETA: 14:15 - loss: 6.7011\n",
      "3900/9797 [==========>...................] - ETA: 14:01 - loss: 6.6976\n",
      "4000/9797 [===========>..................] - ETA: 13:47 - loss: 6.6968\n",
      "4100/9797 [===========>..................] - ETA: 13:33 - loss: 6.6918\n",
      "4200/9797 [===========>..................] - ETA: 13:19 - loss: 6.6883\n",
      "4300/9797 [============>.................] - ETA: 13:04 - loss: 6.6828\n",
      "4400/9797 [============>.................] - ETA: 12:50 - loss: 6.6814\n",
      "4500/9797 [============>.................] - ETA: 12:36 - loss: 6.6812\n",
      "4600/9797 [=============>................] - ETA: 12:22 - loss: 6.6779\n",
      "4700/9797 [=============>................] - ETA: 12:07 - loss: 6.6748\n",
      "4800/9797 [=============>................] - ETA: 11:53 - loss: 6.6755\n",
      "4900/9797 [==============>...............] - ETA: 11:39 - loss: 6.6750\n",
      "5000/9797 [==============>...............] - ETA: 11:25 - loss: 6.6715\n",
      "5100/9797 [==============>...............] - ETA: 11:10 - loss: 6.6677\n",
      "5200/9797 [==============>...............] - ETA: 10:56 - loss: 6.6668\n",
      "5300/9797 [===============>..............] - ETA: 10:42 - loss: 6.6633\n",
      "5400/9797 [===============>..............] - ETA: 10:27 - loss: 6.6587\n",
      "5500/9797 [===============>..............] - ETA: 10:13 - loss: 6.6545\n",
      "5600/9797 [================>.............] - ETA: 9:59 - loss: 6.6507\n",
      "5700/9797 [================>.............] - ETA: 9:45 - loss: 6.6450\n",
      "5800/9797 [================>.............] - ETA: 9:30 - loss: 6.6421\n",
      "5900/9797 [=================>............] - ETA: 9:16 - loss: 6.6373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/9797 [=================>............] - ETA: 9:02 - loss: 6.6322\n",
      "6100/9797 [=================>............] - ETA: 8:48 - loss: 6.6300\n",
      "6200/9797 [=================>............] - ETA: 8:33 - loss: 6.6268\n",
      "6300/9797 [==================>...........] - ETA: 8:19 - loss: 6.6235\n",
      "6400/9797 [==================>...........] - ETA: 8:05 - loss: 6.6190\n",
      "6500/9797 [==================>...........] - ETA: 7:50 - loss: 6.6170\n",
      "6600/9797 [===================>..........] - ETA: 7:36 - loss: 6.6151\n",
      "6700/9797 [===================>..........] - ETA: 7:22 - loss: 6.6127\n",
      "6800/9797 [===================>..........] - ETA: 7:08 - loss: 6.6096\n",
      "6900/9797 [====================>.........] - ETA: 6:53 - loss: 6.6063\n",
      "7000/9797 [====================>.........] - ETA: 6:39 - loss: 6.6016\n",
      "7100/9797 [====================>.........] - ETA: 6:25 - loss: 6.5982\n",
      "7200/9797 [=====================>........] - ETA: 6:10 - loss: 6.5940\n",
      "7300/9797 [=====================>........] - ETA: 5:56 - loss: 6.5893\n",
      "7400/9797 [=====================>........] - ETA: 5:42 - loss: 6.5840\n",
      "7500/9797 [=====================>........] - ETA: 5:28 - loss: 6.5817\n",
      "7600/9797 [======================>.......] - ETA: 5:13 - loss: 6.5786\n",
      "7700/9797 [======================>.......] - ETA: 4:59 - loss: 6.5741\n",
      "7800/9797 [======================>.......] - ETA: 4:45 - loss: 6.5698\n",
      "7900/9797 [=======================>......] - ETA: 4:30 - loss: 6.5655\n",
      "8000/9797 [=======================>......] - ETA: 4:16 - loss: 6.5622\n",
      "8100/9797 [=======================>......] - ETA: 4:02 - loss: 6.5602\n",
      "8200/9797 [========================>.....] - ETA: 3:48 - loss: 6.5566\n",
      "8300/9797 [========================>.....] - ETA: 3:33 - loss: 6.5547\n",
      "8400/9797 [========================>.....] - ETA: 3:19 - loss: 6.5522\n",
      "8500/9797 [=========================>....] - ETA: 3:05 - loss: 6.5497\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 6.5476\n",
      "8700/9797 [=========================>....] - ETA: 2:36 - loss: 6.5449\n",
      "8800/9797 [=========================>....] - ETA: 2:22 - loss: 6.5398\n",
      "8900/9797 [==========================>...] - ETA: 2:08 - loss: 6.5363\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 6.5336\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 6.5296\n",
      "9200/9797 [===========================>..] - ETA: 1:25 - loss: 6.5267\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 6.5239\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 6.5214\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 6.5180\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 6.5143\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 6.5107\n",
      "Epoch 00006: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_006.tlt\n",
      "coleoptera    AP    0.70285\n",
      "diptera       AP    0.7232\n",
      "geometridae   AP    0.82286\n",
      "hemiptera     AP    0.68231\n",
      "hymenoptera   AP    0.67846\n",
      "noctuidae     AP    0.84283\n",
      "odonata       AP    0.78737\n",
      "orthoptera    AP    0.63479\n",
      "trichoptera   AP    0.76\n",
      "              mAP   0.73719\n",
      "Validation loss: 2.348336247522786\n",
      "Epoch 7/20\n",
      " 100/9797 [..............................] - ETA: 23:05 - loss: 6.2310\n",
      " 200/9797 [..............................] - ETA: 22:48 - loss: 6.2460\n",
      " 300/9797 [..............................] - ETA: 22:32 - loss: 6.2494\n",
      " 400/9797 [>.............................] - ETA: 22:18 - loss: 6.2473\n",
      " 500/9797 [>.............................] - ETA: 22:04 - loss: 6.2304\n",
      " 600/9797 [>.............................] - ETA: 21:49 - loss: 6.2079\n",
      " 700/9797 [=>............................] - ETA: 21:35 - loss: 6.1927\n",
      " 800/9797 [=>............................] - ETA: 21:21 - loss: 6.1801\n",
      " 900/9797 [=>............................] - ETA: 21:06 - loss: 6.1794\n",
      "1000/9797 [==>...........................] - ETA: 20:52 - loss: 6.1791\n",
      "1100/9797 [==>...........................] - ETA: 20:38 - loss: 6.1820\n",
      "1200/9797 [==>...........................] - ETA: 20:24 - loss: 6.1807\n",
      "1300/9797 [==>...........................] - ETA: 20:10 - loss: 6.1747\n",
      "1400/9797 [===>..........................] - ETA: 19:56 - loss: 6.1677\n",
      "1500/9797 [===>..........................] - ETA: 19:42 - loss: 6.1626\n",
      "1600/9797 [===>..........................] - ETA: 19:28 - loss: 6.1658\n",
      "1700/9797 [====>.........................] - ETA: 19:14 - loss: 6.1672\n",
      "1800/9797 [====>.........................] - ETA: 19:00 - loss: 6.1603\n",
      "1900/9797 [====>.........................] - ETA: 18:46 - loss: 6.1574\n",
      "2000/9797 [=====>........................] - ETA: 18:32 - loss: 6.1517\n",
      "2100/9797 [=====>........................] - ETA: 18:18 - loss: 6.1453\n",
      "2200/9797 [=====>........................] - ETA: 18:04 - loss: 6.1419\n",
      "2300/9797 [======>.......................] - ETA: 17:49 - loss: 6.1327\n",
      "2400/9797 [======>.......................] - ETA: 17:35 - loss: 6.1316\n",
      "2500/9797 [======>.......................] - ETA: 17:21 - loss: 6.1265\n",
      "2600/9797 [======>.......................] - ETA: 17:07 - loss: 6.1262\n",
      "2700/9797 [=======>......................] - ETA: 16:53 - loss: 6.1213\n",
      "2800/9797 [=======>......................] - ETA: 16:39 - loss: 6.1171\n",
      "2900/9797 [=======>......................] - ETA: 16:24 - loss: 6.1121\n",
      "3000/9797 [========>.....................] - ETA: 16:10 - loss: 6.1048\n",
      "3100/9797 [========>.....................] - ETA: 15:56 - loss: 6.1047\n",
      "3200/9797 [========>.....................] - ETA: 15:42 - loss: 6.1022\n",
      "3300/9797 [=========>....................] - ETA: 15:27 - loss: 6.1020\n",
      "3400/9797 [=========>....................] - ETA: 15:13 - loss: 6.0970\n",
      "3500/9797 [=========>....................] - ETA: 14:59 - loss: 6.0928\n",
      "3600/9797 [==========>...................] - ETA: 14:44 - loss: 6.0916\n",
      "3700/9797 [==========>...................] - ETA: 14:30 - loss: 6.0890\n",
      "3800/9797 [==========>...................] - ETA: 14:16 - loss: 6.0835\n",
      "3900/9797 [==========>...................] - ETA: 14:01 - loss: 6.0817\n",
      "4000/9797 [===========>..................] - ETA: 13:47 - loss: 6.0800\n",
      "4100/9797 [===========>..................] - ETA: 13:33 - loss: 6.0769\n",
      "4200/9797 [===========>..................] - ETA: 13:19 - loss: 6.0760\n",
      "4300/9797 [============>.................] - ETA: 13:04 - loss: 6.0738\n",
      "4400/9797 [============>.................] - ETA: 12:50 - loss: 6.0710\n",
      "4500/9797 [============>.................] - ETA: 12:36 - loss: 6.0671\n",
      "4600/9797 [=============>................] - ETA: 12:22 - loss: 6.0647\n",
      "4700/9797 [=============>................] - ETA: 12:07 - loss: 6.0646\n",
      "4800/9797 [=============>................] - ETA: 11:53 - loss: 6.0608\n",
      "4900/9797 [==============>...............] - ETA: 11:39 - loss: 6.0565\n",
      "5000/9797 [==============>...............] - ETA: 11:25 - loss: 6.0532\n",
      "5100/9797 [==============>...............] - ETA: 11:10 - loss: 6.0492\n",
      "5200/9797 [==============>...............] - ETA: 10:56 - loss: 6.0475\n",
      "5300/9797 [===============>..............] - ETA: 10:42 - loss: 6.0449\n",
      "5400/9797 [===============>..............] - ETA: 10:27 - loss: 6.0441\n",
      "5500/9797 [===============>..............] - ETA: 10:13 - loss: 6.0419\n",
      "5600/9797 [================>.............] - ETA: 9:59 - loss: 6.0399\n",
      "5700/9797 [================>.............] - ETA: 9:45 - loss: 6.0357\n",
      "5800/9797 [================>.............] - ETA: 9:30 - loss: 6.0327\n",
      "5900/9797 [=================>............] - ETA: 9:16 - loss: 6.0303\n",
      "6000/9797 [=================>............] - ETA: 9:02 - loss: 6.0265\n",
      "6100/9797 [=================>............] - ETA: 8:48 - loss: 6.0225\n",
      "6200/9797 [=================>............] - ETA: 8:33 - loss: 6.0190\n",
      "6300/9797 [==================>...........] - ETA: 8:19 - loss: 6.0158\n",
      "6400/9797 [==================>...........] - ETA: 8:05 - loss: 6.0125\n",
      "6500/9797 [==================>...........] - ETA: 7:50 - loss: 6.0080\n",
      "6600/9797 [===================>..........] - ETA: 7:36 - loss: 6.0050\n",
      "6700/9797 [===================>..........] - ETA: 7:22 - loss: 6.0022\n",
      "6800/9797 [===================>..........] - ETA: 7:08 - loss: 6.0006\n",
      "6900/9797 [====================>.........] - ETA: 6:53 - loss: 5.9973\n",
      "7000/9797 [====================>.........] - ETA: 6:39 - loss: 5.9943\n",
      "7100/9797 [====================>.........] - ETA: 6:25 - loss: 5.9923\n",
      "7200/9797 [=====================>........] - ETA: 6:10 - loss: 5.9892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7300/9797 [=====================>........] - ETA: 5:56 - loss: 5.9861\n",
      "7400/9797 [=====================>........] - ETA: 5:42 - loss: 5.9832\n",
      "7500/9797 [=====================>........] - ETA: 5:28 - loss: 5.9791\n",
      "7600/9797 [======================>.......] - ETA: 5:13 - loss: 5.9752\n",
      "7700/9797 [======================>.......] - ETA: 4:59 - loss: 5.9713\n",
      "7800/9797 [======================>.......] - ETA: 4:45 - loss: 5.9678\n",
      "7900/9797 [=======================>......] - ETA: 4:30 - loss: 5.9641\n",
      "8000/9797 [=======================>......] - ETA: 4:16 - loss: 5.9622\n",
      "8100/9797 [=======================>......] - ETA: 4:02 - loss: 5.9603\n",
      "8200/9797 [========================>.....] - ETA: 3:48 - loss: 5.9585\n",
      "8300/9797 [========================>.....] - ETA: 3:33 - loss: 5.9563\n",
      "8400/9797 [========================>.....] - ETA: 3:19 - loss: 5.9523\n",
      "8500/9797 [=========================>....] - ETA: 3:05 - loss: 5.9497\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 5.9463\n",
      "8700/9797 [=========================>....] - ETA: 2:36 - loss: 5.9429\n",
      "8800/9797 [=========================>....] - ETA: 2:22 - loss: 5.9400\n",
      "8900/9797 [==========================>...] - ETA: 2:08 - loss: 5.9365\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 5.9350\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 5.9305\n",
      "9200/9797 [===========================>..] - ETA: 1:25 - loss: 5.9263\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 5.9228\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 5.9177\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 5.9146\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 5.9117\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 5.9096\n",
      "Epoch 00007: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_007.tlt\n",
      "coleoptera    AP    0.65531\n",
      "diptera       AP    0.70493\n",
      "geometridae   AP    0.85097\n",
      "hemiptera     AP    0.69465\n",
      "hymenoptera   AP    0.67051\n",
      "noctuidae     AP    0.82213\n",
      "odonata       AP    0.8028\n",
      "orthoptera    AP    0.76539\n",
      "trichoptera   AP    0.82614\n",
      "              mAP   0.75476\n",
      "Validation loss: 2.348914407210773\n",
      "Epoch 8/20\n",
      " 100/9797 [..............................] - ETA: 23:03 - loss: 5.5980\n",
      " 200/9797 [..............................] - ETA: 22:49 - loss: 5.5856\n",
      " 300/9797 [..............................] - ETA: 22:31 - loss: 5.5758\n",
      " 400/9797 [>.............................] - ETA: 22:16 - loss: 5.5823\n",
      " 500/9797 [>.............................] - ETA: 22:01 - loss: 5.6046\n",
      " 600/9797 [>.............................] - ETA: 21:46 - loss: 5.6068\n",
      " 700/9797 [=>............................] - ETA: 21:32 - loss: 5.6062\n",
      " 800/9797 [=>............................] - ETA: 21:19 - loss: 5.5840\n",
      " 900/9797 [=>............................] - ETA: 21:04 - loss: 5.5910\n",
      "1000/9797 [==>...........................] - ETA: 20:50 - loss: 5.5865\n",
      "1100/9797 [==>...........................] - ETA: 20:36 - loss: 5.5937\n",
      "1200/9797 [==>...........................] - ETA: 20:22 - loss: 5.5994\n",
      "1300/9797 [==>...........................] - ETA: 20:08 - loss: 5.5899\n",
      "1400/9797 [===>..........................] - ETA: 19:54 - loss: 5.5890\n",
      "1500/9797 [===>..........................] - ETA: 19:40 - loss: 5.5852\n",
      "1600/9797 [===>..........................] - ETA: 19:25 - loss: 5.5800\n",
      "1700/9797 [====>.........................] - ETA: 19:11 - loss: 5.5759\n",
      "1800/9797 [====>.........................] - ETA: 18:57 - loss: 5.5768\n",
      "1900/9797 [====>.........................] - ETA: 18:43 - loss: 5.5740\n",
      "2000/9797 [=====>........................] - ETA: 18:29 - loss: 5.5659\n",
      "2100/9797 [=====>........................] - ETA: 18:15 - loss: 5.5616\n",
      "2200/9797 [=====>........................] - ETA: 18:01 - loss: 5.5635\n",
      "2300/9797 [======>.......................] - ETA: 17:47 - loss: 5.5618\n",
      "2400/9797 [======>.......................] - ETA: 17:33 - loss: 5.5583\n",
      "2500/9797 [======>.......................] - ETA: 17:19 - loss: 5.5533\n",
      "2600/9797 [======>.......................] - ETA: 17:05 - loss: 5.5547\n",
      "2700/9797 [=======>......................] - ETA: 16:51 - loss: 5.5542\n",
      "2800/9797 [=======>......................] - ETA: 16:37 - loss: 5.5537\n",
      "2900/9797 [=======>......................] - ETA: 16:23 - loss: 5.5507\n",
      "3000/9797 [========>.....................] - ETA: 16:09 - loss: 5.5506\n",
      "3100/9797 [========>.....................] - ETA: 15:54 - loss: 5.5489\n",
      "3200/9797 [========>.....................] - ETA: 15:40 - loss: 5.5461\n",
      "3300/9797 [=========>....................] - ETA: 15:26 - loss: 5.5463\n",
      "3400/9797 [=========>....................] - ETA: 15:12 - loss: 5.5422\n",
      "3500/9797 [=========>....................] - ETA: 14:58 - loss: 5.5391\n",
      "3600/9797 [==========>...................] - ETA: 14:44 - loss: 5.5367\n",
      "3700/9797 [==========>...................] - ETA: 14:29 - loss: 5.5316\n",
      "3800/9797 [==========>...................] - ETA: 14:15 - loss: 5.5292\n",
      "3900/9797 [==========>...................] - ETA: 14:01 - loss: 5.5268\n",
      "4000/9797 [===========>..................] - ETA: 13:47 - loss: 5.5251\n",
      "4100/9797 [===========>..................] - ETA: 13:32 - loss: 5.5227\n",
      "4200/9797 [===========>..................] - ETA: 13:18 - loss: 5.5206\n",
      "4300/9797 [============>.................] - ETA: 13:04 - loss: 5.5184\n",
      "4400/9797 [============>.................] - ETA: 12:50 - loss: 5.5168\n",
      "4500/9797 [============>.................] - ETA: 12:35 - loss: 5.5152\n",
      "4600/9797 [=============>................] - ETA: 12:21 - loss: 5.5123\n",
      "4700/9797 [=============>................] - ETA: 12:07 - loss: 5.5124\n",
      "4800/9797 [=============>................] - ETA: 11:53 - loss: 5.5090\n",
      "4900/9797 [==============>...............] - ETA: 11:38 - loss: 5.5059\n",
      "5000/9797 [==============>...............] - ETA: 11:24 - loss: 5.5039\n",
      "5100/9797 [==============>...............] - ETA: 11:10 - loss: 5.5035\n",
      "5200/9797 [==============>...............] - ETA: 10:56 - loss: 5.5021\n",
      "5300/9797 [===============>..............] - ETA: 10:41 - loss: 5.4989\n",
      "5400/9797 [===============>..............] - ETA: 10:27 - loss: 5.4970\n",
      "5500/9797 [===============>..............] - ETA: 10:13 - loss: 5.4968\n",
      "5600/9797 [================>.............] - ETA: 9:59 - loss: 5.4938\n",
      "5700/9797 [================>.............] - ETA: 9:44 - loss: 5.4921\n",
      "5800/9797 [================>.............] - ETA: 9:30 - loss: 5.4895\n",
      "5900/9797 [=================>............] - ETA: 9:16 - loss: 5.4893\n",
      "6000/9797 [=================>............] - ETA: 9:01 - loss: 5.4885\n",
      "6100/9797 [=================>............] - ETA: 8:47 - loss: 5.4855\n",
      "6200/9797 [=================>............] - ETA: 8:33 - loss: 5.4837\n",
      "6300/9797 [==================>...........] - ETA: 8:19 - loss: 5.4802\n",
      "6400/9797 [==================>...........] - ETA: 8:04 - loss: 5.4797\n",
      "6500/9797 [==================>...........] - ETA: 7:50 - loss: 5.4778\n",
      "6600/9797 [===================>..........] - ETA: 7:36 - loss: 5.4760\n",
      "6700/9797 [===================>..........] - ETA: 7:21 - loss: 5.4731\n",
      "6800/9797 [===================>..........] - ETA: 7:07 - loss: 5.4723\n",
      "6900/9797 [====================>.........] - ETA: 6:53 - loss: 5.4707\n",
      "7000/9797 [====================>.........] - ETA: 6:39 - loss: 5.4669\n",
      "7100/9797 [====================>.........] - ETA: 6:24 - loss: 5.4662\n",
      "7200/9797 [=====================>........] - ETA: 6:10 - loss: 5.4645\n",
      "7300/9797 [=====================>........] - ETA: 5:56 - loss: 5.4620\n",
      "7400/9797 [=====================>........] - ETA: 5:41 - loss: 5.4601\n",
      "7500/9797 [=====================>........] - ETA: 5:27 - loss: 5.4579\n",
      "7600/9797 [======================>.......] - ETA: 5:13 - loss: 5.4554\n",
      "7700/9797 [======================>.......] - ETA: 4:59 - loss: 5.4538\n",
      "7800/9797 [======================>.......] - ETA: 4:44 - loss: 5.4521\n",
      "7900/9797 [=======================>......] - ETA: 4:30 - loss: 5.4505\n",
      "8000/9797 [=======================>......] - ETA: 4:16 - loss: 5.4480\n",
      "8100/9797 [=======================>......] - ETA: 4:02 - loss: 5.4458\n",
      "8200/9797 [========================>.....] - ETA: 3:47 - loss: 5.4446\n",
      "8300/9797 [========================>.....] - ETA: 3:33 - loss: 5.4414\n",
      "8400/9797 [========================>.....] - ETA: 3:19 - loss: 5.4394\n",
      "8500/9797 [=========================>....] - ETA: 3:05 - loss: 5.4378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 5.4331\n",
      "8700/9797 [=========================>....] - ETA: 2:36 - loss: 5.4307\n",
      "8800/9797 [=========================>....] - ETA: 2:22 - loss: 5.4298\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 5.4280\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 5.4259\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 5.4236\n",
      "9200/9797 [===========================>..] - ETA: 1:25 - loss: 5.4207\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 5.4181\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 5.4168\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 5.4145\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 5.4124\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 5.4104\n",
      "Epoch 00008: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_008.tlt\n",
      "coleoptera    AP    0.70117\n",
      "diptera       AP    0.76473\n",
      "geometridae   AP    0.87726\n",
      "hemiptera     AP    0.73344\n",
      "hymenoptera   AP    0.70065\n",
      "noctuidae     AP    0.87691\n",
      "odonata       AP    0.85565\n",
      "orthoptera    AP    0.77464\n",
      "trichoptera   AP    0.872\n",
      "              mAP   0.79516\n",
      "Validation loss: 2.1090721841191393\n",
      "Epoch 9/20\n",
      " 100/9797 [..............................] - ETA: 23:01 - loss: 5.1532\n",
      " 200/9797 [..............................] - ETA: 22:48 - loss: 5.1766\n",
      " 300/9797 [..............................] - ETA: 22:34 - loss: 5.1812\n",
      " 400/9797 [>.............................] - ETA: 22:18 - loss: 5.1760\n",
      " 500/9797 [>.............................] - ETA: 22:02 - loss: 5.1855\n",
      " 600/9797 [>.............................] - ETA: 21:48 - loss: 5.1924\n",
      " 700/9797 [=>............................] - ETA: 21:33 - loss: 5.2031\n",
      " 800/9797 [=>............................] - ETA: 21:19 - loss: 5.1946\n",
      " 900/9797 [=>............................] - ETA: 21:05 - loss: 5.1943\n",
      "1000/9797 [==>...........................] - ETA: 20:51 - loss: 5.1836\n",
      "1100/9797 [==>...........................] - ETA: 20:37 - loss: 5.1753\n",
      "1200/9797 [==>...........................] - ETA: 20:23 - loss: 5.1694\n",
      "1300/9797 [==>...........................] - ETA: 20:09 - loss: 5.1663\n",
      "1400/9797 [===>..........................] - ETA: 19:54 - loss: 5.1646\n",
      "1500/9797 [===>..........................] - ETA: 19:40 - loss: 5.1568\n",
      "1600/9797 [===>..........................] - ETA: 19:26 - loss: 5.1592\n",
      "1700/9797 [====>.........................] - ETA: 19:12 - loss: 5.1597\n",
      "1800/9797 [====>.........................] - ETA: 18:58 - loss: 5.1528\n",
      "1900/9797 [====>.........................] - ETA: 18:43 - loss: 5.1514\n",
      "2000/9797 [=====>........................] - ETA: 18:29 - loss: 5.1464\n",
      "2100/9797 [=====>........................] - ETA: 18:15 - loss: 5.1413\n",
      "2200/9797 [=====>........................] - ETA: 18:01 - loss: 5.1383\n",
      "2300/9797 [======>.......................] - ETA: 17:47 - loss: 5.1371\n",
      "2400/9797 [======>.......................] - ETA: 17:33 - loss: 5.1409\n",
      "2500/9797 [======>.......................] - ETA: 17:19 - loss: 5.1400\n",
      "2600/9797 [======>.......................] - ETA: 17:05 - loss: 5.1404\n",
      "2700/9797 [=======>......................] - ETA: 16:50 - loss: 5.1414\n",
      "2800/9797 [=======>......................] - ETA: 16:36 - loss: 5.1452\n",
      "2900/9797 [=======>......................] - ETA: 16:22 - loss: 5.1441\n",
      "3000/9797 [========>.....................] - ETA: 16:08 - loss: 5.1436\n",
      "3100/9797 [========>.....................] - ETA: 15:54 - loss: 5.1429\n",
      "3200/9797 [========>.....................] - ETA: 15:40 - loss: 5.1451\n",
      "3300/9797 [=========>....................] - ETA: 15:25 - loss: 5.1447\n",
      "3400/9797 [=========>....................] - ETA: 15:11 - loss: 5.1407\n",
      "3500/9797 [=========>....................] - ETA: 14:57 - loss: 5.1410\n",
      "3600/9797 [==========>...................] - ETA: 14:42 - loss: 5.1394\n",
      "3700/9797 [==========>...................] - ETA: 14:28 - loss: 5.1364\n",
      "3800/9797 [==========>...................] - ETA: 14:14 - loss: 5.1343\n",
      "3900/9797 [==========>...................] - ETA: 14:00 - loss: 5.1333\n",
      "4000/9797 [===========>..................] - ETA: 13:45 - loss: 5.1347\n",
      "4100/9797 [===========>..................] - ETA: 13:31 - loss: 5.1312\n",
      "4200/9797 [===========>..................] - ETA: 13:17 - loss: 5.1308\n",
      "4300/9797 [============>.................] - ETA: 13:03 - loss: 5.1328\n",
      "4400/9797 [============>.................] - ETA: 12:49 - loss: 5.1306\n",
      "4500/9797 [============>.................] - ETA: 12:34 - loss: 5.1301\n",
      "4600/9797 [=============>................] - ETA: 12:20 - loss: 5.1289\n",
      "4700/9797 [=============>................] - ETA: 12:06 - loss: 5.1286\n",
      "4800/9797 [=============>................] - ETA: 11:52 - loss: 5.1260\n",
      "4900/9797 [==============>...............] - ETA: 11:37 - loss: 5.1252\n",
      "5000/9797 [==============>...............] - ETA: 11:23 - loss: 5.1244\n",
      "5100/9797 [==============>...............] - ETA: 11:09 - loss: 5.1230\n",
      "5200/9797 [==============>...............] - ETA: 10:55 - loss: 5.1211\n",
      "5300/9797 [===============>..............] - ETA: 10:40 - loss: 5.1198\n",
      "5400/9797 [===============>..............] - ETA: 10:26 - loss: 5.1180\n",
      "5500/9797 [===============>..............] - ETA: 10:12 - loss: 5.1165\n",
      "5600/9797 [================>.............] - ETA: 9:58 - loss: 5.1138\n",
      "5700/9797 [================>.............] - ETA: 9:43 - loss: 5.1111\n",
      "5800/9797 [================>.............] - ETA: 9:29 - loss: 5.1093\n",
      "5900/9797 [=================>............] - ETA: 9:15 - loss: 5.1064\n",
      "6000/9797 [=================>............] - ETA: 9:01 - loss: 5.1050\n",
      "6100/9797 [=================>............] - ETA: 8:47 - loss: 5.1039\n",
      "6200/9797 [=================>............] - ETA: 8:32 - loss: 5.1024\n",
      "6300/9797 [==================>...........] - ETA: 8:18 - loss: 5.1012\n",
      "6400/9797 [==================>...........] - ETA: 8:04 - loss: 5.0998\n",
      "6500/9797 [==================>...........] - ETA: 7:50 - loss: 5.0988\n",
      "6600/9797 [===================>..........] - ETA: 7:35 - loss: 5.0985\n",
      "6700/9797 [===================>..........] - ETA: 7:21 - loss: 5.0968\n",
      "6800/9797 [===================>..........] - ETA: 7:07 - loss: 5.0930\n",
      "6900/9797 [====================>.........] - ETA: 6:53 - loss: 5.0928\n",
      "7000/9797 [====================>.........] - ETA: 6:38 - loss: 5.0907\n",
      "7100/9797 [====================>.........] - ETA: 6:24 - loss: 5.0893\n",
      "7200/9797 [=====================>........] - ETA: 6:10 - loss: 5.0879\n",
      "7300/9797 [=====================>........] - ETA: 5:56 - loss: 5.0857\n",
      "7400/9797 [=====================>........] - ETA: 5:41 - loss: 5.0831\n",
      "7500/9797 [=====================>........] - ETA: 5:27 - loss: 5.0827\n",
      "7600/9797 [======================>.......] - ETA: 5:13 - loss: 5.0816\n",
      "7700/9797 [======================>.......] - ETA: 4:58 - loss: 5.0812\n",
      "7800/9797 [======================>.......] - ETA: 4:44 - loss: 5.0789\n",
      "7900/9797 [=======================>......] - ETA: 4:30 - loss: 5.0781\n",
      "8000/9797 [=======================>......] - ETA: 4:16 - loss: 5.0754\n",
      "8100/9797 [=======================>......] - ETA: 4:01 - loss: 5.0731\n",
      "8200/9797 [========================>.....] - ETA: 3:47 - loss: 5.0714\n",
      "8300/9797 [========================>.....] - ETA: 3:33 - loss: 5.0706\n",
      "8400/9797 [========================>.....] - ETA: 3:19 - loss: 5.0681\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 5.0674\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 5.0664\n",
      "8700/9797 [=========================>....] - ETA: 2:36 - loss: 5.0649\n",
      "8800/9797 [=========================>....] - ETA: 2:22 - loss: 5.0637\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 5.0627\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 5.0604\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 5.0592\n",
      "9200/9797 [===========================>..] - ETA: 1:25 - loss: 5.0577\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 5.0564\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 5.0547\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 5.0548\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 5.0540\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 5.0529\n",
      "Epoch 00009: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_009.tlt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coleoptera    AP    0.73894\n",
      "diptera       AP    0.76539\n",
      "geometridae   AP    0.81946\n",
      "hemiptera     AP    0.73667\n",
      "hymenoptera   AP    0.67289\n",
      "noctuidae     AP    0.82932\n",
      "odonata       AP    0.86373\n",
      "orthoptera    AP    0.83063\n",
      "trichoptera   AP    0.86676\n",
      "              mAP   0.79153\n",
      "Validation loss: 2.1744982670303\n",
      "Epoch 10/20\n",
      " 100/9797 [..............................] - ETA: 23:05 - loss: 5.0029\n",
      " 200/9797 [..............................] - ETA: 22:54 - loss: 5.0230\n",
      " 300/9797 [..............................] - ETA: 22:35 - loss: 4.9699\n",
      " 400/9797 [>.............................] - ETA: 22:20 - loss: 4.9485\n",
      " 500/9797 [>.............................] - ETA: 22:07 - loss: 4.9252\n",
      " 600/9797 [>.............................] - ETA: 21:52 - loss: 4.9278\n",
      " 700/9797 [=>............................] - ETA: 21:37 - loss: 4.9180\n",
      " 800/9797 [=>............................] - ETA: 21:23 - loss: 4.9107\n",
      " 900/9797 [=>............................] - ETA: 21:08 - loss: 4.9131\n",
      "1000/9797 [==>...........................] - ETA: 20:54 - loss: 4.9081\n",
      "1100/9797 [==>...........................] - ETA: 20:39 - loss: 4.9082\n",
      "1200/9797 [==>...........................] - ETA: 20:24 - loss: 4.9066\n",
      "1300/9797 [==>...........................] - ETA: 20:10 - loss: 4.9041\n",
      "1400/9797 [===>..........................] - ETA: 19:56 - loss: 4.9029\n",
      "1500/9797 [===>..........................] - ETA: 19:41 - loss: 4.8972\n",
      "1600/9797 [===>..........................] - ETA: 19:26 - loss: 4.8944\n",
      "1700/9797 [====>.........................] - ETA: 19:12 - loss: 4.8874\n",
      "1800/9797 [====>.........................] - ETA: 18:57 - loss: 4.8867\n",
      "1900/9797 [====>.........................] - ETA: 18:43 - loss: 4.8873\n",
      "2000/9797 [=====>........................] - ETA: 18:29 - loss: 4.8846\n",
      "2100/9797 [=====>........................] - ETA: 18:14 - loss: 4.8858\n",
      "2200/9797 [=====>........................] - ETA: 18:00 - loss: 4.8836\n",
      "2300/9797 [======>.......................] - ETA: 17:46 - loss: 4.8804\n",
      "2400/9797 [======>.......................] - ETA: 17:32 - loss: 4.8807\n",
      "2500/9797 [======>.......................] - ETA: 17:18 - loss: 4.8815\n",
      "2600/9797 [======>.......................] - ETA: 17:03 - loss: 4.8785\n",
      "2700/9797 [=======>......................] - ETA: 16:49 - loss: 4.8781\n",
      "2800/9797 [=======>......................] - ETA: 16:35 - loss: 4.8808\n",
      "2900/9797 [=======>......................] - ETA: 16:21 - loss: 4.8756\n",
      "3000/9797 [========>.....................] - ETA: 16:07 - loss: 4.8719\n",
      "3100/9797 [========>.....................] - ETA: 15:53 - loss: 4.8689\n",
      "3200/9797 [========>.....................] - ETA: 15:38 - loss: 4.8695\n",
      "3300/9797 [=========>....................] - ETA: 15:24 - loss: 4.8644\n",
      "3400/9797 [=========>....................] - ETA: 15:10 - loss: 4.8606\n",
      "3500/9797 [=========>....................] - ETA: 14:56 - loss: 4.8606\n",
      "3600/9797 [==========>...................] - ETA: 14:42 - loss: 4.8608\n",
      "3700/9797 [==========>...................] - ETA: 14:28 - loss: 4.8601\n",
      "3800/9797 [==========>...................] - ETA: 14:13 - loss: 4.8595\n",
      "3900/9797 [==========>...................] - ETA: 13:59 - loss: 4.8586\n",
      "4000/9797 [===========>..................] - ETA: 13:45 - loss: 4.8556\n",
      "4100/9797 [===========>..................] - ETA: 13:31 - loss: 4.8550\n",
      "4200/9797 [===========>..................] - ETA: 13:17 - loss: 4.8547\n",
      "4300/9797 [============>.................] - ETA: 13:02 - loss: 4.8499\n",
      "4400/9797 [============>.................] - ETA: 12:48 - loss: 4.8460\n",
      "4500/9797 [============>.................] - ETA: 12:34 - loss: 4.8459\n",
      "4600/9797 [=============>................] - ETA: 12:20 - loss: 4.8437\n",
      "4700/9797 [=============>................] - ETA: 12:05 - loss: 4.8415\n",
      "4800/9797 [=============>................] - ETA: 11:51 - loss: 4.8388\n",
      "4900/9797 [==============>...............] - ETA: 11:37 - loss: 4.8376\n",
      "5000/9797 [==============>...............] - ETA: 11:23 - loss: 4.8360\n",
      "5100/9797 [==============>...............] - ETA: 11:09 - loss: 4.8339\n",
      "5200/9797 [==============>...............] - ETA: 10:55 - loss: 4.8316\n",
      "5300/9797 [===============>..............] - ETA: 10:40 - loss: 4.8299\n",
      "5400/9797 [===============>..............] - ETA: 10:26 - loss: 4.8294\n",
      "5500/9797 [===============>..............] - ETA: 10:12 - loss: 4.8290\n",
      "5600/9797 [================>.............] - ETA: 9:58 - loss: 4.8261\n",
      "5700/9797 [================>.............] - ETA: 9:43 - loss: 4.8250\n",
      "5800/9797 [================>.............] - ETA: 9:29 - loss: 4.8236\n",
      "5900/9797 [=================>............] - ETA: 9:15 - loss: 4.8227\n",
      "6000/9797 [=================>............] - ETA: 9:01 - loss: 4.8207\n",
      "6100/9797 [=================>............] - ETA: 8:46 - loss: 4.8201\n",
      "6200/9797 [=================>............] - ETA: 8:32 - loss: 4.8201\n",
      "6300/9797 [==================>...........] - ETA: 8:18 - loss: 4.8196\n",
      "6400/9797 [==================>...........] - ETA: 8:04 - loss: 4.8178\n",
      "6500/9797 [==================>...........] - ETA: 7:49 - loss: 4.8157\n",
      "6600/9797 [===================>..........] - ETA: 7:35 - loss: 4.8136\n",
      "6700/9797 [===================>..........] - ETA: 7:21 - loss: 4.8108\n",
      "6800/9797 [===================>..........] - ETA: 7:07 - loss: 4.8097\n",
      "6900/9797 [====================>.........] - ETA: 6:52 - loss: 4.8068\n",
      "7000/9797 [====================>.........] - ETA: 6:38 - loss: 4.8049\n",
      "7100/9797 [====================>.........] - ETA: 6:24 - loss: 4.8043\n",
      "7200/9797 [=====================>........] - ETA: 6:10 - loss: 4.8043\n",
      "7300/9797 [=====================>........] - ETA: 5:55 - loss: 4.8029\n",
      "7400/9797 [=====================>........] - ETA: 5:41 - loss: 4.8031\n",
      "7500/9797 [=====================>........] - ETA: 5:27 - loss: 4.8017\n",
      "7600/9797 [======================>.......] - ETA: 5:13 - loss: 4.8007\n",
      "7700/9797 [======================>.......] - ETA: 4:58 - loss: 4.7983\n",
      "7800/9797 [======================>.......] - ETA: 4:44 - loss: 4.7960\n",
      "7900/9797 [=======================>......] - ETA: 4:30 - loss: 4.7951\n",
      "8000/9797 [=======================>......] - ETA: 4:16 - loss: 4.7936\n",
      "8100/9797 [=======================>......] - ETA: 4:01 - loss: 4.7920\n",
      "8200/9797 [========================>.....] - ETA: 3:47 - loss: 4.7912\n",
      "8300/9797 [========================>.....] - ETA: 3:33 - loss: 4.7913\n",
      "8400/9797 [========================>.....] - ETA: 3:19 - loss: 4.7900\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 4.7892\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 4.7881\n",
      "8700/9797 [=========================>....] - ETA: 2:36 - loss: 4.7873\n",
      "8800/9797 [=========================>....] - ETA: 2:22 - loss: 4.7884\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 4.7889\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 4.7886\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 4.7875\n",
      "9200/9797 [===========================>..] - ETA: 1:25 - loss: 4.7861\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 4.7852\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 4.7851\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 4.7839\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 4.7828\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 4.7812\n",
      "Epoch 00010: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_010.tlt\n",
      "coleoptera    AP    0.75552\n",
      "diptera       AP    0.77062\n",
      "geometridae   AP    0.87302\n",
      "hemiptera     AP    0.71107\n",
      "hymenoptera   AP    0.71586\n",
      "noctuidae     AP    0.87217\n",
      "odonata       AP    0.85595\n",
      "orthoptera    AP    0.84403\n",
      "trichoptera   AP    0.87802\n",
      "              mAP   0.80847\n",
      "Validation loss: 2.1543643176487186\n",
      "Epoch 11/20\n",
      " 100/9797 [..............................] - ETA: 23:04 - loss: 4.7078\n",
      " 200/9797 [..............................] - ETA: 22:45 - loss: 4.6746\n",
      " 300/9797 [..............................] - ETA: 22:31 - loss: 4.6695\n",
      " 400/9797 [>.............................] - ETA: 22:16 - loss: 4.6681\n",
      " 500/9797 [>.............................] - ETA: 22:00 - loss: 4.6525\n",
      " 600/9797 [>.............................] - ETA: 21:45 - loss: 4.6502\n",
      " 700/9797 [=>............................] - ETA: 21:30 - loss: 4.6412\n",
      " 800/9797 [=>............................] - ETA: 21:16 - loss: 4.6376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 900/9797 [=>............................] - ETA: 21:02 - loss: 4.6196\n",
      "1000/9797 [==>...........................] - ETA: 20:48 - loss: 4.6176\n",
      "1100/9797 [==>...........................] - ETA: 20:33 - loss: 4.6089\n",
      "1200/9797 [==>...........................] - ETA: 20:20 - loss: 4.6096\n",
      "1300/9797 [==>...........................] - ETA: 20:06 - loss: 4.6139\n",
      "1400/9797 [===>..........................] - ETA: 19:52 - loss: 4.6133\n",
      "1500/9797 [===>..........................] - ETA: 19:38 - loss: 4.6171\n",
      "1600/9797 [===>..........................] - ETA: 19:23 - loss: 4.6145\n",
      "1700/9797 [====>.........................] - ETA: 19:09 - loss: 4.6142\n",
      "1800/9797 [====>.........................] - ETA: 18:55 - loss: 4.6136\n",
      "1900/9797 [====>.........................] - ETA: 18:41 - loss: 4.6111\n",
      "2000/9797 [=====>........................] - ETA: 18:26 - loss: 4.6056\n",
      "2100/9797 [=====>........................] - ETA: 18:12 - loss: 4.6030\n",
      "2200/9797 [=====>........................] - ETA: 17:58 - loss: 4.5997\n",
      "2300/9797 [======>.......................] - ETA: 17:44 - loss: 4.5976\n",
      "2400/9797 [======>.......................] - ETA: 17:29 - loss: 4.5954\n",
      "2500/9797 [======>.......................] - ETA: 17:15 - loss: 4.5943\n",
      "2600/9797 [======>.......................] - ETA: 17:01 - loss: 4.5914\n",
      "2700/9797 [=======>......................] - ETA: 16:47 - loss: 4.5918\n",
      "2800/9797 [=======>......................] - ETA: 16:33 - loss: 4.5895\n",
      "2900/9797 [=======>......................] - ETA: 16:19 - loss: 4.5904\n",
      "3000/9797 [========>.....................] - ETA: 16:04 - loss: 4.5886\n",
      "3100/9797 [========>.....................] - ETA: 15:50 - loss: 4.5893\n",
      "3200/9797 [========>.....................] - ETA: 15:36 - loss: 4.5918\n",
      "3300/9797 [=========>....................] - ETA: 15:22 - loss: 4.5905\n",
      "3400/9797 [=========>....................] - ETA: 15:08 - loss: 4.5901\n",
      "3500/9797 [=========>....................] - ETA: 14:54 - loss: 4.5881\n",
      "3600/9797 [==========>...................] - ETA: 14:40 - loss: 4.5869\n",
      "3700/9797 [==========>...................] - ETA: 14:25 - loss: 4.5847\n",
      "3800/9797 [==========>...................] - ETA: 14:11 - loss: 4.5834\n",
      "3900/9797 [==========>...................] - ETA: 13:57 - loss: 4.5815\n",
      "4000/9797 [===========>..................] - ETA: 13:43 - loss: 4.5804\n",
      "4100/9797 [===========>..................] - ETA: 13:29 - loss: 4.5805\n",
      "4200/9797 [===========>..................] - ETA: 13:15 - loss: 4.5825\n",
      "4300/9797 [============>.................] - ETA: 13:01 - loss: 4.5863\n",
      "4400/9797 [============>.................] - ETA: 12:47 - loss: 4.5845\n",
      "4500/9797 [============>.................] - ETA: 12:32 - loss: 4.5836\n",
      "4600/9797 [=============>................] - ETA: 12:18 - loss: 4.5835\n",
      "4700/9797 [=============>................] - ETA: 12:04 - loss: 4.5850\n",
      "4800/9797 [=============>................] - ETA: 11:50 - loss: 4.5840\n",
      "4900/9797 [==============>...............] - ETA: 11:36 - loss: 4.5812\n",
      "5000/9797 [==============>...............] - ETA: 11:21 - loss: 4.5815\n",
      "5100/9797 [==============>...............] - ETA: 11:07 - loss: 4.5815\n",
      "5200/9797 [==============>...............] - ETA: 10:53 - loss: 4.5815\n",
      "5300/9797 [===============>..............] - ETA: 10:39 - loss: 4.5813\n",
      "5400/9797 [===============>..............] - ETA: 10:25 - loss: 4.5774\n",
      "5500/9797 [===============>..............] - ETA: 10:10 - loss: 4.5761\n",
      "5600/9797 [================>.............] - ETA: 9:56 - loss: 4.5743\n",
      "5700/9797 [================>.............] - ETA: 9:42 - loss: 4.5719\n",
      "5800/9797 [================>.............] - ETA: 9:28 - loss: 4.5708\n",
      "5900/9797 [=================>............] - ETA: 9:13 - loss: 4.5699\n",
      "6000/9797 [=================>............] - ETA: 8:59 - loss: 4.5696\n",
      "6100/9797 [=================>............] - ETA: 8:45 - loss: 4.5696\n",
      "6200/9797 [=================>............] - ETA: 8:31 - loss: 4.5711\n",
      "6300/9797 [==================>...........] - ETA: 8:17 - loss: 4.5697\n",
      "6400/9797 [==================>...........] - ETA: 8:03 - loss: 4.5677\n",
      "6500/9797 [==================>...........] - ETA: 7:48 - loss: 4.5647\n",
      "6600/9797 [===================>..........] - ETA: 7:34 - loss: 4.5635\n",
      "6700/9797 [===================>..........] - ETA: 7:20 - loss: 4.5629\n",
      "6800/9797 [===================>..........] - ETA: 7:06 - loss: 4.5628\n",
      "6900/9797 [====================>.........] - ETA: 6:52 - loss: 4.5607\n",
      "7000/9797 [====================>.........] - ETA: 6:37 - loss: 4.5608\n",
      "7100/9797 [====================>.........] - ETA: 6:23 - loss: 4.5601\n",
      "7200/9797 [=====================>........] - ETA: 6:09 - loss: 4.5595\n",
      "7300/9797 [=====================>........] - ETA: 5:55 - loss: 4.5577\n",
      "7400/9797 [=====================>........] - ETA: 5:40 - loss: 4.5567\n",
      "7500/9797 [=====================>........] - ETA: 5:26 - loss: 4.5554\n",
      "7600/9797 [======================>.......] - ETA: 5:12 - loss: 4.5550\n",
      "7700/9797 [======================>.......] - ETA: 4:58 - loss: 4.5538\n",
      "7800/9797 [======================>.......] - ETA: 4:44 - loss: 4.5535\n",
      "7900/9797 [=======================>......] - ETA: 4:29 - loss: 4.5534\n",
      "8000/9797 [=======================>......] - ETA: 4:15 - loss: 4.5525\n",
      "8100/9797 [=======================>......] - ETA: 4:01 - loss: 4.5507\n",
      "8200/9797 [========================>.....] - ETA: 3:47 - loss: 4.5494\n",
      "8300/9797 [========================>.....] - ETA: 3:32 - loss: 4.5482\n",
      "8400/9797 [========================>.....] - ETA: 3:18 - loss: 4.5480\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 4.5466\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 4.5454\n",
      "8700/9797 [=========================>....] - ETA: 2:36 - loss: 4.5445\n",
      "8800/9797 [=========================>....] - ETA: 2:21 - loss: 4.5432\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 4.5417\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 4.5408\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 4.5416\n",
      "9200/9797 [===========================>..] - ETA: 1:24 - loss: 4.5402\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 4.5383\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 4.5372\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 4.5360\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 4.5354\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 4.5338\n",
      "Epoch 00011: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_011.tlt\n",
      "coleoptera    AP    0.72051\n",
      "diptera       AP    0.78977\n",
      "geometridae   AP    0.88464\n",
      "hemiptera     AP    0.72405\n",
      "hymenoptera   AP    0.74028\n",
      "noctuidae     AP    0.8781\n",
      "odonata       AP    0.79864\n",
      "orthoptera    AP    0.83161\n",
      "trichoptera   AP    0.85419\n",
      "              mAP   0.80242\n",
      "Validation loss: 2.1728229054109076\n",
      "Epoch 12/20\n",
      " 100/9797 [..............................] - ETA: 23:00 - loss: 4.4482\n",
      " 200/9797 [..............................] - ETA: 22:46 - loss: 4.4640\n",
      " 300/9797 [..............................] - ETA: 22:33 - loss: 4.4453\n",
      " 400/9797 [>.............................] - ETA: 22:18 - loss: 4.4424\n",
      " 500/9797 [>.............................] - ETA: 22:03 - loss: 4.4550\n",
      " 600/9797 [>.............................] - ETA: 21:48 - loss: 4.4530\n",
      " 700/9797 [=>............................] - ETA: 21:34 - loss: 4.4279\n",
      " 800/9797 [=>............................] - ETA: 21:19 - loss: 4.4111\n",
      " 900/9797 [=>............................] - ETA: 21:04 - loss: 4.4075\n",
      "1000/9797 [==>...........................] - ETA: 20:49 - loss: 4.4032\n",
      "1100/9797 [==>...........................] - ETA: 20:35 - loss: 4.4023\n",
      "1200/9797 [==>...........................] - ETA: 20:21 - loss: 4.4035\n",
      "1300/9797 [==>...........................] - ETA: 20:06 - loss: 4.4088\n",
      "1400/9797 [===>..........................] - ETA: 19:52 - loss: 4.4029\n",
      "1500/9797 [===>..........................] - ETA: 19:38 - loss: 4.4066\n",
      "1600/9797 [===>..........................] - ETA: 19:24 - loss: 4.4008\n",
      "1700/9797 [====>.........................] - ETA: 19:10 - loss: 4.4037\n",
      "1800/9797 [====>.........................] - ETA: 18:55 - loss: 4.4039\n",
      "1900/9797 [====>.........................] - ETA: 18:41 - loss: 4.3997\n",
      "2000/9797 [=====>........................] - ETA: 18:27 - loss: 4.3988\n",
      "2100/9797 [=====>........................] - ETA: 18:12 - loss: 4.4025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/9797 [=====>........................] - ETA: 17:58 - loss: 4.3989\n",
      "2300/9797 [======>.......................] - ETA: 17:44 - loss: 4.3959\n",
      "2400/9797 [======>.......................] - ETA: 17:30 - loss: 4.3964\n",
      "2500/9797 [======>.......................] - ETA: 17:15 - loss: 4.3933\n",
      "2600/9797 [======>.......................] - ETA: 17:01 - loss: 4.3968\n",
      "2700/9797 [=======>......................] - ETA: 16:47 - loss: 4.3986\n",
      "2800/9797 [=======>......................] - ETA: 16:33 - loss: 4.3983\n",
      "2900/9797 [=======>......................] - ETA: 16:19 - loss: 4.3958\n",
      "3000/9797 [========>.....................] - ETA: 16:04 - loss: 4.3982\n",
      "3100/9797 [========>.....................] - ETA: 15:50 - loss: 4.3974\n",
      "3200/9797 [========>.....................] - ETA: 15:36 - loss: 4.3920\n",
      "3300/9797 [=========>....................] - ETA: 15:22 - loss: 4.3916\n",
      "3400/9797 [=========>....................] - ETA: 15:08 - loss: 4.3906\n",
      "3500/9797 [=========>....................] - ETA: 14:53 - loss: 4.3883\n",
      "3600/9797 [==========>...................] - ETA: 14:39 - loss: 4.3884\n",
      "3700/9797 [==========>...................] - ETA: 14:25 - loss: 4.3899\n",
      "3800/9797 [==========>...................] - ETA: 14:11 - loss: 4.3905\n",
      "3900/9797 [==========>...................] - ETA: 13:57 - loss: 4.3880\n",
      "4000/9797 [===========>..................] - ETA: 13:42 - loss: 4.3867\n",
      "4100/9797 [===========>..................] - ETA: 13:28 - loss: 4.3863\n",
      "4200/9797 [===========>..................] - ETA: 13:14 - loss: 4.3843\n",
      "4300/9797 [============>.................] - ETA: 13:00 - loss: 4.3831\n",
      "4400/9797 [============>.................] - ETA: 12:45 - loss: 4.3808\n",
      "4500/9797 [============>.................] - ETA: 12:31 - loss: 4.3789\n",
      "4600/9797 [=============>................] - ETA: 12:17 - loss: 4.3781\n",
      "4700/9797 [=============>................] - ETA: 12:03 - loss: 4.3789\n",
      "4800/9797 [=============>................] - ETA: 11:49 - loss: 4.3737\n",
      "4900/9797 [==============>...............] - ETA: 11:35 - loss: 4.3715\n",
      "5000/9797 [==============>...............] - ETA: 11:20 - loss: 4.3717\n",
      "5100/9797 [==============>...............] - ETA: 11:06 - loss: 4.3698\n",
      "5200/9797 [==============>...............] - ETA: 10:52 - loss: 4.3691\n",
      "5300/9797 [===============>..............] - ETA: 10:38 - loss: 4.3688\n",
      "5400/9797 [===============>..............] - ETA: 10:24 - loss: 4.3683\n",
      "5500/9797 [===============>..............] - ETA: 10:10 - loss: 4.3676\n",
      "5600/9797 [================>.............] - ETA: 9:55 - loss: 4.3656\n",
      "5700/9797 [================>.............] - ETA: 9:41 - loss: 4.3637\n",
      "5800/9797 [================>.............] - ETA: 9:27 - loss: 4.3620\n",
      "5900/9797 [=================>............] - ETA: 9:13 - loss: 4.3626\n",
      "6000/9797 [=================>............] - ETA: 8:59 - loss: 4.3638\n",
      "6100/9797 [=================>............] - ETA: 8:45 - loss: 4.3640\n",
      "6200/9797 [=================>............] - ETA: 8:30 - loss: 4.3643\n",
      "6300/9797 [==================>...........] - ETA: 8:16 - loss: 4.3644\n",
      "6400/9797 [==================>...........] - ETA: 8:02 - loss: 4.3627\n",
      "6500/9797 [==================>...........] - ETA: 7:48 - loss: 4.3628\n",
      "6600/9797 [===================>..........] - ETA: 7:34 - loss: 4.3613\n",
      "6700/9797 [===================>..........] - ETA: 7:20 - loss: 4.3613\n",
      "6800/9797 [===================>..........] - ETA: 7:05 - loss: 4.3612\n",
      "6900/9797 [====================>.........] - ETA: 6:51 - loss: 4.3605\n",
      "7000/9797 [====================>.........] - ETA: 6:37 - loss: 4.3611\n",
      "7100/9797 [====================>.........] - ETA: 6:23 - loss: 4.3607\n",
      "7200/9797 [=====================>........] - ETA: 6:09 - loss: 4.3601\n",
      "7300/9797 [=====================>........] - ETA: 5:54 - loss: 4.3596\n",
      "7400/9797 [=====================>........] - ETA: 5:40 - loss: 4.3579\n",
      "7500/9797 [=====================>........] - ETA: 5:26 - loss: 4.3577\n",
      "7600/9797 [======================>.......] - ETA: 5:12 - loss: 4.3568\n",
      "7700/9797 [======================>.......] - ETA: 4:58 - loss: 4.3567\n",
      "7800/9797 [======================>.......] - ETA: 4:43 - loss: 4.3546\n",
      "7900/9797 [=======================>......] - ETA: 4:29 - loss: 4.3544\n",
      "8000/9797 [=======================>......] - ETA: 4:15 - loss: 4.3531\n",
      "8100/9797 [=======================>......] - ETA: 4:01 - loss: 4.3513\n",
      "8200/9797 [========================>.....] - ETA: 3:47 - loss: 4.3518\n",
      "8300/9797 [========================>.....] - ETA: 3:32 - loss: 4.3499\n",
      "8400/9797 [========================>.....] - ETA: 3:18 - loss: 4.3482\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 4.3465\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 4.3456\n",
      "8700/9797 [=========================>....] - ETA: 2:35 - loss: 4.3442\n",
      "8800/9797 [=========================>....] - ETA: 2:21 - loss: 4.3422\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 4.3407\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 4.3405\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 4.3397\n",
      "9200/9797 [===========================>..] - ETA: 1:24 - loss: 4.3393\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 4.3385\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 4.3371\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 4.3367\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 4.3365\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 4.3364\n",
      "Epoch 00012: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_012.tlt\n",
      "coleoptera    AP    0.79053\n",
      "diptera       AP    0.79812\n",
      "geometridae   AP    0.89256\n",
      "hemiptera     AP    0.77969\n",
      "hymenoptera   AP    0.78463\n",
      "noctuidae     AP    0.86603\n",
      "odonata       AP    0.89337\n",
      "orthoptera    AP    0.85059\n",
      "trichoptera   AP    0.88807\n",
      "              mAP   0.83817\n",
      "Validation loss: 1.9702458405609682\n",
      "Epoch 13/20\n",
      " 100/9797 [..............................] - ETA: 23:04 - loss: 4.3412\n",
      " 200/9797 [..............................] - ETA: 22:41 - loss: 4.2832\n",
      " 300/9797 [..............................] - ETA: 22:28 - loss: 4.2685\n",
      " 400/9797 [>.............................] - ETA: 22:16 - loss: 4.2757\n",
      " 500/9797 [>.............................] - ETA: 22:02 - loss: 4.2570\n",
      " 600/9797 [>.............................] - ETA: 21:48 - loss: 4.2556\n",
      " 700/9797 [=>............................] - ETA: 21:33 - loss: 4.2466\n",
      " 800/9797 [=>............................] - ETA: 21:19 - loss: 4.2320\n",
      " 900/9797 [=>............................] - ETA: 21:06 - loss: 4.2306\n",
      "1000/9797 [==>...........................] - ETA: 20:51 - loss: 4.2219\n",
      "1100/9797 [==>...........................] - ETA: 20:37 - loss: 4.2221\n",
      "1200/9797 [==>...........................] - ETA: 20:22 - loss: 4.2213\n",
      "1300/9797 [==>...........................] - ETA: 20:08 - loss: 4.2178\n",
      "1400/9797 [===>..........................] - ETA: 19:54 - loss: 4.2171\n",
      "1500/9797 [===>..........................] - ETA: 19:40 - loss: 4.2182\n",
      "1600/9797 [===>..........................] - ETA: 19:26 - loss: 4.2241\n",
      "1700/9797 [====>.........................] - ETA: 19:11 - loss: 4.2206\n",
      "1800/9797 [====>.........................] - ETA: 18:57 - loss: 4.2222\n",
      "1900/9797 [====>.........................] - ETA: 18:43 - loss: 4.2230\n",
      "2000/9797 [=====>........................] - ETA: 18:29 - loss: 4.2231\n",
      "2100/9797 [=====>........................] - ETA: 18:15 - loss: 4.2225\n",
      "2200/9797 [=====>........................] - ETA: 18:00 - loss: 4.2195\n",
      "2300/9797 [======>.......................] - ETA: 17:46 - loss: 4.2134\n",
      "2400/9797 [======>.......................] - ETA: 17:32 - loss: 4.2140\n",
      "2500/9797 [======>.......................] - ETA: 17:18 - loss: 4.2123\n",
      "2600/9797 [======>.......................] - ETA: 17:04 - loss: 4.2125\n",
      "2700/9797 [=======>......................] - ETA: 16:49 - loss: 4.2131\n",
      "2800/9797 [=======>......................] - ETA: 16:35 - loss: 4.2146\n",
      "2900/9797 [=======>......................] - ETA: 16:21 - loss: 4.2122\n",
      "3000/9797 [========>.....................] - ETA: 16:07 - loss: 4.2122\n",
      "3100/9797 [========>.....................] - ETA: 15:52 - loss: 4.2118\n",
      "3200/9797 [========>.....................] - ETA: 15:38 - loss: 4.2106\n",
      "3300/9797 [=========>....................] - ETA: 15:24 - loss: 4.2111\n",
      "3400/9797 [=========>....................] - ETA: 15:10 - loss: 4.2096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500/9797 [=========>....................] - ETA: 14:56 - loss: 4.2116\n",
      "3600/9797 [==========>...................] - ETA: 14:41 - loss: 4.2125\n",
      "3700/9797 [==========>...................] - ETA: 14:27 - loss: 4.2081\n",
      "3800/9797 [==========>...................] - ETA: 14:13 - loss: 4.2059\n",
      "3900/9797 [==========>...................] - ETA: 13:59 - loss: 4.2047\n",
      "4000/9797 [===========>..................] - ETA: 13:45 - loss: 4.2053\n",
      "4100/9797 [===========>..................] - ETA: 13:30 - loss: 4.2057\n",
      "4200/9797 [===========>..................] - ETA: 13:16 - loss: 4.2048\n",
      "4300/9797 [============>.................] - ETA: 13:02 - loss: 4.2034\n",
      "4400/9797 [============>.................] - ETA: 12:48 - loss: 4.2018\n",
      "4500/9797 [============>.................] - ETA: 12:34 - loss: 4.2018\n",
      "4600/9797 [=============>................] - ETA: 12:19 - loss: 4.2007\n",
      "4700/9797 [=============>................] - ETA: 12:05 - loss: 4.2004\n",
      "4800/9797 [=============>................] - ETA: 11:51 - loss: 4.2006\n",
      "4900/9797 [==============>...............] - ETA: 11:37 - loss: 4.1989\n",
      "5000/9797 [==============>...............] - ETA: 11:22 - loss: 4.1986\n",
      "5100/9797 [==============>...............] - ETA: 11:08 - loss: 4.1958\n",
      "5200/9797 [==============>...............] - ETA: 10:54 - loss: 4.1951\n",
      "5300/9797 [===============>..............] - ETA: 10:40 - loss: 4.1940\n",
      "5400/9797 [===============>..............] - ETA: 10:26 - loss: 4.1946\n",
      "5500/9797 [===============>..............] - ETA: 10:11 - loss: 4.1937\n",
      "5600/9797 [================>.............] - ETA: 9:57 - loss: 4.1938\n",
      "5700/9797 [================>.............] - ETA: 9:43 - loss: 4.1914\n",
      "5800/9797 [================>.............] - ETA: 9:29 - loss: 4.1907\n",
      "5900/9797 [=================>............] - ETA: 9:14 - loss: 4.1908\n",
      "6000/9797 [=================>............] - ETA: 9:00 - loss: 4.1894\n",
      "6100/9797 [=================>............] - ETA: 8:46 - loss: 4.1900\n",
      "6200/9797 [=================>............] - ETA: 8:32 - loss: 4.1886\n",
      "6300/9797 [==================>...........] - ETA: 8:17 - loss: 4.1873\n",
      "6400/9797 [==================>...........] - ETA: 8:03 - loss: 4.1871\n",
      "6500/9797 [==================>...........] - ETA: 7:49 - loss: 4.1859\n",
      "6600/9797 [===================>..........] - ETA: 7:35 - loss: 4.1866\n",
      "6700/9797 [===================>..........] - ETA: 7:20 - loss: 4.1857\n",
      "6800/9797 [===================>..........] - ETA: 7:06 - loss: 4.1845\n",
      "6900/9797 [====================>.........] - ETA: 6:52 - loss: 4.1837\n",
      "7000/9797 [====================>.........] - ETA: 6:38 - loss: 4.1815\n",
      "7100/9797 [====================>.........] - ETA: 6:23 - loss: 4.1794\n",
      "7200/9797 [=====================>........] - ETA: 6:09 - loss: 4.1782\n",
      "7300/9797 [=====================>........] - ETA: 5:55 - loss: 4.1786\n",
      "7400/9797 [=====================>........] - ETA: 5:41 - loss: 4.1782\n",
      "7500/9797 [=====================>........] - ETA: 5:27 - loss: 4.1777\n",
      "7600/9797 [======================>.......] - ETA: 5:12 - loss: 4.1779\n",
      "7700/9797 [======================>.......] - ETA: 4:58 - loss: 4.1773\n",
      "7800/9797 [======================>.......] - ETA: 4:44 - loss: 4.1763\n",
      "7900/9797 [=======================>......] - ETA: 4:30 - loss: 4.1759\n",
      "8000/9797 [=======================>......] - ETA: 4:15 - loss: 4.1762\n",
      "8100/9797 [=======================>......] - ETA: 4:01 - loss: 4.1751\n",
      "8200/9797 [========================>.....] - ETA: 3:47 - loss: 4.1748\n",
      "8300/9797 [========================>.....] - ETA: 3:33 - loss: 4.1740\n",
      "8400/9797 [========================>.....] - ETA: 3:18 - loss: 4.1726\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 4.1718\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 4.1720\n",
      "8700/9797 [=========================>....] - ETA: 2:36 - loss: 4.1723\n",
      "8800/9797 [=========================>....] - ETA: 2:21 - loss: 4.1720\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 4.1716\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 4.1706\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 4.1694\n",
      "9200/9797 [===========================>..] - ETA: 1:24 - loss: 4.1676\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 4.1661\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 4.1656\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 4.1646\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 4.1637\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 4.1628\n",
      "Epoch 00013: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_013.tlt\n",
      "coleoptera    AP    0.75696\n",
      "diptera       AP    0.8052\n",
      "geometridae   AP    0.88885\n",
      "hemiptera     AP    0.7696\n",
      "hymenoptera   AP    0.75339\n",
      "noctuidae     AP    0.87858\n",
      "odonata       AP    0.85937\n",
      "orthoptera    AP    0.80483\n",
      "trichoptera   AP    0.88284\n",
      "              mAP   0.82218\n",
      "Validation loss: 1.878907761711509\n",
      "Epoch 14/20\n",
      " 100/9797 [..............................] - ETA: 23:00 - loss: 4.1329\n",
      " 200/9797 [..............................] - ETA: 22:44 - loss: 4.0873\n",
      " 300/9797 [..............................] - ETA: 22:30 - loss: 4.0605\n",
      " 400/9797 [>.............................] - ETA: 22:16 - loss: 4.0639\n",
      " 500/9797 [>.............................] - ETA: 22:01 - loss: 4.0484\n",
      " 600/9797 [>.............................] - ETA: 21:48 - loss: 4.0648\n",
      " 700/9797 [=>............................] - ETA: 21:34 - loss: 4.0677\n",
      " 800/9797 [=>............................] - ETA: 21:19 - loss: 4.0600\n",
      " 900/9797 [=>............................] - ETA: 21:06 - loss: 4.0516\n",
      "1000/9797 [==>...........................] - ETA: 20:51 - loss: 4.0535\n",
      "1100/9797 [==>...........................] - ETA: 20:38 - loss: 4.0575\n",
      "1200/9797 [==>...........................] - ETA: 20:24 - loss: 4.0526\n",
      "1300/9797 [==>...........................] - ETA: 20:09 - loss: 4.0434\n",
      "1400/9797 [===>..........................] - ETA: 19:55 - loss: 4.0412\n",
      "1500/9797 [===>..........................] - ETA: 19:40 - loss: 4.0436\n",
      "1600/9797 [===>..........................] - ETA: 19:26 - loss: 4.0403\n",
      "1700/9797 [====>.........................] - ETA: 19:12 - loss: 4.0422\n",
      "1800/9797 [====>.........................] - ETA: 18:57 - loss: 4.0453\n",
      "1900/9797 [====>.........................] - ETA: 18:43 - loss: 4.0446\n",
      "2000/9797 [=====>........................] - ETA: 18:29 - loss: 4.0431\n",
      "2100/9797 [=====>........................] - ETA: 18:14 - loss: 4.0435\n",
      "2200/9797 [=====>........................] - ETA: 18:00 - loss: 4.0425\n",
      "2300/9797 [======>.......................] - ETA: 17:45 - loss: 4.0407\n",
      "2400/9797 [======>.......................] - ETA: 17:31 - loss: 4.0373\n",
      "2500/9797 [======>.......................] - ETA: 17:17 - loss: 4.0414\n",
      "2600/9797 [======>.......................] - ETA: 17:03 - loss: 4.0380\n",
      "2700/9797 [=======>......................] - ETA: 16:49 - loss: 4.0391\n",
      "2800/9797 [=======>......................] - ETA: 16:34 - loss: 4.0394\n",
      "2900/9797 [=======>......................] - ETA: 16:20 - loss: 4.0396\n",
      "3000/9797 [========>.....................] - ETA: 16:06 - loss: 4.0372\n",
      "3100/9797 [========>.....................] - ETA: 15:52 - loss: 4.0369\n",
      "3200/9797 [========>.....................] - ETA: 15:37 - loss: 4.0338\n",
      "3300/9797 [=========>....................] - ETA: 15:23 - loss: 4.0321\n",
      "3400/9797 [=========>....................] - ETA: 15:09 - loss: 4.0315\n",
      "3500/9797 [=========>....................] - ETA: 14:55 - loss: 4.0311\n",
      "3600/9797 [==========>...................] - ETA: 14:41 - loss: 4.0322\n",
      "3700/9797 [==========>...................] - ETA: 14:26 - loss: 4.0322\n",
      "3800/9797 [==========>...................] - ETA: 14:12 - loss: 4.0304\n",
      "3900/9797 [==========>...................] - ETA: 13:58 - loss: 4.0305\n",
      "4000/9797 [===========>..................] - ETA: 13:44 - loss: 4.0313\n",
      "4100/9797 [===========>..................] - ETA: 13:30 - loss: 4.0319\n",
      "4200/9797 [===========>..................] - ETA: 13:16 - loss: 4.0298\n",
      "4300/9797 [============>.................] - ETA: 13:01 - loss: 4.0332\n",
      "4400/9797 [============>.................] - ETA: 12:47 - loss: 4.0341\n",
      "4500/9797 [============>.................] - ETA: 12:33 - loss: 4.0332\n",
      "4600/9797 [=============>................] - ETA: 12:19 - loss: 4.0300\n",
      "4700/9797 [=============>................] - ETA: 12:04 - loss: 4.0282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800/9797 [=============>................] - ETA: 11:50 - loss: 4.0296\n",
      "4900/9797 [==============>...............] - ETA: 11:36 - loss: 4.0289\n",
      "5000/9797 [==============>...............] - ETA: 11:22 - loss: 4.0277\n",
      "5100/9797 [==============>...............] - ETA: 11:08 - loss: 4.0279\n",
      "5200/9797 [==============>...............] - ETA: 10:53 - loss: 4.0261\n",
      "5300/9797 [===============>..............] - ETA: 10:39 - loss: 4.0267\n",
      "5400/9797 [===============>..............] - ETA: 10:25 - loss: 4.0273\n",
      "5500/9797 [===============>..............] - ETA: 10:11 - loss: 4.0250\n",
      "5600/9797 [================>.............] - ETA: 9:56 - loss: 4.0256\n",
      "5700/9797 [================>.............] - ETA: 9:42 - loss: 4.0228\n",
      "5800/9797 [================>.............] - ETA: 9:28 - loss: 4.0218\n",
      "5900/9797 [=================>............] - ETA: 9:14 - loss: 4.0211\n",
      "6000/9797 [=================>............] - ETA: 8:59 - loss: 4.0197\n",
      "6100/9797 [=================>............] - ETA: 8:45 - loss: 4.0206\n",
      "6200/9797 [=================>............] - ETA: 8:31 - loss: 4.0193\n",
      "6300/9797 [==================>...........] - ETA: 8:17 - loss: 4.0178\n",
      "6400/9797 [==================>...........] - ETA: 8:03 - loss: 4.0173\n",
      "6500/9797 [==================>...........] - ETA: 7:48 - loss: 4.0180\n",
      "6600/9797 [===================>..........] - ETA: 7:34 - loss: 4.0167\n",
      "6700/9797 [===================>..........] - ETA: 7:20 - loss: 4.0154\n",
      "6800/9797 [===================>..........] - ETA: 7:06 - loss: 4.0153\n",
      "6900/9797 [====================>.........] - ETA: 6:51 - loss: 4.0165\n",
      "7000/9797 [====================>.........] - ETA: 6:37 - loss: 4.0161\n",
      "7100/9797 [====================>.........] - ETA: 6:23 - loss: 4.0146\n",
      "7200/9797 [=====================>........] - ETA: 6:09 - loss: 4.0141\n",
      "7300/9797 [=====================>........] - ETA: 5:55 - loss: 4.0127\n",
      "7400/9797 [=====================>........] - ETA: 5:40 - loss: 4.0125\n",
      "7500/9797 [=====================>........] - ETA: 5:26 - loss: 4.0119\n",
      "7600/9797 [======================>.......] - ETA: 5:12 - loss: 4.0105\n",
      "7700/9797 [======================>.......] - ETA: 4:58 - loss: 4.0089\n",
      "7800/9797 [======================>.......] - ETA: 4:43 - loss: 4.0081\n",
      "7900/9797 [=======================>......] - ETA: 4:29 - loss: 4.0070\n",
      "8000/9797 [=======================>......] - ETA: 4:15 - loss: 4.0063\n",
      "8100/9797 [=======================>......] - ETA: 4:01 - loss: 4.0056\n",
      "8200/9797 [========================>.....] - ETA: 3:47 - loss: 4.0052\n",
      "8300/9797 [========================>.....] - ETA: 3:32 - loss: 4.0044\n",
      "8400/9797 [========================>.....] - ETA: 3:18 - loss: 4.0034\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 4.0027\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 4.0015\n",
      "8700/9797 [=========================>....] - ETA: 2:35 - loss: 4.0009\n",
      "8800/9797 [=========================>....] - ETA: 2:21 - loss: 4.0007\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 4.0001\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 3.9994\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 3.9992\n",
      "9200/9797 [===========================>..] - ETA: 1:24 - loss: 3.9984\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 3.9960\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 3.9956\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 3.9945\n",
      "9600/9797 [============================>.] - ETA: 28s - loss: 3.9940\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 3.9927\n",
      "Epoch 00014: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_014.tlt\n",
      "coleoptera    AP    0.75606\n",
      "diptera       AP    0.83374\n",
      "geometridae   AP    0.88702\n",
      "hemiptera     AP    0.78268\n",
      "hymenoptera   AP    0.79647\n",
      "noctuidae     AP    0.89525\n",
      "odonata       AP    0.86239\n",
      "orthoptera    AP    0.80348\n",
      "trichoptera   AP    0.87339\n",
      "              mAP   0.83227\n",
      "Validation loss: 1.916596524010134\n",
      "Epoch 15/20\n",
      " 100/9797 [..............................] - ETA: 22:47 - loss: 3.7833\n",
      " 200/9797 [..............................] - ETA: 22:36 - loss: 3.8724\n",
      " 300/9797 [..............................] - ETA: 22:25 - loss: 3.8676\n",
      " 400/9797 [>.............................] - ETA: 22:12 - loss: 3.8562\n",
      " 500/9797 [>.............................] - ETA: 21:59 - loss: 3.8563\n",
      " 600/9797 [>.............................] - ETA: 21:43 - loss: 3.8456\n",
      " 700/9797 [=>............................] - ETA: 21:28 - loss: 3.8527\n",
      " 800/9797 [=>............................] - ETA: 21:15 - loss: 3.8608\n",
      " 900/9797 [=>............................] - ETA: 21:01 - loss: 3.8720\n",
      "1000/9797 [==>...........................] - ETA: 20:47 - loss: 3.8776\n",
      "1100/9797 [==>...........................] - ETA: 20:33 - loss: 3.8814\n",
      "1200/9797 [==>...........................] - ETA: 20:20 - loss: 3.8804\n",
      "1300/9797 [==>...........................] - ETA: 20:05 - loss: 3.8809\n",
      "1400/9797 [===>..........................] - ETA: 19:51 - loss: 3.8867\n",
      "1500/9797 [===>..........................] - ETA: 19:37 - loss: 3.8801\n",
      "1600/9797 [===>..........................] - ETA: 19:23 - loss: 3.8764\n",
      "1700/9797 [====>.........................] - ETA: 19:09 - loss: 3.8778\n",
      "1800/9797 [====>.........................] - ETA: 18:55 - loss: 3.8806\n",
      "1900/9797 [====>.........................] - ETA: 18:41 - loss: 3.8884\n",
      "2000/9797 [=====>........................] - ETA: 18:26 - loss: 3.8855\n",
      "2100/9797 [=====>........................] - ETA: 18:12 - loss: 3.8807\n",
      "2200/9797 [=====>........................] - ETA: 17:58 - loss: 3.8787\n",
      "2300/9797 [======>.......................] - ETA: 17:44 - loss: 3.8818\n",
      "2400/9797 [======>.......................] - ETA: 17:30 - loss: 3.8819\n",
      "2500/9797 [======>.......................] - ETA: 17:16 - loss: 3.8791\n",
      "2600/9797 [======>.......................] - ETA: 17:01 - loss: 3.8811\n",
      "2700/9797 [=======>......................] - ETA: 16:47 - loss: 3.8801\n",
      "2800/9797 [=======>......................] - ETA: 16:33 - loss: 3.8832\n",
      "2900/9797 [=======>......................] - ETA: 16:19 - loss: 3.8864\n",
      "3000/9797 [========>.....................] - ETA: 16:04 - loss: 3.8865\n",
      "3100/9797 [========>.....................] - ETA: 15:50 - loss: 3.8862\n",
      "3200/9797 [========>.....................] - ETA: 15:36 - loss: 3.8890\n",
      "3300/9797 [=========>....................] - ETA: 15:22 - loss: 3.8895\n",
      "3400/9797 [=========>....................] - ETA: 15:08 - loss: 3.8891\n",
      "3500/9797 [=========>....................] - ETA: 14:53 - loss: 3.8896\n",
      "3600/9797 [==========>...................] - ETA: 14:39 - loss: 3.8898\n",
      "3700/9797 [==========>...................] - ETA: 14:25 - loss: 3.8909\n",
      "3800/9797 [==========>...................] - ETA: 14:11 - loss: 3.8908\n",
      "3900/9797 [==========>...................] - ETA: 13:57 - loss: 3.8890\n",
      "4000/9797 [===========>..................] - ETA: 13:43 - loss: 3.8874\n",
      "4100/9797 [===========>..................] - ETA: 13:28 - loss: 3.8839\n",
      "4200/9797 [===========>..................] - ETA: 13:14 - loss: 3.8846\n",
      "4300/9797 [============>.................] - ETA: 13:00 - loss: 3.8854\n",
      "4400/9797 [============>.................] - ETA: 12:46 - loss: 3.8859\n",
      "4500/9797 [============>.................] - ETA: 12:32 - loss: 3.8830\n",
      "4600/9797 [=============>................] - ETA: 12:18 - loss: 3.8841\n",
      "4700/9797 [=============>................] - ETA: 12:03 - loss: 3.8817\n",
      "4800/9797 [=============>................] - ETA: 11:49 - loss: 3.8818\n",
      "4900/9797 [==============>...............] - ETA: 11:35 - loss: 3.8793\n",
      "5000/9797 [==============>...............] - ETA: 11:21 - loss: 3.8786\n",
      "5100/9797 [==============>...............] - ETA: 11:07 - loss: 3.8784\n",
      "5200/9797 [==============>...............] - ETA: 10:52 - loss: 3.8779\n",
      "5300/9797 [===============>..............] - ETA: 10:38 - loss: 3.8785\n",
      "5400/9797 [===============>..............] - ETA: 10:24 - loss: 3.8786\n",
      "5500/9797 [===============>..............] - ETA: 10:10 - loss: 3.8778\n",
      "5600/9797 [================>.............] - ETA: 9:56 - loss: 3.8765\n",
      "5700/9797 [================>.............] - ETA: 9:41 - loss: 3.8750\n",
      "5800/9797 [================>.............] - ETA: 9:27 - loss: 3.8735\n",
      "5900/9797 [=================>............] - ETA: 9:13 - loss: 3.8729\n",
      "6000/9797 [=================>............] - ETA: 8:59 - loss: 3.8717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6100/9797 [=================>............] - ETA: 8:45 - loss: 3.8696\n",
      "6200/9797 [=================>............] - ETA: 8:30 - loss: 3.8695\n",
      "6300/9797 [==================>...........] - ETA: 8:16 - loss: 3.8679\n",
      "6400/9797 [==================>...........] - ETA: 8:02 - loss: 3.8670\n",
      "6500/9797 [==================>...........] - ETA: 7:48 - loss: 3.8651\n",
      "6600/9797 [===================>..........] - ETA: 7:33 - loss: 3.8638\n",
      "6700/9797 [===================>..........] - ETA: 7:19 - loss: 3.8622\n",
      "6800/9797 [===================>..........] - ETA: 7:05 - loss: 3.8630\n",
      "6900/9797 [====================>.........] - ETA: 6:51 - loss: 3.8607\n",
      "7000/9797 [====================>.........] - ETA: 6:37 - loss: 3.8606\n",
      "7100/9797 [====================>.........] - ETA: 6:22 - loss: 3.8606\n",
      "7200/9797 [=====================>........] - ETA: 6:08 - loss: 3.8613\n",
      "7300/9797 [=====================>........] - ETA: 5:54 - loss: 3.8588\n",
      "7400/9797 [=====================>........] - ETA: 5:40 - loss: 3.8577\n",
      "7500/9797 [=====================>........] - ETA: 5:26 - loss: 3.8576\n",
      "7600/9797 [======================>.......] - ETA: 5:12 - loss: 3.8568\n",
      "7700/9797 [======================>.......] - ETA: 4:57 - loss: 3.8558\n",
      "7800/9797 [======================>.......] - ETA: 4:43 - loss: 3.8567\n",
      "7900/9797 [=======================>......] - ETA: 4:29 - loss: 3.8582\n",
      "8000/9797 [=======================>......] - ETA: 4:15 - loss: 3.8589\n",
      "8100/9797 [=======================>......] - ETA: 4:00 - loss: 3.8592\n",
      "8200/9797 [========================>.....] - ETA: 3:46 - loss: 3.8585\n",
      "8300/9797 [========================>.....] - ETA: 3:32 - loss: 3.8578\n",
      "8400/9797 [========================>.....] - ETA: 3:18 - loss: 3.8580\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 3.8559\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 3.8546\n",
      "8700/9797 [=========================>....] - ETA: 2:35 - loss: 3.8531\n",
      "8800/9797 [=========================>....] - ETA: 2:21 - loss: 3.8514\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 3.8485\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 3.8467\n",
      "9100/9797 [==========================>...] - ETA: 1:38 - loss: 3.8460\n",
      "9200/9797 [===========================>..] - ETA: 1:24 - loss: 3.8442\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 3.8442\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 3.8429\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 3.8428\n",
      "9600/9797 [============================>.] - ETA: 27s - loss: 3.8416\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 3.8407\n",
      "Epoch 00015: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_015.tlt\n",
      "coleoptera    AP    0.80087\n",
      "diptera       AP    0.80718\n",
      "geometridae   AP    0.8919\n",
      "hemiptera     AP    0.80741\n",
      "hymenoptera   AP    0.7739\n",
      "noctuidae     AP    0.88992\n",
      "odonata       AP    0.88722\n",
      "orthoptera    AP    0.85228\n",
      "trichoptera   AP    0.88421\n",
      "              mAP   0.84388\n",
      "Validation loss: 1.8186113772298635\n",
      "Epoch 16/20\n",
      " 100/9797 [..............................] - ETA: 22:57 - loss: 3.6164\n",
      " 200/9797 [..............................] - ETA: 22:42 - loss: 3.7223\n",
      " 300/9797 [..............................] - ETA: 22:26 - loss: 3.7085\n",
      " 400/9797 [>.............................] - ETA: 22:13 - loss: 3.7137\n",
      " 500/9797 [>.............................] - ETA: 21:59 - loss: 3.7295\n",
      " 600/9797 [>.............................] - ETA: 21:46 - loss: 3.7337\n",
      " 700/9797 [=>............................] - ETA: 21:32 - loss: 3.7346\n",
      " 800/9797 [=>............................] - ETA: 21:18 - loss: 3.7354\n",
      " 900/9797 [=>............................] - ETA: 21:03 - loss: 3.7380\n",
      "1000/9797 [==>...........................] - ETA: 20:50 - loss: 3.7385\n",
      "1100/9797 [==>...........................] - ETA: 20:35 - loss: 3.7374\n",
      "1200/9797 [==>...........................] - ETA: 20:21 - loss: 3.7387\n",
      "1300/9797 [==>...........................] - ETA: 20:07 - loss: 3.7392\n",
      "1400/9797 [===>..........................] - ETA: 19:53 - loss: 3.7309\n",
      "1500/9797 [===>..........................] - ETA: 19:39 - loss: 3.7254\n",
      "1600/9797 [===>..........................] - ETA: 19:24 - loss: 3.7267\n",
      "1700/9797 [====>.........................] - ETA: 19:10 - loss: 3.7295\n",
      "1800/9797 [====>.........................] - ETA: 18:56 - loss: 3.7281\n",
      "1900/9797 [====>.........................] - ETA: 18:41 - loss: 3.7312\n",
      "2000/9797 [=====>........................] - ETA: 18:27 - loss: 3.7325\n",
      "2100/9797 [=====>........................] - ETA: 18:13 - loss: 3.7332\n",
      "2200/9797 [=====>........................] - ETA: 17:59 - loss: 3.7356\n",
      "2300/9797 [======>.......................] - ETA: 17:45 - loss: 3.7370\n",
      "2400/9797 [======>.......................] - ETA: 17:30 - loss: 3.7344\n",
      "2500/9797 [======>.......................] - ETA: 17:16 - loss: 3.7315\n",
      "2600/9797 [======>.......................] - ETA: 17:02 - loss: 3.7294\n",
      "2700/9797 [=======>......................] - ETA: 16:48 - loss: 3.7266\n",
      "2800/9797 [=======>......................] - ETA: 16:34 - loss: 3.7242\n",
      "2900/9797 [=======>......................] - ETA: 16:19 - loss: 3.7245\n",
      "3000/9797 [========>.....................] - ETA: 16:05 - loss: 3.7278\n",
      "3100/9797 [========>.....................] - ETA: 15:51 - loss: 3.7251\n",
      "3200/9797 [========>.....................] - ETA: 15:37 - loss: 3.7246\n",
      "3300/9797 [=========>....................] - ETA: 15:23 - loss: 3.7271\n",
      "3400/9797 [=========>....................] - ETA: 15:08 - loss: 3.7266\n",
      "3500/9797 [=========>....................] - ETA: 14:54 - loss: 3.7201\n",
      "3600/9797 [==========>...................] - ETA: 14:40 - loss: 3.7222\n",
      "3700/9797 [==========>...................] - ETA: 14:26 - loss: 3.7208\n",
      "3800/9797 [==========>...................] - ETA: 14:12 - loss: 3.7214\n",
      "3900/9797 [==========>...................] - ETA: 13:57 - loss: 3.7197\n",
      "4000/9797 [===========>..................] - ETA: 13:43 - loss: 3.7155\n",
      "4100/9797 [===========>..................] - ETA: 13:29 - loss: 3.7151\n",
      "4200/9797 [===========>..................] - ETA: 13:15 - loss: 3.7140\n",
      "4300/9797 [============>.................] - ETA: 13:01 - loss: 3.7137\n",
      "4400/9797 [============>.................] - ETA: 12:46 - loss: 3.7104\n",
      "4500/9797 [============>.................] - ETA: 12:32 - loss: 3.7094\n",
      "4600/9797 [=============>................] - ETA: 12:18 - loss: 3.7090\n",
      "4700/9797 [=============>................] - ETA: 12:04 - loss: 3.7093\n",
      "4800/9797 [=============>................] - ETA: 11:50 - loss: 3.7094\n",
      "4900/9797 [==============>...............] - ETA: 11:35 - loss: 3.7087\n",
      "5000/9797 [==============>...............] - ETA: 11:21 - loss: 3.7083\n",
      "5100/9797 [==============>...............] - ETA: 11:07 - loss: 3.7084\n",
      "5200/9797 [==============>...............] - ETA: 10:53 - loss: 3.7085\n",
      "5300/9797 [===============>..............] - ETA: 10:39 - loss: 3.7061\n",
      "5400/9797 [===============>..............] - ETA: 10:24 - loss: 3.7069\n",
      "5500/9797 [===============>..............] - ETA: 10:10 - loss: 3.7080\n",
      "5600/9797 [================>.............] - ETA: 9:56 - loss: 3.7079\n",
      "5700/9797 [================>.............] - ETA: 9:42 - loss: 3.7087\n",
      "5800/9797 [================>.............] - ETA: 9:28 - loss: 3.7088\n",
      "5900/9797 [=================>............] - ETA: 9:13 - loss: 3.7093\n",
      "6000/9797 [=================>............] - ETA: 8:59 - loss: 3.7087\n",
      "6100/9797 [=================>............] - ETA: 8:45 - loss: 3.7064\n",
      "6200/9797 [=================>............] - ETA: 8:31 - loss: 3.7058\n",
      "6300/9797 [==================>...........] - ETA: 8:17 - loss: 3.7050\n",
      "6400/9797 [==================>...........] - ETA: 8:02 - loss: 3.7053\n",
      "6500/9797 [==================>...........] - ETA: 7:48 - loss: 3.7035\n",
      "6600/9797 [===================>..........] - ETA: 7:34 - loss: 3.7038\n",
      "6700/9797 [===================>..........] - ETA: 7:20 - loss: 3.7023\n",
      "6800/9797 [===================>..........] - ETA: 7:05 - loss: 3.7023\n",
      "6900/9797 [====================>.........] - ETA: 6:51 - loss: 3.7028\n",
      "7000/9797 [====================>.........] - ETA: 6:37 - loss: 3.7027\n",
      "7100/9797 [====================>.........] - ETA: 6:23 - loss: 3.7025\n",
      "7200/9797 [=====================>........] - ETA: 6:09 - loss: 3.7023\n",
      "7300/9797 [=====================>........] - ETA: 5:54 - loss: 3.7015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400/9797 [=====================>........] - ETA: 5:40 - loss: 3.7018\n",
      "7500/9797 [=====================>........] - ETA: 5:26 - loss: 3.7006\n",
      "7600/9797 [======================>.......] - ETA: 5:12 - loss: 3.7000\n",
      "7700/9797 [======================>.......] - ETA: 4:58 - loss: 3.6996\n",
      "7800/9797 [======================>.......] - ETA: 4:43 - loss: 3.6987\n",
      "7900/9797 [=======================>......] - ETA: 4:29 - loss: 3.6983\n",
      "8000/9797 [=======================>......] - ETA: 4:15 - loss: 3.6962\n",
      "8100/9797 [=======================>......] - ETA: 4:01 - loss: 3.6951\n",
      "8200/9797 [========================>.....] - ETA: 3:46 - loss: 3.6942\n",
      "8300/9797 [========================>.....] - ETA: 3:32 - loss: 3.6943\n",
      "8400/9797 [========================>.....] - ETA: 3:18 - loss: 3.6937\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 3.6938\n",
      "8600/9797 [=========================>....] - ETA: 2:50 - loss: 3.6942\n",
      "8700/9797 [=========================>....] - ETA: 2:35 - loss: 3.6941\n",
      "8800/9797 [=========================>....] - ETA: 2:21 - loss: 3.6940\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 3.6944\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 3.6946\n",
      "9100/9797 [==========================>...] - ETA: 1:39 - loss: 3.6945\n",
      "9200/9797 [===========================>..] - ETA: 1:24 - loss: 3.6928\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 3.6920\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 3.6915\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 3.6906\n",
      "9600/9797 [============================>.] - ETA: 27s - loss: 3.6898\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 3.6894\n",
      "Epoch 00016: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_016.tlt\n",
      "coleoptera    AP    0.79229\n",
      "diptera       AP    0.84688\n",
      "geometridae   AP    0.89438\n",
      "hemiptera     AP    0.80533\n",
      "hymenoptera   AP    0.81723\n",
      "noctuidae     AP    0.89033\n",
      "odonata       AP    0.89088\n",
      "orthoptera    AP    0.85536\n",
      "trichoptera   AP    0.89523\n",
      "              mAP   0.85421\n",
      "Validation loss: 1.745060818501717\n",
      "Epoch 17/20\n",
      " 100/9797 [..............................] - ETA: 22:40 - loss: 3.5968\n",
      " 200/9797 [..............................] - ETA: 22:34 - loss: 3.6121\n",
      " 300/9797 [..............................] - ETA: 22:20 - loss: 3.6149\n",
      " 400/9797 [>.............................] - ETA: 22:07 - loss: 3.6296\n",
      " 500/9797 [>.............................] - ETA: 21:52 - loss: 3.6096\n",
      " 600/9797 [>.............................] - ETA: 21:39 - loss: 3.6148\n",
      " 700/9797 [=>............................] - ETA: 21:25 - loss: 3.6046\n",
      " 800/9797 [=>............................] - ETA: 21:12 - loss: 3.6004\n",
      " 900/9797 [=>............................] - ETA: 20:57 - loss: 3.5977\n",
      "1000/9797 [==>...........................] - ETA: 20:44 - loss: 3.6011\n",
      "1100/9797 [==>...........................] - ETA: 20:29 - loss: 3.6033\n",
      "1200/9797 [==>...........................] - ETA: 20:16 - loss: 3.6109\n",
      "1300/9797 [==>...........................] - ETA: 20:02 - loss: 3.6159\n",
      "1400/9797 [===>..........................] - ETA: 19:48 - loss: 3.6100\n",
      "1500/9797 [===>..........................] - ETA: 19:34 - loss: 3.6066\n",
      "1600/9797 [===>..........................] - ETA: 19:20 - loss: 3.6034\n",
      "1700/9797 [====>.........................] - ETA: 19:06 - loss: 3.5980\n",
      "1800/9797 [====>.........................] - ETA: 18:52 - loss: 3.5988\n",
      "1900/9797 [====>.........................] - ETA: 18:38 - loss: 3.6049\n",
      "2000/9797 [=====>........................] - ETA: 18:24 - loss: 3.6015\n",
      "2100/9797 [=====>........................] - ETA: 18:10 - loss: 3.6004\n",
      "2200/9797 [=====>........................] - ETA: 17:56 - loss: 3.6028\n",
      "2300/9797 [======>.......................] - ETA: 17:42 - loss: 3.6050\n",
      "2400/9797 [======>.......................] - ETA: 17:28 - loss: 3.6051\n",
      "2500/9797 [======>.......................] - ETA: 17:14 - loss: 3.6036\n",
      "2600/9797 [======>.......................] - ETA: 17:00 - loss: 3.6042\n",
      "2700/9797 [=======>......................] - ETA: 16:46 - loss: 3.6029\n",
      "2800/9797 [=======>......................] - ETA: 16:32 - loss: 3.6030\n",
      "2900/9797 [=======>......................] - ETA: 16:18 - loss: 3.5998\n",
      "3000/9797 [========>.....................] - ETA: 16:03 - loss: 3.5967\n",
      "3100/9797 [========>.....................] - ETA: 15:49 - loss: 3.5981\n",
      "3200/9797 [========>.....................] - ETA: 15:35 - loss: 3.5977\n",
      "3300/9797 [=========>....................] - ETA: 15:21 - loss: 3.5950\n",
      "3400/9797 [=========>....................] - ETA: 15:07 - loss: 3.5964\n",
      "3500/9797 [=========>....................] - ETA: 14:53 - loss: 3.5934\n",
      "3600/9797 [==========>...................] - ETA: 14:38 - loss: 3.5924\n",
      "3700/9797 [==========>...................] - ETA: 14:24 - loss: 3.5898\n",
      "3800/9797 [==========>...................] - ETA: 14:10 - loss: 3.5884\n",
      "3900/9797 [==========>...................] - ETA: 13:56 - loss: 3.5894\n",
      "4000/9797 [===========>..................] - ETA: 13:42 - loss: 3.5898\n",
      "4100/9797 [===========>..................] - ETA: 13:27 - loss: 3.5896\n",
      "4200/9797 [===========>..................] - ETA: 13:13 - loss: 3.5877\n",
      "4300/9797 [============>.................] - ETA: 12:59 - loss: 3.5867\n",
      "4400/9797 [============>.................] - ETA: 12:45 - loss: 3.5860\n",
      "4500/9797 [============>.................] - ETA: 12:31 - loss: 3.5841\n",
      "4600/9797 [=============>................] - ETA: 12:17 - loss: 3.5837\n",
      "4700/9797 [=============>................] - ETA: 12:02 - loss: 3.5813\n",
      "4800/9797 [=============>................] - ETA: 11:48 - loss: 3.5784\n",
      "4900/9797 [==============>...............] - ETA: 11:34 - loss: 3.5781\n",
      "5000/9797 [==============>...............] - ETA: 11:20 - loss: 3.5789\n",
      "5100/9797 [==============>...............] - ETA: 11:06 - loss: 3.5792\n",
      "5200/9797 [==============>...............] - ETA: 10:52 - loss: 3.5813\n",
      "5300/9797 [===============>..............] - ETA: 10:37 - loss: 3.5798\n",
      "5400/9797 [===============>..............] - ETA: 10:23 - loss: 3.5810\n",
      "5500/9797 [===============>..............] - ETA: 10:09 - loss: 3.5801\n",
      "5600/9797 [================>.............] - ETA: 9:55 - loss: 3.5785\n",
      "5700/9797 [================>.............] - ETA: 9:41 - loss: 3.5798\n",
      "5800/9797 [================>.............] - ETA: 9:27 - loss: 3.5792\n",
      "5900/9797 [=================>............] - ETA: 9:12 - loss: 3.5786\n",
      "6000/9797 [=================>............] - ETA: 8:58 - loss: 3.5762\n",
      "6100/9797 [=================>............] - ETA: 8:44 - loss: 3.5757\n",
      "6200/9797 [=================>............] - ETA: 8:30 - loss: 3.5736\n",
      "6300/9797 [==================>...........] - ETA: 8:16 - loss: 3.5741\n",
      "6400/9797 [==================>...........] - ETA: 8:02 - loss: 3.5732\n",
      "6500/9797 [==================>...........] - ETA: 7:47 - loss: 3.5727\n",
      "6600/9797 [===================>..........] - ETA: 7:33 - loss: 3.5719\n",
      "6700/9797 [===================>..........] - ETA: 7:19 - loss: 3.5712\n",
      "6800/9797 [===================>..........] - ETA: 7:05 - loss: 3.5712\n",
      "6900/9797 [====================>.........] - ETA: 6:51 - loss: 3.5708\n",
      "7000/9797 [====================>.........] - ETA: 6:36 - loss: 3.5709\n",
      "7100/9797 [====================>.........] - ETA: 6:22 - loss: 3.5693\n",
      "7200/9797 [=====================>........] - ETA: 6:08 - loss: 3.5694\n",
      "7300/9797 [=====================>........] - ETA: 5:54 - loss: 3.5689\n",
      "7400/9797 [=====================>........] - ETA: 5:40 - loss: 3.5681\n",
      "7500/9797 [=====================>........] - ETA: 5:25 - loss: 3.5667\n",
      "7600/9797 [======================>.......] - ETA: 5:11 - loss: 3.5656\n",
      "7700/9797 [======================>.......] - ETA: 4:57 - loss: 3.5640\n",
      "7800/9797 [======================>.......] - ETA: 4:43 - loss: 3.5647\n",
      "7900/9797 [=======================>......] - ETA: 4:29 - loss: 3.5624\n",
      "8000/9797 [=======================>......] - ETA: 4:14 - loss: 3.5627\n",
      "8100/9797 [=======================>......] - ETA: 4:00 - loss: 3.5625\n",
      "8200/9797 [========================>.....] - ETA: 3:46 - loss: 3.5628\n",
      "8300/9797 [========================>.....] - ETA: 3:32 - loss: 3.5622\n",
      "8400/9797 [========================>.....] - ETA: 3:18 - loss: 3.5599\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 3.5586\n",
      "8600/9797 [=========================>....] - ETA: 2:49 - loss: 3.5583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8700/9797 [=========================>....] - ETA: 2:35 - loss: 3.5583\n",
      "8800/9797 [=========================>....] - ETA: 2:21 - loss: 3.5571\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 3.5571\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 3.5559\n",
      "9100/9797 [==========================>...] - ETA: 1:38 - loss: 3.5557\n",
      "9200/9797 [===========================>..] - ETA: 1:24 - loss: 3.5543\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 3.5536\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 3.5533\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 3.5525\n",
      "9600/9797 [============================>.] - ETA: 27s - loss: 3.5522\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 3.5516\n",
      "Epoch 00017: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_017.tlt\n",
      "coleoptera    AP    0.79861\n",
      "diptera       AP    0.83618\n",
      "geometridae   AP    0.89822\n",
      "hemiptera     AP    0.81545\n",
      "hymenoptera   AP    0.82246\n",
      "noctuidae     AP    0.90279\n",
      "odonata       AP    0.89081\n",
      "orthoptera    AP    0.82884\n",
      "trichoptera   AP    0.89019\n",
      "              mAP   0.85373\n",
      "Validation loss: 1.7361444032173066\n",
      "Epoch 18/20\n",
      " 100/9797 [..............................] - ETA: 22:51 - loss: 3.5097\n",
      " 200/9797 [..............................] - ETA: 22:36 - loss: 3.4721\n",
      " 300/9797 [..............................] - ETA: 22:25 - loss: 3.4860\n",
      " 400/9797 [>.............................] - ETA: 22:12 - loss: 3.4792\n",
      " 500/9797 [>.............................] - ETA: 21:59 - loss: 3.4698\n",
      " 600/9797 [>.............................] - ETA: 21:45 - loss: 3.4874\n",
      " 700/9797 [=>............................] - ETA: 21:31 - loss: 3.4872\n",
      " 800/9797 [=>............................] - ETA: 21:16 - loss: 3.4758\n",
      " 900/9797 [=>............................] - ETA: 21:02 - loss: 3.4794\n",
      "1000/9797 [==>...........................] - ETA: 20:48 - loss: 3.4857\n",
      "1100/9797 [==>...........................] - ETA: 20:34 - loss: 3.4868\n",
      "1200/9797 [==>...........................] - ETA: 20:19 - loss: 3.4783\n",
      "1300/9797 [==>...........................] - ETA: 20:05 - loss: 3.4758\n",
      "1400/9797 [===>..........................] - ETA: 19:49 - loss: 3.4871\n",
      "1500/9797 [===>..........................] - ETA: 19:35 - loss: 3.4867\n",
      "1600/9797 [===>..........................] - ETA: 19:21 - loss: 3.4788\n",
      "1700/9797 [====>.........................] - ETA: 19:08 - loss: 3.4813\n",
      "1800/9797 [====>.........................] - ETA: 18:53 - loss: 3.4787\n",
      "1900/9797 [====>.........................] - ETA: 18:39 - loss: 3.4774\n",
      "2000/9797 [=====>........................] - ETA: 18:25 - loss: 3.4755\n",
      "2100/9797 [=====>........................] - ETA: 18:11 - loss: 3.4721\n",
      "2200/9797 [=====>........................] - ETA: 17:57 - loss: 3.4736\n",
      "2300/9797 [======>.......................] - ETA: 17:42 - loss: 3.4726\n",
      "2400/9797 [======>.......................] - ETA: 17:28 - loss: 3.4764\n",
      "2500/9797 [======>.......................] - ETA: 17:14 - loss: 3.4759\n",
      "2600/9797 [======>.......................] - ETA: 17:00 - loss: 3.4728\n",
      "2700/9797 [=======>......................] - ETA: 16:46 - loss: 3.4741\n",
      "2800/9797 [=======>......................] - ETA: 16:31 - loss: 3.4738\n",
      "2900/9797 [=======>......................] - ETA: 16:17 - loss: 3.4703\n",
      "3000/9797 [========>.....................] - ETA: 16:03 - loss: 3.4664\n",
      "3100/9797 [========>.....................] - ETA: 15:49 - loss: 3.4652\n",
      "3200/9797 [========>.....................] - ETA: 15:34 - loss: 3.4651\n",
      "3300/9797 [=========>....................] - ETA: 15:20 - loss: 3.4638\n",
      "3400/9797 [=========>....................] - ETA: 15:06 - loss: 3.4617\n",
      "3500/9797 [=========>....................] - ETA: 14:52 - loss: 3.4622\n",
      "3600/9797 [==========>...................] - ETA: 14:38 - loss: 3.4621\n",
      "3700/9797 [==========>...................] - ETA: 14:24 - loss: 3.4607\n",
      "3800/9797 [==========>...................] - ETA: 14:10 - loss: 3.4576\n",
      "3900/9797 [==========>...................] - ETA: 13:55 - loss: 3.4566\n",
      "4000/9797 [===========>..................] - ETA: 13:41 - loss: 3.4570\n",
      "4100/9797 [===========>..................] - ETA: 13:27 - loss: 3.4567\n",
      "4200/9797 [===========>..................] - ETA: 13:13 - loss: 3.4564\n",
      "4300/9797 [============>.................] - ETA: 12:59 - loss: 3.4561\n",
      "4400/9797 [============>.................] - ETA: 12:45 - loss: 3.4545\n",
      "4500/9797 [============>.................] - ETA: 12:31 - loss: 3.4520\n",
      "4600/9797 [=============>................] - ETA: 12:17 - loss: 3.4522\n",
      "4700/9797 [=============>................] - ETA: 12:02 - loss: 3.4524\n",
      "4800/9797 [=============>................] - ETA: 11:48 - loss: 3.4519\n",
      "4900/9797 [==============>...............] - ETA: 11:34 - loss: 3.4510\n",
      "5000/9797 [==============>...............] - ETA: 11:20 - loss: 3.4500\n",
      "5100/9797 [==============>...............] - ETA: 11:06 - loss: 3.4506\n",
      "5200/9797 [==============>...............] - ETA: 10:51 - loss: 3.4490\n",
      "5300/9797 [===============>..............] - ETA: 10:37 - loss: 3.4473\n",
      "5400/9797 [===============>..............] - ETA: 10:23 - loss: 3.4475\n",
      "5500/9797 [===============>..............] - ETA: 10:09 - loss: 3.4467\n",
      "5600/9797 [================>.............] - ETA: 9:55 - loss: 3.4474\n",
      "5700/9797 [================>.............] - ETA: 9:41 - loss: 3.4462\n",
      "5800/9797 [================>.............] - ETA: 9:26 - loss: 3.4462\n",
      "5900/9797 [=================>............] - ETA: 9:12 - loss: 3.4453\n",
      "6000/9797 [=================>............] - ETA: 8:58 - loss: 3.4442\n",
      "6100/9797 [=================>............] - ETA: 8:44 - loss: 3.4441\n",
      "6200/9797 [=================>............] - ETA: 8:30 - loss: 3.4431\n",
      "6300/9797 [==================>...........] - ETA: 8:16 - loss: 3.4440\n",
      "6400/9797 [==================>...........] - ETA: 8:01 - loss: 3.4437\n",
      "6500/9797 [==================>...........] - ETA: 7:47 - loss: 3.4442\n",
      "6600/9797 [===================>..........] - ETA: 7:33 - loss: 3.4434\n",
      "6700/9797 [===================>..........] - ETA: 7:19 - loss: 3.4430\n",
      "6800/9797 [===================>..........] - ETA: 7:05 - loss: 3.4425\n",
      "6900/9797 [====================>.........] - ETA: 6:51 - loss: 3.4405\n",
      "7000/9797 [====================>.........] - ETA: 6:36 - loss: 3.4406\n",
      "7100/9797 [====================>.........] - ETA: 6:22 - loss: 3.4398\n",
      "7200/9797 [=====================>........] - ETA: 6:08 - loss: 3.4397\n",
      "7300/9797 [=====================>........] - ETA: 5:54 - loss: 3.4393\n",
      "7400/9797 [=====================>........] - ETA: 5:40 - loss: 3.4378\n",
      "7500/9797 [=====================>........] - ETA: 5:25 - loss: 3.4363\n",
      "7600/9797 [======================>.......] - ETA: 5:11 - loss: 3.4365\n",
      "7700/9797 [======================>.......] - ETA: 4:57 - loss: 3.4352\n",
      "7800/9797 [======================>.......] - ETA: 4:43 - loss: 3.4347\n",
      "7900/9797 [=======================>......] - ETA: 4:29 - loss: 3.4352\n",
      "8000/9797 [=======================>......] - ETA: 4:14 - loss: 3.4339\n",
      "8100/9797 [=======================>......] - ETA: 4:00 - loss: 3.4338\n",
      "8200/9797 [========================>.....] - ETA: 3:46 - loss: 3.4330\n",
      "8300/9797 [========================>.....] - ETA: 3:32 - loss: 3.4332\n",
      "8400/9797 [========================>.....] - ETA: 3:18 - loss: 3.4328\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 3.4324\n",
      "8600/9797 [=========================>....] - ETA: 2:49 - loss: 3.4316\n",
      "8700/9797 [=========================>....] - ETA: 2:35 - loss: 3.4316\n",
      "8800/9797 [=========================>....] - ETA: 2:21 - loss: 3.4312\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 3.4304\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 3.4297\n",
      "9100/9797 [==========================>...] - ETA: 1:38 - loss: 3.4284\n",
      "9200/9797 [===========================>..] - ETA: 1:24 - loss: 3.4278\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 3.4275\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 3.4270\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 3.4258\n",
      "9600/9797 [============================>.] - ETA: 27s - loss: 3.4256\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 3.4248\n",
      "Epoch 00018: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_018.tlt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coleoptera    AP    0.82892\n",
      "diptera       AP    0.86604\n",
      "geometridae   AP    0.90343\n",
      "hemiptera     AP    0.83363\n",
      "hymenoptera   AP    0.83146\n",
      "noctuidae     AP    0.89714\n",
      "odonata       AP    0.88424\n",
      "orthoptera    AP    0.88265\n",
      "trichoptera   AP    0.89287\n",
      "              mAP   0.86893\n",
      "Validation loss: 1.6764436389670823\n",
      "Epoch 19/20\n",
      " 100/9797 [..............................] - ETA: 22:50 - loss: 3.3701\n",
      " 200/9797 [..............................] - ETA: 22:38 - loss: 3.3937\n",
      " 300/9797 [..............................] - ETA: 22:26 - loss: 3.3948\n",
      " 400/9797 [>.............................] - ETA: 22:13 - loss: 3.3588\n",
      " 500/9797 [>.............................] - ETA: 22:00 - loss: 3.3476\n",
      " 600/9797 [>.............................] - ETA: 21:45 - loss: 3.3439\n",
      " 700/9797 [=>............................] - ETA: 21:30 - loss: 3.3470\n",
      " 800/9797 [=>............................] - ETA: 21:16 - loss: 3.3402\n",
      " 900/9797 [=>............................] - ETA: 21:02 - loss: 3.3359\n",
      "1000/9797 [==>...........................] - ETA: 20:47 - loss: 3.3388\n",
      "1100/9797 [==>...........................] - ETA: 20:33 - loss: 3.3430\n",
      "1200/9797 [==>...........................] - ETA: 20:19 - loss: 3.3442\n",
      "1300/9797 [==>...........................] - ETA: 20:05 - loss: 3.3518\n",
      "1400/9797 [===>..........................] - ETA: 19:50 - loss: 3.3503\n",
      "1500/9797 [===>..........................] - ETA: 19:37 - loss: 3.3467\n",
      "1600/9797 [===>..........................] - ETA: 19:22 - loss: 3.3446\n",
      "1700/9797 [====>.........................] - ETA: 19:08 - loss: 3.3423\n",
      "1800/9797 [====>.........................] - ETA: 18:54 - loss: 3.3394\n",
      "1900/9797 [====>.........................] - ETA: 18:40 - loss: 3.3469\n",
      "2000/9797 [=====>........................] - ETA: 18:26 - loss: 3.3434\n",
      "2100/9797 [=====>........................] - ETA: 18:12 - loss: 3.3396\n",
      "2200/9797 [=====>........................] - ETA: 17:57 - loss: 3.3394\n",
      "2300/9797 [======>.......................] - ETA: 17:43 - loss: 3.3375\n",
      "2400/9797 [======>.......................] - ETA: 17:29 - loss: 3.3360\n",
      "2500/9797 [======>.......................] - ETA: 17:15 - loss: 3.3379\n",
      "2600/9797 [======>.......................] - ETA: 17:00 - loss: 3.3373\n",
      "2700/9797 [=======>......................] - ETA: 16:46 - loss: 3.3344\n",
      "2800/9797 [=======>......................] - ETA: 16:32 - loss: 3.3318\n",
      "2900/9797 [=======>......................] - ETA: 16:18 - loss: 3.3322\n",
      "3000/9797 [========>.....................] - ETA: 16:04 - loss: 3.3332\n",
      "3100/9797 [========>.....................] - ETA: 15:49 - loss: 3.3355\n",
      "3200/9797 [========>.....................] - ETA: 15:35 - loss: 3.3357\n",
      "3300/9797 [=========>....................] - ETA: 15:21 - loss: 3.3353\n",
      "3400/9797 [=========>....................] - ETA: 15:07 - loss: 3.3353\n",
      "3500/9797 [=========>....................] - ETA: 14:53 - loss: 3.3360\n",
      "3600/9797 [==========>...................] - ETA: 14:38 - loss: 3.3361\n",
      "3700/9797 [==========>...................] - ETA: 14:24 - loss: 3.3349\n",
      "3800/9797 [==========>...................] - ETA: 14:10 - loss: 3.3323\n",
      "3900/9797 [==========>...................] - ETA: 13:56 - loss: 3.3321\n",
      "4000/9797 [===========>..................] - ETA: 13:41 - loss: 3.3365\n",
      "4100/9797 [===========>..................] - ETA: 13:27 - loss: 3.3355\n",
      "4200/9797 [===========>..................] - ETA: 13:13 - loss: 3.3330\n",
      "4300/9797 [============>.................] - ETA: 12:59 - loss: 3.3303\n",
      "4400/9797 [============>.................] - ETA: 12:45 - loss: 3.3301\n",
      "4500/9797 [============>.................] - ETA: 12:30 - loss: 3.3281\n",
      "4600/9797 [=============>................] - ETA: 12:16 - loss: 3.3271\n",
      "4700/9797 [=============>................] - ETA: 12:02 - loss: 3.3271\n",
      "4800/9797 [=============>................] - ETA: 11:48 - loss: 3.3261\n",
      "4900/9797 [==============>...............] - ETA: 11:34 - loss: 3.3259\n",
      "5000/9797 [==============>...............] - ETA: 11:20 - loss: 3.3256\n",
      "5100/9797 [==============>...............] - ETA: 11:05 - loss: 3.3242\n",
      "5200/9797 [==============>...............] - ETA: 10:51 - loss: 3.3232\n",
      "5300/9797 [===============>..............] - ETA: 10:37 - loss: 3.3214\n",
      "5400/9797 [===============>..............] - ETA: 10:23 - loss: 3.3215\n",
      "5500/9797 [===============>..............] - ETA: 10:09 - loss: 3.3200\n",
      "5600/9797 [================>.............] - ETA: 9:54 - loss: 3.3190\n",
      "5700/9797 [================>.............] - ETA: 9:40 - loss: 3.3181\n",
      "5800/9797 [================>.............] - ETA: 9:26 - loss: 3.3175\n",
      "5900/9797 [=================>............] - ETA: 9:12 - loss: 3.3180\n",
      "6000/9797 [=================>............] - ETA: 8:58 - loss: 3.3183\n",
      "6100/9797 [=================>............] - ETA: 8:44 - loss: 3.3182\n",
      "6200/9797 [=================>............] - ETA: 8:29 - loss: 3.3184\n",
      "6300/9797 [==================>...........] - ETA: 8:15 - loss: 3.3171\n",
      "6400/9797 [==================>...........] - ETA: 8:01 - loss: 3.3169\n",
      "6500/9797 [==================>...........] - ETA: 7:47 - loss: 3.3174\n",
      "6600/9797 [===================>..........] - ETA: 7:33 - loss: 3.3169\n",
      "6700/9797 [===================>..........] - ETA: 7:19 - loss: 3.3163\n",
      "6800/9797 [===================>..........] - ETA: 7:04 - loss: 3.3169\n",
      "6900/9797 [====================>.........] - ETA: 6:50 - loss: 3.3160\n",
      "7000/9797 [====================>.........] - ETA: 6:36 - loss: 3.3168\n",
      "7100/9797 [====================>.........] - ETA: 6:22 - loss: 3.3160\n",
      "7200/9797 [=====================>........] - ETA: 6:08 - loss: 3.3148\n",
      "7300/9797 [=====================>........] - ETA: 5:54 - loss: 3.3136\n",
      "7400/9797 [=====================>........] - ETA: 5:39 - loss: 3.3131\n",
      "7500/9797 [=====================>........] - ETA: 5:25 - loss: 3.3107\n",
      "7600/9797 [======================>.......] - ETA: 5:11 - loss: 3.3101\n",
      "7700/9797 [======================>.......] - ETA: 4:57 - loss: 3.3081\n",
      "7800/9797 [======================>.......] - ETA: 4:43 - loss: 3.3078\n",
      "7900/9797 [=======================>......] - ETA: 4:28 - loss: 3.3060\n",
      "8000/9797 [=======================>......] - ETA: 4:14 - loss: 3.3057\n",
      "8100/9797 [=======================>......] - ETA: 4:00 - loss: 3.3047\n",
      "8200/9797 [========================>.....] - ETA: 3:46 - loss: 3.3048\n",
      "8300/9797 [========================>.....] - ETA: 3:32 - loss: 3.3036\n",
      "8400/9797 [========================>.....] - ETA: 3:18 - loss: 3.3034\n",
      "8500/9797 [=========================>....] - ETA: 3:03 - loss: 3.3031\n",
      "8600/9797 [=========================>....] - ETA: 2:49 - loss: 3.3031\n",
      "8700/9797 [=========================>....] - ETA: 2:35 - loss: 3.3019\n",
      "8800/9797 [=========================>....] - ETA: 2:21 - loss: 3.3019\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 3.3021\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 3.3014\n",
      "9100/9797 [==========================>...] - ETA: 1:38 - loss: 3.3006\n",
      "9200/9797 [===========================>..] - ETA: 1:24 - loss: 3.3000\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 3.2994\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 3.2988\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 3.2985\n",
      "9600/9797 [============================>.] - ETA: 27s - loss: 3.2973\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 3.2975\n",
      "Epoch 00019: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_019.tlt\n",
      "coleoptera    AP    0.82949\n",
      "diptera       AP    0.86658\n",
      "geometridae   AP    0.90256\n",
      "hemiptera     AP    0.82588\n",
      "hymenoptera   AP    0.85229\n",
      "noctuidae     AP    0.9019\n",
      "odonata       AP    0.89631\n",
      "orthoptera    AP    0.88052\n",
      "trichoptera   AP    0.89953\n",
      "              mAP   0.87278\n",
      "Validation loss: 1.6135612700885733\n",
      "Epoch 20/20\n",
      " 100/9797 [..............................] - ETA: 22:38 - loss: 3.2802\n",
      " 200/9797 [..............................] - ETA: 22:29 - loss: 3.2320\n",
      " 300/9797 [..............................] - ETA: 22:17 - loss: 3.2306\n",
      " 400/9797 [>.............................] - ETA: 22:06 - loss: 3.2106\n",
      " 500/9797 [>.............................] - ETA: 21:54 - loss: 3.2297\n",
      " 600/9797 [>.............................] - ETA: 21:38 - loss: 3.2348\n",
      " 700/9797 [=>............................] - ETA: 21:24 - loss: 3.2253\n",
      " 800/9797 [=>............................] - ETA: 21:11 - loss: 3.2293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 900/9797 [=>............................] - ETA: 20:57 - loss: 3.2341\n",
      "1000/9797 [==>...........................] - ETA: 20:42 - loss: 3.2251\n",
      "1100/9797 [==>...........................] - ETA: 20:29 - loss: 3.2265\n",
      "1200/9797 [==>...........................] - ETA: 20:15 - loss: 3.2222\n",
      "1300/9797 [==>...........................] - ETA: 20:00 - loss: 3.2188\n",
      "1400/9797 [===>..........................] - ETA: 19:46 - loss: 3.2172\n",
      "1500/9797 [===>..........................] - ETA: 19:32 - loss: 3.2110\n",
      "1600/9797 [===>..........................] - ETA: 19:18 - loss: 3.2083\n",
      "1700/9797 [====>.........................] - ETA: 19:04 - loss: 3.2035\n",
      "1800/9797 [====>.........................] - ETA: 18:50 - loss: 3.2050\n",
      "1900/9797 [====>.........................] - ETA: 18:36 - loss: 3.2052\n",
      "2000/9797 [=====>........................] - ETA: 18:22 - loss: 3.1979\n",
      "2100/9797 [=====>........................] - ETA: 18:08 - loss: 3.1937\n",
      "2200/9797 [=====>........................] - ETA: 17:54 - loss: 3.1950\n",
      "2300/9797 [======>.......................] - ETA: 17:40 - loss: 3.1987\n",
      "2400/9797 [======>.......................] - ETA: 17:25 - loss: 3.1992\n",
      "2500/9797 [======>.......................] - ETA: 17:11 - loss: 3.2043\n",
      "2600/9797 [======>.......................] - ETA: 16:57 - loss: 3.2022\n",
      "2700/9797 [=======>......................] - ETA: 16:43 - loss: 3.2019\n",
      "2800/9797 [=======>......................] - ETA: 16:29 - loss: 3.2022\n",
      "2900/9797 [=======>......................] - ETA: 16:15 - loss: 3.2031\n",
      "3000/9797 [========>.....................] - ETA: 16:01 - loss: 3.2013\n",
      "3100/9797 [========>.....................] - ETA: 15:47 - loss: 3.2013\n",
      "3200/9797 [========>.....................] - ETA: 15:33 - loss: 3.2043\n",
      "3300/9797 [=========>....................] - ETA: 15:19 - loss: 3.2039\n",
      "3400/9797 [=========>....................] - ETA: 15:05 - loss: 3.2044\n",
      "3500/9797 [=========>....................] - ETA: 14:51 - loss: 3.2045\n",
      "3600/9797 [==========>...................] - ETA: 14:37 - loss: 3.2046\n",
      "3700/9797 [==========>...................] - ETA: 14:23 - loss: 3.2051\n",
      "3800/9797 [==========>...................] - ETA: 14:09 - loss: 3.2037\n",
      "3900/9797 [==========>...................] - ETA: 13:54 - loss: 3.2022\n",
      "4000/9797 [===========>..................] - ETA: 13:40 - loss: 3.2020\n",
      "4100/9797 [===========>..................] - ETA: 13:26 - loss: 3.2038\n",
      "4200/9797 [===========>..................] - ETA: 13:12 - loss: 3.2037\n",
      "4300/9797 [============>.................] - ETA: 12:58 - loss: 3.2046\n",
      "4400/9797 [============>.................] - ETA: 12:44 - loss: 3.2049\n",
      "4500/9797 [============>.................] - ETA: 12:30 - loss: 3.2038\n",
      "4600/9797 [=============>................] - ETA: 12:17 - loss: 3.2027\n",
      "4700/9797 [=============>................] - ETA: 12:04 - loss: 3.2025\n",
      "4800/9797 [=============>................] - ETA: 11:49 - loss: 3.2006\n",
      "4900/9797 [==============>...............] - ETA: 11:35 - loss: 3.2004\n",
      "5000/9797 [==============>...............] - ETA: 11:21 - loss: 3.1978\n",
      "5100/9797 [==============>...............] - ETA: 11:07 - loss: 3.1978\n",
      "5200/9797 [==============>...............] - ETA: 10:53 - loss: 3.2005\n",
      "5300/9797 [===============>..............] - ETA: 10:38 - loss: 3.2001\n",
      "5400/9797 [===============>..............] - ETA: 10:24 - loss: 3.1987\n",
      "5500/9797 [===============>..............] - ETA: 10:10 - loss: 3.1979\n",
      "5600/9797 [================>.............] - ETA: 9:56 - loss: 3.1965\n",
      "5700/9797 [================>.............] - ETA: 9:41 - loss: 3.1953\n",
      "5800/9797 [================>.............] - ETA: 9:27 - loss: 3.1962\n",
      "5900/9797 [=================>............] - ETA: 9:13 - loss: 3.1956\n",
      "6000/9797 [=================>............] - ETA: 8:59 - loss: 3.1945\n",
      "6100/9797 [=================>............] - ETA: 8:45 - loss: 3.1946\n",
      "6200/9797 [=================>............] - ETA: 8:31 - loss: 3.1931\n",
      "6300/9797 [==================>...........] - ETA: 8:16 - loss: 3.1931\n",
      "6400/9797 [==================>...........] - ETA: 8:02 - loss: 3.1922\n",
      "6500/9797 [==================>...........] - ETA: 7:48 - loss: 3.1918\n",
      "6600/9797 [===================>..........] - ETA: 7:34 - loss: 3.1909\n",
      "6700/9797 [===================>..........] - ETA: 7:19 - loss: 3.1913\n",
      "6800/9797 [===================>..........] - ETA: 7:05 - loss: 3.1906\n",
      "6900/9797 [====================>.........] - ETA: 6:51 - loss: 3.1890\n",
      "7000/9797 [====================>.........] - ETA: 6:37 - loss: 3.1894\n",
      "7100/9797 [====================>.........] - ETA: 6:23 - loss: 3.1894\n",
      "7200/9797 [=====================>........] - ETA: 6:08 - loss: 3.1886\n",
      "7300/9797 [=====================>........] - ETA: 5:54 - loss: 3.1875\n",
      "7400/9797 [=====================>........] - ETA: 5:40 - loss: 3.1873\n",
      "7500/9797 [=====================>........] - ETA: 5:26 - loss: 3.1871\n",
      "7600/9797 [======================>.......] - ETA: 5:12 - loss: 3.1856\n",
      "7700/9797 [======================>.......] - ETA: 4:57 - loss: 3.1858\n",
      "7800/9797 [======================>.......] - ETA: 4:43 - loss: 3.1861\n",
      "7900/9797 [=======================>......] - ETA: 4:29 - loss: 3.1862\n",
      "8000/9797 [=======================>......] - ETA: 4:15 - loss: 3.1852\n",
      "8100/9797 [=======================>......] - ETA: 4:01 - loss: 3.1850\n",
      "8200/9797 [========================>.....] - ETA: 3:46 - loss: 3.1846\n",
      "8300/9797 [========================>.....] - ETA: 3:32 - loss: 3.1837\n",
      "8400/9797 [========================>.....] - ETA: 3:18 - loss: 3.1830\n",
      "8500/9797 [=========================>....] - ETA: 3:04 - loss: 3.1825\n",
      "8600/9797 [=========================>....] - ETA: 2:49 - loss: 3.1820\n",
      "8700/9797 [=========================>....] - ETA: 2:35 - loss: 3.1805\n",
      "8800/9797 [=========================>....] - ETA: 2:21 - loss: 3.1810\n",
      "8900/9797 [==========================>...] - ETA: 2:07 - loss: 3.1807\n",
      "9000/9797 [==========================>...] - ETA: 1:53 - loss: 3.1800\n",
      "9100/9797 [==========================>...] - ETA: 1:38 - loss: 3.1798\n",
      "9200/9797 [===========================>..] - ETA: 1:24 - loss: 3.1792\n",
      "9300/9797 [===========================>..] - ETA: 1:10 - loss: 3.1779\n",
      "9400/9797 [===========================>..] - ETA: 56s - loss: 3.1780\n",
      "9500/9797 [============================>.] - ETA: 42s - loss: 3.1776\n",
      "9600/9797 [============================>.] - ETA: 27s - loss: 3.1774\n",
      "9700/9797 [============================>.] - ETA: 13s - loss: 3.1761\n",
      "Epoch 00020: saving model to /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet34_epoch_020.tlt\n",
      "coleoptera    AP    0.82905\n",
      "diptera       AP    0.85969\n",
      "geometridae   AP    0.90184\n",
      "hemiptera     AP    0.84377\n",
      "hymenoptera   AP    0.8552\n",
      "noctuidae     AP    0.90046\n",
      "odonata       AP    0.89624\n",
      "orthoptera    AP    0.86595\n",
      "trichoptera   AP    0.88239\n",
      "              mAP   0.87051\n",
      "Validation loss: 1.6676979792442517\n"
     ]
    }
   ],
   "source": [
    "print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n",
    "!yolo_v4 train -e $SPECS_DIR/yolo_v4_train_resnet34_kitti.txt \\\n",
    "               -r $DATA_DOWNLOAD_DIR/yolo_v4 \\\n",
    "               -k $KEY \\\n",
    "               --use_amp \\\n",
    "               --gpus 1 \\\n",
    "               2> /dev/null \\\n",
    "               | stdbuf -o0 -i0 grep -P \"mAP|Epoch|noctuidae|geometridae|coleoptera|diptera|odonata|orthoptera|hemiptera|hymenoptera|trichoptera|\\d{0,1}\\d00\\/9797|Validation|Class\" \\\n",
    "               | stdbuf -o0 -i0 tee -a $DATA_DOWNLOAD_DIR/$LOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To resume from checkpoint, please change pretrain_model_path to resume_model_path in config file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for each epoch:\n",
      "---------------------\n",
      "total 774M\n",
      "-rw-r--r-- 1 root root 232M Mar 30 09:35 yolov4_resnet18_epoch_010.tlt\n",
      "-rw-r--r-- 1 root root 232M Mar 30 10:13 yolov4_resnet18_epoch_020.tlt\n",
      "-rw-r--r-- 1 root root  78M Mar 30 11:09 yolov4_resnet18_epoch_025.tlt\n",
      "-rw-r--r-- 1 root root  78M Mar 30 11:26 yolov4_resnet18_epoch_030.tlt\n",
      "-rw-r--r-- 1 root root  78M Mar 30 11:45 yolov4_resnet18_epoch_035.tlt\n",
      "-rw-r--r-- 1 root root  78M Mar 30 12:04 yolov4_resnet18_epoch_040.tlt\n"
     ]
    }
   ],
   "source": [
    "print('Model for each epoch:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,AP_coleoptera,AP_diptera,AP_geometridae,AP_noctuidae,loss,lr,mAP,validation_loss\n",
      "1,0.010987527527321593,0.045566402077420236,0.002746770402670313,0.039775349000553156,61.804099213814624,8.425e-06,0.024769012251991324,17.05926891034477\n",
      "2,0.3850855776989521,0.445422088529778,0.3897902871937887,0.5192997974867524,19.06776086305317,1.6749998e-05,0.43489943772731776,4.485714787352609\n",
      "3,0.5723589421640828,0.7134163188290134,0.568712942423448,0.8625999564197654,12.823797789943276,2.5074998e-05,0.6792720399590774,2.4730444934016975\n",
      "4,0.7140295690283561,0.6305972139084953,0.6625141067083372,0.8440913058799258,11.332257657393313,3.3399996e-05,0.7128080488812787,2.060204979677225\n",
      "5,0.6721608037633762,0.7688224050762745,0.7707369635880601,0.8699115940159656,10.387087464218505,4.1724998e-05,0.7704079416109191,2.1629225449968676\n",
      "6,0.6851877249157674,0.7638337224721717,0.7235427236743687,0.8733834333103432,9.438171020124518,5.0049995e-05,0.7614869010931627,2.1832635448884594\n",
      "7,0.7586909724052815,0.8142351746163496,0.7350262467669074,0.8941135001387005,8.561347887778396,5.8374997e-05,0.8005164734818098,2.0804680351755107\n",
      "8,0.7066708177677117,0.7979312426368241,0.7565062391295249,0.8161244312460365,7.750368343335019,6.67e-05,0.7693081826950243,2.0700854621602995\n",
      "9,0.7519329026476574,0.7806798755469009,0.5437435419906299,0.8740013284792594,7.084673283499394,7.5025e-05,0.737589412166112,2.1616887816265673\n",
      "10,0.6362835484999488,0.8083987333115373,0.8167997057603638,0.8572082109874539,6.569308934827741,8.3349994e-05,0.779672549639826,2.2202919575800473\n",
      "env: EPOCH=010\n"
     ]
    }
   ],
   "source": [
    "# Now check the evaluation stats in the csv file and pick the model with highest eval accuracy.\n",
    "!cat $USER_EXPERIMENT_DIR/experiment_dir_unpruned/yolov4_training_log_resnet18.csv\n",
    "%set_env EPOCH=010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate trained models <a class=\"anchor\" id=\"head-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "2021-04-04 13:57:39,751 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2021-04-04 13:57:39,752 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2021-04-04 13:57:39,754 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2021-04-04 13:57:41,479 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2021-04-04 13:57:41,531 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2021-04-04 13:57:41,548 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "2021-04-04 13:57:42,066 [WARNING] tensorflow: From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "2021-04-04 13:57:42,238 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "2021-04-04 13:57:42,933 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2021-04-04 13:57:42,934 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2021-04-04 13:57:42,934 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2021-04-04 13:57:43,210 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "2021-04-04 13:57:43,210 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2021-04-04 13:57:43,546 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2021-04-04 13:57:43,962 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/losses/base_loss.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2021-04-04 13:57:44,012 [WARNING] tensorflow: From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/losses/base_loss.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "2021-04-04 13:57:45,897 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "2021-04-04 13:57:46,785 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Using TLT model for inference, setting batch size to the one in eval_config: 2\n",
      "Producing predictions: 100%|██████████████████| 581/581 [00:18<00:00, 31.65it/s]\n",
      "Start to calculate AP for each class\n",
      "*******************************\n",
      "coleoptera    AP    0.63602\n",
      "diptera       AP    0.80852\n",
      "geometridae   AP    0.81661\n",
      "noctuidae     AP    0.85721\n",
      "              mAP   0.77959\n",
      "*******************************\n"
     ]
    }
   ],
   "source": [
    "!yolo_v4 evaluate -e $SPECS_DIR/yolo_v4_train_resnet18_kitti.txt \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov4_resnet18_epoch_$EPOCH.tlt \\\n",
    "                  -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prune trained models <a class=\"anchor\" id=\"head-5\"></a>\n",
    "* Specify pre-trained model\n",
    "* Equalization criterion (`Only for resnets as they have element wise operations or MobileNets.`)\n",
    "* Threshold for pruning.\n",
    "* A key to save and load the model\n",
    "* Output directory to store the model\n",
    "\n",
    "Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold value depends on the dataset and the model. `0.5` in the block below is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $USER_EXPERIMENT_DIR/experiment_dir_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2021-03-30 13:19:04,222 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2021-03-30 13:19:04,273 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2021-03-30 13:19:04,283 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2021-03-30 13:19:04,291 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2021-03-30 13:19:04,295 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "2021-03-30 13:19:04,907 [WARNING] tensorflow: From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "2021-03-30 13:19:05,117 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "2021-03-30 13:19:05,934 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2021-03-30 13:19:05,934 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2021-03-30 13:19:05,934 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2021-03-30 13:19:06,215 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "2021-03-30 13:19:06,216 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2021-03-30 13:19:06,517 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2021-03-30 13:19:06,963 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/losses/base_loss.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2021-03-30 13:19:07,015 [WARNING] tensorflow: From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/losses/base_loss.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2021-03-30 13:19:07,076 [INFO] modulus.pruning.pruning: Exploring graph for retainable indices\n",
      "2021-03-30 13:19:09,557 [INFO] modulus.pruning.pruning: Pruning model and appending pruned nodes to new graph\n",
      "2021-03-30 13:20:07,498 [INFO] __main__: Pruning ratio (pruned model / original model): 0.8143495294792997\n"
     ]
    }
   ],
   "source": [
    "!yolo_v4 prune -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov4_resnet18_epoch_$EPOCH.tlt \\\n",
    "               -e $SPECS_DIR/yolo_v4_train_resnet18_kitti.txt \\\n",
    "               -o $USER_EXPERIMENT_DIR/experiment_dir_pruned/yolov4_resnet18_pruned.tlt \\\n",
    "               -eq intersection \\\n",
    "               -pth 0.01 \\\n",
    "               -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 64748\r\n",
      "-rw-r--r-- 1 root root 66298592 Mar 30 13:20 yolov4_resnet18_pruned.tlt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -rlt $USER_EXPERIMENT_DIR/experiment_dir_pruned/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrain pruned models <a class=\"anchor\" id=\"head-6\"></a>\n",
    "* Model needs to be re-trained to bring back accuracy after pruning\n",
    "* Specify re-training specification\n",
    "* WARNING: training will take several hours or one day to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed: 42\r\n",
      "yolov4_config {\r\n",
      "  big_anchor_shape: \"[(662.73, 374.53),(879.24, 481.50),(703.77, 663.88)]\"\r\n",
      "  mid_anchor_shape: \"[(272.97, 444.13), (392.70, 343.68), (442.03, 453.12)]\"\r\n",
      "  small_anchor_shape: \"[(156.85, 199.63), (194.99, 367.89),(297.06, 245.06)]\"\r\n",
      "  box_matching_iou: 0.25\r\n",
      "  arch: \"resnet\"\r\n",
      "  nlayers: 18\r\n",
      "  arch_conv_blocks: 2\r\n",
      "  loss_loc_weight: 0.8\r\n",
      "  loss_neg_obj_weights: 100.0\r\n",
      "  loss_class_weights: 0.5\r\n",
      "  label_smoothing: 0.0\r\n",
      "  big_grid_xy_extend: 0.05\r\n",
      "  mid_grid_xy_extend: 0.1\r\n",
      "  small_grid_xy_extend: 0.2\r\n",
      "  freeze_bn: false\r\n",
      "  #freeze_blocks: 0\r\n",
      "  force_relu: false\r\n",
      "}\r\n",
      "training_config {\r\n",
      "  batch_size_per_gpu: 2\r\n",
      "  num_epochs: 40\r\n",
      "  enable_qat: false\r\n",
      "  checkpoint_interval: 2\r\n",
      "  learning_rate {\r\n",
      "    soft_start_cosine_annealing_schedule {\r\n",
      "      min_learning_rate: 1e-7\r\n",
      "      max_learning_rate: 1e-4\r\n",
      "      soft_start: 0.3\r\n",
      "    }\r\n",
      "  }\r\n",
      "  regularizer {\r\n",
      "    type: NO_REG\r\n",
      "    weight: 3e-9\r\n",
      "  }\r\n",
      "  optimizer {\r\n",
      "    adam {\r\n",
      "      epsilon: 1e-7\r\n",
      "      beta1: 0.9\r\n",
      "      beta2: 0.999\r\n",
      "      amsgrad: false\r\n",
      "    }\r\n",
      "  }\r\n",
      "  pruned_model_path: \"/workspace/tlt-experiments/insect-thesis/experiment_dir_pruned/yolov4_resnet18_pruned.tlt\"\r\n",
      "}\r\n",
      "eval_config {\r\n",
      "  average_precision_mode: SAMPLE\r\n",
      "  batch_size: 8\r\n",
      "  matching_iou_threshold: 0.5\r\n",
      "}\r\n",
      "nms_config {\r\n",
      "  confidence_threshold: 0.001\r\n",
      "  clustering_iou_threshold: 0.5\r\n",
      "  top_k: 200\r\n",
      "}\r\n",
      "augmentation_config {\r\n",
      "  hue: 0.1\r\n",
      "  saturation: 1.5\r\n",
      "  exposure:1.5\r\n",
      "  vertical_flip:0\r\n",
      "  horizontal_flip: 0.5\r\n",
      "  jitter: 0.3\r\n",
      "  output_width: 1248\r\n",
      "  output_height: 384\r\n",
      "  randomize_input_shape_period: 0\r\n",
      "  mosaic_prob: 0.5\r\n",
      "  mosaic_min_ratio:0.2\r\n",
      "}\r\n",
      "dataset_config {\r\n",
      "  data_sources: {\r\n",
      "      label_directory_path: \"/workspace/tlt-experiments/insect-thesis/data/kitti/fusion/train/labels\"\r\n",
      "      image_directory_path: \"/workspace/tlt-experiments/insect-thesis/data/kitti/fusion/train/images\"\r\n",
      "  }\r\n",
      "  include_difficult_in_training: true\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"noctuidae\"\r\n",
      "      value: \"noctuidae\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"geometridae\"\r\n",
      "      value: \"geometridae\"\r\n",
      "  }\r\n",
      "  \r\n",
      "  validation_data_sources: {\r\n",
      "      label_directory_path: \"/workspace/tlt-experiments/insect-thesis/data/kitti/fusion/val/label\"\r\n",
      "      image_directory_path: \"/workspace/tlt-experiments/insect-thesis/data/kitti/fusion/val/image\"\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "# Printing the retrain spec file. \n",
    "# Here we have updated the spec file to include the newly pruned model as a pretrained weights.\n",
    "!sed -i 's,EXPERIMENT_DIR,'\"$USER_EXPERIMENT_DIR\"',' $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt\n",
    "!cat $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $USER_EXPERIMENT_DIR/experiment_dir_retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2021-03-30 13:20:32,360 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2021-03-30 13:20:32,360 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py:49: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2021-03-30 13:20:32,479 [WARNING] tensorflow: From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py:49: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2021-03-30 13:20:32,479 [WARNING] tensorflow: From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2021-03-30 13:20:33,219 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2021-03-30 13:20:33,257 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2021-03-30 13:20:33,273 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "2021-03-30 13:20:33,764 [WARNING] tensorflow: From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "2021-03-30 13:20:33,937 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "2021-03-30 13:20:34,615 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "2021-03-30 13:20:34,615 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2021-03-30 13:20:34,921 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2021-03-30 13:20:40,157 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/losses/base_loss.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2021-03-30 13:20:40,207 [WARNING] tensorflow: From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/losses/base_loss.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 3, 384, 1248) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 64, 192, 624) 9408        Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 64, 192, 624) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 192, 624) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_1 (Conv2D)        (None, 64, 96, 312)  36864       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_1 (BatchNormalizati (None, 64, 96, 312)  256         block_1a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_1 (Activation)    (None, 64, 96, 312)  0           block_1a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_2 (Conv2D)        (None, 64, 96, 312)  36864       block_1a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_shortcut (Conv2D) (None, 64, 96, 312)  4096        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_2 (BatchNormalizati (None, 64, 96, 312)  256         block_1a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_shortcut (BatchNorm (None, 64, 96, 312)  256         block_1a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 64, 96, 312)  0           block_1a_bn_2[0][0]              \n",
      "                                                                 block_1a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu (Activation)      (None, 64, 96, 312)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_1 (Conv2D)        (None, 64, 96, 312)  36864       block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_1 (BatchNormalizati (None, 64, 96, 312)  256         block_1b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_1 (Activation)    (None, 64, 96, 312)  0           block_1b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_2 (Conv2D)        (None, 64, 96, 312)  36864       block_1b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_shortcut (Conv2D) (None, 64, 96, 312)  4096        block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_2 (BatchNormalizati (None, 64, 96, 312)  256         block_1b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_shortcut (BatchNorm (None, 64, 96, 312)  256         block_1b_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 64, 96, 312)  0           block_1b_bn_2[0][0]              \n",
      "                                                                 block_1b_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu (Activation)      (None, 64, 96, 312)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_1 (Conv2D)        (None, 128, 48, 156) 73728       block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_1 (BatchNormalizati (None, 128, 48, 156) 512         block_2a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_1 (Activation)    (None, 128, 48, 156) 0           block_2a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_2 (Conv2D)        (None, 128, 48, 156) 147456      block_2a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_shortcut (Conv2D) (None, 128, 48, 156) 8192        block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_2 (BatchNormalizati (None, 128, 48, 156) 512         block_2a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_shortcut (BatchNorm (None, 128, 48, 156) 512         block_2a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 128, 48, 156) 0           block_2a_bn_2[0][0]              \n",
      "                                                                 block_2a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu (Activation)      (None, 128, 48, 156) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_1 (Conv2D)        (None, 128, 48, 156) 147456      block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_1 (BatchNormalizati (None, 128, 48, 156) 512         block_2b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_1 (Activation)    (None, 128, 48, 156) 0           block_2b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_2 (Conv2D)        (None, 128, 48, 156) 147456      block_2b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_shortcut (Conv2D) (None, 128, 48, 156) 16384       block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_2 (BatchNormalizati (None, 128, 48, 156) 512         block_2b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_shortcut (BatchNorm (None, 128, 48, 156) 512         block_2b_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 128, 48, 156) 0           block_2b_bn_2[0][0]              \n",
      "                                                                 block_2b_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu (Activation)      (None, 128, 48, 156) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_1 (Conv2D)        (None, 256, 24, 78)  294912      block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_1 (BatchNormalizati (None, 256, 24, 78)  1024        block_3a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_1 (Activation)    (None, 256, 24, 78)  0           block_3a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_2 (Conv2D)        (None, 256, 24, 78)  589824      block_3a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_shortcut (Conv2D) (None, 256, 24, 78)  32768       block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_2 (BatchNormalizati (None, 256, 24, 78)  1024        block_3a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_shortcut (BatchNorm (None, 256, 24, 78)  1024        block_3a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 256, 24, 78)  0           block_3a_bn_2[0][0]              \n",
      "                                                                 block_3a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu (Activation)      (None, 256, 24, 78)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_1 (Conv2D)        (None, 248, 24, 78)  571392      block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_1 (BatchNormalizati (None, 248, 24, 78)  992         block_3b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_1 (Activation)    (None, 248, 24, 78)  0           block_3b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_2 (Conv2D)        (None, 256, 24, 78)  571392      block_3b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_shortcut (Conv2D) (None, 256, 24, 78)  65536       block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_2 (BatchNormalizati (None, 256, 24, 78)  1024        block_3b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_shortcut (BatchNorm (None, 256, 24, 78)  1024        block_3b_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 256, 24, 78)  0           block_3b_bn_2[0][0]              \n",
      "                                                                 block_3b_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu (Activation)      (None, 256, 24, 78)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_1 (Conv2D)        (None, 408, 24, 78)  940032      block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_1 (BatchNormalizati (None, 408, 24, 78)  1632        block_4a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_1 (Activation)    (None, 408, 24, 78)  0           block_4a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_2 (Conv2D)        (None, 488, 24, 78)  1791936     block_4a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_shortcut (Conv2D) (None, 488, 24, 78)  124928      block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_2 (BatchNormalizati (None, 488, 24, 78)  1952        block_4a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_shortcut (BatchNorm (None, 488, 24, 78)  1952        block_4a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 488, 24, 78)  0           block_4a_bn_2[0][0]              \n",
      "                                                                 block_4a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu (Activation)      (None, 488, 24, 78)  0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_1 (Conv2D)        (None, 240, 24, 78)  1054080     block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_1 (BatchNormalizati (None, 240, 24, 78)  960         block_4b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_1 (Activation)    (None, 240, 24, 78)  0           block_4b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_2 (Conv2D)        (None, 512, 24, 78)  1105920     block_4b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_shortcut (Conv2D) (None, 512, 24, 78)  249856      block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_2 (BatchNormalizati (None, 512, 24, 78)  2048        block_4b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_shortcut (BatchNorm (None, 512, 24, 78)  2048        block_4b_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 512, 24, 78)  0           block_4b_bn_2[0][0]              \n",
      "                                                                 block_4b_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu (Activation)      (None, 512, 24, 78)  0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "yolo_spp_pool_1 (MaxPooling2D)  (None, 512, 24, 78)  0           block_4b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_spp_pool_2 (MaxPooling2D)  (None, 512, 24, 78)  0           block_4b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_spp_pool_3 (MaxPooling2D)  (None, 512, 24, 78)  0           block_4b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_spp_concat (Concatenate)   (None, 2048, 24, 78) 0           yolo_spp_pool_1[0][0]            \n",
      "                                                                 yolo_spp_pool_2[0][0]            \n",
      "                                                                 yolo_spp_pool_3[0][0]            \n",
      "                                                                 block_4b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_spp_conv (Conv2D)          (None, 512, 24, 78)  1048576     yolo_spp_concat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_spp_conv_bn (BatchNormaliz (None, 512, 24, 78)  2048        yolo_spp_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_spp_conv_lrelu (LeakyReLU) (None, 512, 24, 78)  0           yolo_spp_conv_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "yolo_expand_conv1 (Conv2D)      (None, 512, 12, 39)  2359296     yolo_spp_conv_lrelu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "yolo_expand_conv1_bn (BatchNorm (None, 512, 12, 39)  2048        yolo_expand_conv1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "yolo_expand_conv1_lrelu (LeakyR (None, 512, 12, 39)  0           yolo_expand_conv1_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_1 (Conv2D)           (None, 248, 12, 39)  126976      yolo_expand_conv1_lrelu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_1_bn (BatchNormaliza (None, 248, 12, 39)  992         yolo_conv1_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_1_lrelu (LeakyReLU)  (None, 248, 12, 39)  0           yolo_conv1_1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_2 (Conv2D)           (None, 512, 12, 39)  1142784     yolo_conv1_1_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_2_bn (BatchNormaliza (None, 512, 12, 39)  2048        yolo_conv1_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_2_lrelu (LeakyReLU)  (None, 512, 12, 39)  0           yolo_conv1_2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_3 (Conv2D)           (None, 256, 12, 39)  131072      yolo_conv1_2_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_3_bn (BatchNormaliza (None, 256, 12, 39)  1024        yolo_conv1_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_3_lrelu (LeakyReLU)  (None, 256, 12, 39)  0           yolo_conv1_3_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_4 (Conv2D)           (None, 512, 12, 39)  1179648     yolo_conv1_3_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_4_bn (BatchNormaliza (None, 512, 12, 39)  2048        yolo_conv1_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_4_lrelu (LeakyReLU)  (None, 512, 12, 39)  0           yolo_conv1_4_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_5 (Conv2D)           (None, 256, 12, 39)  131072      yolo_conv1_4_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_5_bn (BatchNormaliza (None, 256, 12, 39)  1024        yolo_conv1_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_5_lrelu (LeakyReLU)  (None, 256, 12, 39)  0           yolo_conv1_5_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv2 (Conv2D)             (None, 128, 12, 39)  32768       yolo_conv1_5_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv2_bn (BatchNormalizati (None, 128, 12, 39)  512         yolo_conv2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv2_lrelu (LeakyReLU)    (None, 128, 12, 39)  0           yolo_conv2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "upsample0 (UpSampling2D)        (None, 128, 24, 78)  0           yolo_conv2_lrelu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 384, 24, 78)  0           upsample0[0][0]                  \n",
      "                                                                 block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_1 (Conv2D)           (None, 128, 24, 78)  49152       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_1_bn (BatchNormaliza (None, 128, 24, 78)  512         yolo_conv3_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_1_lrelu (LeakyReLU)  (None, 128, 24, 78)  0           yolo_conv3_1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_2 (Conv2D)           (None, 256, 24, 78)  294912      yolo_conv3_1_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_2_bn (BatchNormaliza (None, 256, 24, 78)  1024        yolo_conv3_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_2_lrelu (LeakyReLU)  (None, 256, 24, 78)  0           yolo_conv3_2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_3 (Conv2D)           (None, 128, 24, 78)  32768       yolo_conv3_2_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_3_bn (BatchNormaliza (None, 128, 24, 78)  512         yolo_conv3_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_3_lrelu (LeakyReLU)  (None, 128, 24, 78)  0           yolo_conv3_3_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_4 (Conv2D)           (None, 256, 24, 78)  294912      yolo_conv3_3_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_4_bn (BatchNormaliza (None, 256, 24, 78)  1024        yolo_conv3_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_4_lrelu (LeakyReLU)  (None, 256, 24, 78)  0           yolo_conv3_4_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_5 (Conv2D)           (None, 128, 24, 78)  32768       yolo_conv3_4_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_5_bn (BatchNormaliza (None, 128, 24, 78)  512         yolo_conv3_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_5_lrelu (LeakyReLU)  (None, 128, 24, 78)  0           yolo_conv3_5_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv4 (Conv2D)             (None, 64, 24, 78)   8192        yolo_conv3_5_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv4_bn (BatchNormalizati (None, 64, 24, 78)   256         yolo_conv4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv4_lrelu (LeakyReLU)    (None, 64, 24, 78)   0           yolo_conv4_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "upsample1 (UpSampling2D)        (None, 64, 48, 156)  0           yolo_conv4_lrelu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 192, 48, 156) 0           upsample1[0][0]                  \n",
      "                                                                 block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_1 (Conv2D)           (None, 64, 48, 156)  12288       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_1_bn (BatchNormaliza (None, 64, 48, 156)  256         yolo_conv5_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_1_lrelu (LeakyReLU)  (None, 64, 48, 156)  0           yolo_conv5_1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_2 (Conv2D)           (None, 128, 48, 156) 73728       yolo_conv5_1_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_2_bn (BatchNormaliza (None, 128, 48, 156) 512         yolo_conv5_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_2_lrelu (LeakyReLU)  (None, 128, 48, 156) 0           yolo_conv5_2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_3 (Conv2D)           (None, 64, 48, 156)  8192        yolo_conv5_2_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_3_bn (BatchNormaliza (None, 64, 48, 156)  256         yolo_conv5_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_3_lrelu (LeakyReLU)  (None, 64, 48, 156)  0           yolo_conv5_3_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_4 (Conv2D)           (None, 128, 48, 156) 73728       yolo_conv5_3_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_4_bn (BatchNormaliza (None, 128, 48, 156) 512         yolo_conv5_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_4_lrelu (LeakyReLU)  (None, 128, 48, 156) 0           yolo_conv5_4_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_5 (Conv2D)           (None, 64, 48, 156)  8192        yolo_conv5_4_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_5_bn (BatchNormaliza (None, 64, 48, 156)  256         yolo_conv5_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_5_lrelu (LeakyReLU)  (None, 64, 48, 156)  0           yolo_conv5_5_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_6 (Conv2D)           (None, 392, 12, 39)  903168      yolo_conv1_5_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_6 (Conv2D)           (None, 248, 24, 78)  285696      yolo_conv3_5_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_6 (Conv2D)           (None, 128, 48, 156) 73728       yolo_conv5_5_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_6_bn (BatchNormaliza (None, 392, 12, 39)  1568        yolo_conv1_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_6_bn (BatchNormaliza (None, 248, 24, 78)  992         yolo_conv3_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_6_bn (BatchNormaliza (None, 128, 48, 156) 512         yolo_conv5_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv1_6_lrelu (LeakyReLU)  (None, 392, 12, 39)  0           yolo_conv1_6_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv3_6_lrelu (LeakyReLU)  (None, 248, 24, 78)  0           yolo_conv3_6_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv5_6_lrelu (LeakyReLU)  (None, 128, 48, 156) 0           yolo_conv5_6_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_big_object (Conv2D)        (None, 21, 12, 39)   8253        yolo_conv1_6_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_mid_object (Conv2D)        (None, 21, 24, 78)   5229        yolo_conv3_6_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_sm_object (Conv2D)         (None, 21, 48, 156)  2709        yolo_conv5_6_lrelu[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bg_permute (Permute)            (None, 12, 39, 21)   0           conv_big_object[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "md_permute (Permute)            (None, 24, 78, 21)   0           conv_mid_object[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sm_permute (Permute)            (None, 48, 156, 21)  0           conv_sm_object[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bg_reshape (Reshape)            (None, 1404, 7)      0           bg_permute[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "md_reshape (Reshape)            (None, 5616, 7)      0           md_permute[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sm_reshape (Reshape)            (None, 22464, 7)     0           sm_permute[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bg_anchor (YOLOAnchorBox)       (None, 1404, 6)      0           conv_big_object[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bg_bbox_processor (BBoxPostProc (None, 1404, 7)      0           bg_reshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "md_anchor (YOLOAnchorBox)       (None, 5616, 6)      0           conv_mid_object[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "md_bbox_processor (BBoxPostProc (None, 5616, 7)      0           md_reshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sm_anchor (YOLOAnchorBox)       (None, 22464, 6)     0           conv_sm_object[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sm_bbox_processor (BBoxPostProc (None, 22464, 7)     0           sm_reshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoded_bg (Concatenate)        (None, 1404, 13)     0           bg_anchor[0][0]                  \n",
      "                                                                 bg_bbox_processor[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "encoded_md (Concatenate)        (None, 5616, 13)     0           md_anchor[0][0]                  \n",
      "                                                                 md_bbox_processor[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "encoded_sm (Concatenate)        (None, 22464, 13)    0           sm_anchor[0][0]                  \n",
      "                                                                 sm_bbox_processor[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "encoded_detections (Concatenate (None, 29484, 13)    0           encoded_bg[0][0]                 \n",
      "                                                                 encoded_md[0][0]                 \n",
      "                                                                 encoded_sm[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 16,460,127\n",
      "Trainable params: 16,439,119\n",
      "Non-trainable params: 21,008\n",
      "__________________________________________________________________________________________________\n",
      "2021-03-30 13:20:41,289 [INFO] __main__: Number of images in the training dataset:\t  6088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "2021-03-30 13:20:42,534 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "2021-03-30 13:20:43,433 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/40\n",
      "   2/3044 [..............................] - ETA: 5:44:19 - loss: 4.0661/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.264549). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "3044/3044 [==============================] - 281s 92ms/step - loss: 2.9767\n",
      "Epoch 2/40\n",
      "3044/3044 [==============================] - 262s 86ms/step - loss: 2.4707\n",
      "\n",
      "Epoch 00002: saving model to /workspace/tlt-experiments/insect-thesis/experiment_dir_retrain/weights/yolov4_resnet18_epoch_002.tlt\n",
      "Producing predictions: 100%|████████████████████| 85/85 [00:16<00:00,  5.27it/s]\n",
      "Start to calculate AP for each class\n",
      "*******************************\n",
      "geometridae   AP    0.46627\n",
      "noctuidae     AP    0.89798\n",
      "              mAP   0.68212\n",
      "*******************************\n",
      "Validation loss: 1.9684693192589213\n",
      "Epoch 3/40\n",
      "3044/3044 [==============================] - 262s 86ms/step - loss: 2.4233\n",
      "Epoch 4/40\n",
      "3044/3044 [==============================] - 258s 85ms/step - loss: 2.4601\n",
      "\n",
      "Epoch 00004: saving model to /workspace/tlt-experiments/insect-thesis/experiment_dir_retrain/weights/yolov4_resnet18_epoch_004.tlt\n",
      "Producing predictions: 100%|████████████████████| 85/85 [00:16<00:00,  5.01it/s]\n",
      "Start to calculate AP for each class\n",
      "*******************************\n",
      "geometridae   AP    0.42227\n",
      "noctuidae     AP    0.90291\n",
      "              mAP   0.66259\n",
      "*******************************\n",
      "Validation loss: 2.0010027130679973\n",
      "Epoch 5/40\n",
      "3044/3044 [==============================] - 255s 84ms/step - loss: 2.4719\n",
      "Epoch 6/40\n",
      "3044/3044 [==============================] - 254s 83ms/step - loss: 2.4961\n",
      "\n",
      "Epoch 00006: saving model to /workspace/tlt-experiments/insect-thesis/experiment_dir_retrain/weights/yolov4_resnet18_epoch_006.tlt\n",
      "Producing predictions: 100%|████████████████████| 85/85 [00:15<00:00,  5.39it/s]\n",
      "Start to calculate AP for each class\n",
      "*******************************\n",
      "geometridae   AP    0.45812\n",
      "noctuidae     AP    0.89504\n",
      "              mAP   0.67658\n",
      "*******************************\n",
      "Validation loss: 2.141530870685916\n",
      "Epoch 7/40\n",
      "3044/3044 [==============================] - 254s 83ms/step - loss: 2.5212\n",
      "Epoch 8/40\n",
      "3044/3044 [==============================] - 255s 84ms/step - loss: 2.5362\n",
      "\n",
      "Epoch 00008: saving model to /workspace/tlt-experiments/insect-thesis/experiment_dir_retrain/weights/yolov4_resnet18_epoch_008.tlt\n",
      "Producing predictions: 100%|████████████████████| 85/85 [00:12<00:00,  6.59it/s]\n",
      "Start to calculate AP for each class\n",
      "*******************************\n",
      "geometridae   AP    0.38901\n",
      "noctuidae     AP    0.89203\n",
      "              mAP   0.64052\n",
      "*******************************\n",
      "Validation loss: 2.184854455248139\n",
      "Epoch 9/40\n",
      "3044/3044 [==============================] - 254s 83ms/step - loss: 2.5304\n",
      "Epoch 10/40\n",
      "3044/3044 [==============================] - 255s 84ms/step - loss: 2.5665\n",
      "\n",
      "Epoch 00010: saving model to /workspace/tlt-experiments/insect-thesis/experiment_dir_retrain/weights/yolov4_resnet18_epoch_010.tlt\n",
      "Producing predictions: 100%|████████████████████| 85/85 [00:12<00:00,  6.65it/s]\n",
      "Start to calculate AP for each class\n",
      "*******************************\n",
      "geometridae   AP    0.45436\n",
      "noctuidae     AP    0.85554\n",
      "              mAP   0.65495\n",
      "*******************************\n",
      "Validation loss: 2.1330735739871596\n",
      "Epoch 11/40\n",
      "3044/3044 [==============================] - 255s 84ms/step - loss: 2.5707\n",
      "Epoch 12/40\n",
      "3044/3044 [==============================] - 256s 84ms/step - loss: 2.5944\n",
      "\n",
      "Epoch 00012: saving model to /workspace/tlt-experiments/insect-thesis/experiment_dir_retrain/weights/yolov4_resnet18_epoch_012.tlt\n",
      "Producing predictions: 100%|████████████████████| 85/85 [00:14<00:00,  5.95it/s]\n",
      "Start to calculate AP for each class\n",
      "*******************************\n",
      "geometridae   AP    0.4447\n",
      "noctuidae     AP    0.8972\n",
      "              mAP   0.67095\n",
      "*******************************\n",
      "Validation loss: 2.1755771792146583\n",
      "Epoch 13/40\n",
      "3044/3044 [==============================] - 255s 84ms/step - loss: 2.5719\n",
      "Epoch 14/40\n",
      "1479/3044 [=============>................] - ETA: 2:13 - loss: 2.5791^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py\", line 209, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/yolo_v4\", line 8, in <module>\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py\", line 205, in main\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py\", line 162, in run_experiment\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1418, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 217, in fit_generator\n",
      "    class_weight=class_weight)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1217, in train_on_batch\n",
      "    outputs = self.train_function(ins)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2697, in __call__\n",
      "    if hasattr(get_session(), '_make_callable_from_options'):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 193, in get_session\n",
      "    if not getattr(v, '_keras_initialized', False):\n",
      "KeyboardInterrupt\n",
      "    sys.exit(main())\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/entrypoint/yolo_v4.py\", line 12, in main\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/entrypoint/entrypoint.py\", line 294, in launch_job\n",
      "  File \"/usr/lib/python3.6/subprocess.py\", line 289, in call\n",
      "    return p.wait(timeout=timeout)\n",
      "  File \"/usr/lib/python3.6/subprocess.py\", line 1477, in wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/usr/lib/python3.6/subprocess.py\", line 1424, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Retraining using the pruned model as pretrained weights \n",
    "!yolo_v4 train --gpus 1 \\\n",
    "               --use_amp \\\n",
    "               -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "               -r $USER_EXPERIMENT_DIR/experiment_dir_retrain \\\n",
    "               -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 475080\r\n",
      "-rw-r--r-- 1 root root 162160480 Mar 30 12:35 yolov4_resnet18_epoch_005.tlt\r\n",
      "-rw-r--r-- 1 root root 162160480 Mar 30 12:56 yolov4_resnet18_epoch_010.tlt\r\n",
      "-rw-r--r-- 1 root root 162160480 Mar 30 13:17 yolov4_resnet18_epoch_015.tlt\r\n"
     ]
    }
   ],
   "source": [
    "# Listing the newly retrained model.\n",
    "!ls -rlt $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,AP_geometridae,AP_noctuidae,loss,lr,mAP,validation_loss\n",
      "1,nan,nan,2.9699207752864716,8.425e-06,nan,nan\n",
      "2,nan,nan,2.4719353376174253,1.6749998e-05,nan,nan\n",
      "3,nan,nan,2.426941933303719,2.5074998e-05,nan,nan\n",
      "4,nan,nan,2.4175226155193656,3.3399996e-05,nan,nan\n",
      "5,0.40039567749065,0.9050360334235674,2.4868202339294863,4.1724998e-05,0.6527158554571088,2.13381304120171\n",
      "6,nan,nan,2.506702417906731,5.0049995e-05,nan,nan\n",
      "7,nan,nan,2.5245346796261967,5.8374997e-05,nan,nan\n",
      "8,nan,nan,2.5163331839908283,6.67e-05,nan,nan\n",
      "9,nan,nan,2.5604243331176377,7.5025e-05,nan,nan\n",
      "10,0.4346957101610327,0.8968245366205689,2.5762420302739124,8.3349994e-05,0.6657601233908008,2.211998022519625\n",
      "11,nan,nan,2.5954854175468,9.167499e-05,nan,nan\n",
      "12,nan,nan,2.624354499829113,9.999999e-05,nan,nan\n",
      "13,nan,nan,2.5883319928517636,9.984602e-05,nan,nan\n",
      "14,nan,nan,2.5596985588444987,9.938503e-05,nan,nan\n",
      "15,0.4190668131475363,0.8742922661585135,2.5287796151230744,9.861987e-05,0.6466795396530249,2.247077383233245\n",
      "env: EPOCH=040\n"
     ]
    }
   ],
   "source": [
    "# Now check the evaluation stats in the csv file and pick the model with highest eval accuracy.\n",
    "!cat $USER_EXPERIMENT_DIR/experiment_dir_retrain/yolov4_training_log_resnet18.csv\n",
    "%set_env EPOCH=040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate retrained model <a class=\"anchor\" id=\"head-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "2021-03-30 13:18:20,302 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2021-03-30 13:18:20,303 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2021-03-30 13:18:20,305 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/evaluate.py\", line 200, in <module>\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/evaluate.py\", line 196, in main\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/evaluate.py\", line 115, in evaluate\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/utils/model_io.py\", line 60, in load_model\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/workspace/tlt-experiments/insect-thesis/experiment_dir_retrain/weights/yolov4_resnet18_epoch_040.tlt'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/yolo_v4\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/entrypoint/yolo_v4.py\", line 12, in main\n",
      "  File \"/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/entrypoint/entrypoint.py\", line 296, in launch_job\n",
      "AssertionError: Process run failed.\n"
     ]
    }
   ],
   "source": [
    "!yolo_v4 evaluate -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_resnet18_epoch_$EPOCH.tlt \\\n",
    "                  -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize inferences <a class=\"anchor\" id=\"head-8\"></a>\n",
    "In this section, we run the tlt-infer tool to generate inferences on the trained models and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy some test images\n",
    "#!mkdir -p /workspace/tlt-experiments/insect-thesis/yolo_v4/test_samples\n",
    "#!cp $DATA_DOWNLOAD_DIR/kitti/fusion2/inference/images/* /workspace/tlt-experiments/insect-thesis/yolo_v4/test_samples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove previous inference results\n",
    "!rm /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/inference_*/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "2021-05-15 17:35:22,475 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2021-05-15 17:35:22,475 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2021-05-15 17:35:22,477 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2021-05-15 17:35:24,953 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2021-05-15 17:35:24,973 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2021-05-15 17:35:24,988 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "2021-05-15 17:35:25,513 [WARNING] tensorflow: From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "2021-05-15 17:35:25,697 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "2021-05-15 17:35:26,406 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2021-05-15 17:35:26,406 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2021-05-15 17:35:26,406 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2021-05-15 17:35:26,693 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "2021-05-15 17:35:26,693 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2021-05-15 17:35:27,063 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2021-05-15 17:35:27,527 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/losses/base_loss.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2021-05-15 17:35:27,580 [WARNING] tensorflow: From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/losses/base_loss.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "2021-05-15 17:35:29,576 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "2021-05-15 17:35:30,488 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Using TLT model for inference, setting batch size to the one in eval_config: 1\n",
      "100%|███████████████████████████████████████| 4354/4354 [01:46<00:00, 40.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Running inference for detection on n images\n",
    "!yolo_v4 inference -i /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/val/images \\\n",
    "                   -o /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/inference_images \\\n",
    "                   -e $SPECS_DIR/yolo_v4_train_resnet18_kitti.txt \\\n",
    "                   -m /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/weights/yolov4_resnet18_epoch_020.tlt \\\n",
    "                   -l /workspace/tlt-experiments/insect-thesis/data/kitti/final-test/yolo_v4/inference_labels \\\n",
    "                   -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tlt-infer` tool produces two outputs. \n",
    "1. Overlain images in `$USER_EXPERIMENT_DIR/yolo_infer_images`\n",
    "2. Frame by frame bbox labels in kitti format located in `$USER_EXPERIMENT_DIR/yolo_infer_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['USER_EXPERIMENT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "OUTPUT_PATH = 'yolo_infer_images' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 3 # number of columns in the visualizer grid.\n",
    "IMAGES = 9 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8bis. KITTI TO YOLO for metrics <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4354/4354 [00:13<00:00, 314.98it/s]\n"
     ]
    }
   ],
   "source": [
    "!rm ../data/kitti/final-test/val/labels_yolo/*\n",
    "!python3 ../scripts/KITTI_to_YOLO.py validation ../data/kitti/final-test/val ../data/kitti/final-test/val/labels_yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '../data/kitti/final-test/yolo_v4/inference_labels_yolo/*': No such file or directory\n",
      "100%|██████████████████████████████████████| 4354/4354 [00:09<00:00, 447.84it/s]\n"
     ]
    }
   ],
   "source": [
    "!rm ../data/kitti/final-test/yolo_v4/inference_labels_yolo/*\n",
    "!python3 ../scripts/KITTI_to_YOLO.py inference ../data/kitti/final-test/yolo_v4 ../data/kitti/final-test/yolo_v4/inference_labels_yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod -R a+rw /workspace/tlt-experiments/insect-thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../data/kitti/final-test/yolo_v4/stats-18-cmd/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "AP: 93.93% (0)\n",
      "AP: 90.17% (1)\n",
      "AP: 86.40% (2)\n",
      "AP: 81.50% (3)\n",
      "AP: 92.39% (4)\n",
      "AP: 89.97% (5)\n",
      "AP: 85.39% (6)\n",
      "AP: 84.00% (7)\n",
      "AP: 92.40% (8)\n",
      "mAP: 88.46%\n"
     ]
    }
   ],
   "source": [
    "!python3 /workspace/tlt-experiments/Object-Detection-Metrics/pascalvoc.py \\\n",
    "        -gt ../insect-thesis/data/kitti/final-test/val/labels_yolo/ \\\n",
    "        -det ../insect-thesis/data/kitti/final-test/yolo_v4/inference_labels_yolo/ \\\n",
    "        -sp ../insect-thesis/data/kitti/final-test/yolo_v4/stats-18-cmd\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deploy! <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained a non-QAT model, you may export in FP32, FP16 or INT8 mode using the code block below. For INT8, you need to provide calibration image directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tlt-export will fail if .etlt already exists. So we clear the export folder before tlt-export\n",
    "!rm -rf $USER_EXPERIMENT_DIR/export\n",
    "!mkdir -p $USER_EXPERIMENT_DIR/export\n",
    "# Export in FP32 mode. Change --data_type to fp16 for FP16 mode\n",
    "!yolo_v4 export -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_resnet18_epoch_$EPOCH.tlt \\\n",
    "                -k $KEY \\\n",
    "                -o $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt \\\n",
    "                -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "                --batch_size 16 \\\n",
    "                --data_type fp32\n",
    "\n",
    "# Uncomment to export in INT8 mode (generate calibration cache file). \n",
    "# !yolo_v4 export -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_resnet18_epoch_$EPOCH.tlt  \\\n",
    "#                 -o $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt \\\n",
    "#                 -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "#                 -k $KEY \\\n",
    "#                 --cal_image_dir  $USER_EXPERIMENT_DIR/data/testing/image_2 \\\n",
    "#                 --data_type int8 \\\n",
    "#                 --batch_size 16 \\\n",
    "#                 --batches 10 \\\n",
    "#                 --cal_cache_file $USER_EXPERIMENT_DIR/export/cal.bin  \\\n",
    "#                 --cal_data_file $USER_EXPERIMENT_DIR/export/cal.tensorfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` In this example, for ease of execution we restrict the number of calibrating batches to 10. TLT recommends the use of at least 10% of the training dataset for int8 calibration.\n",
    "\n",
    "If you train a QAT model, you may only export in INT8 mode using following code block. This generates an etlt file and the corresponding calibration cache. You can throw away the calibration cache and just use the etlt file in tlt-converter or DeepStream for FP32 or FP16 mode. But please note this gives sub-optimal results. If you want to deploy in FP32 or FP16, you should disable QAT in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to export QAT model in INT8 mode (generate calibration cache file).\n",
    "# !rm -rf $USER_EXPERIMENT_DIR/export\n",
    "# !mkdir -p $USER_EXPERIMENT_DIR/export\n",
    "# !yolo_v4 export -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_resnet18_epoch_$EPOCH.tlt  \\\n",
    "#                 -o $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt \\\n",
    "#                 -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "#                 -k $KEY \\\n",
    "#                 --data_type int8 \\\n",
    "#                 --cal_cache_file $USER_EXPERIMENT_DIR/export/cal.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exported model:')\n",
    "print('------------')\n",
    "!ls -lh $USER_EXPERIMENT_DIR/export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify engine generation using the `tlt-converter` utility included with the docker.\n",
    "\n",
    "The `tlt-converter` produces optimized tensorrt engines for the platform that it resides on. Therefore, to get maximum performance, please instantiate this docker and execute the `tlt-converter` command, with the exported `.etlt` file and calibration cache (for int8 mode) on your target device. The converter utility included in this docker only works for x86 devices, with discrete NVIDIA GPU's. \n",
    "\n",
    "For the jetson devices, please download the converter for jetson from the dev zone link [here](https://developer.nvidia.com/tlt-converter). \n",
    "\n",
    "If you choose to integrate your model into deepstream directly, you may do so by simply copying the exported `.etlt` file along with the calibration cache to the target device and updating the spec file that configures the `gst-nvinfer` element to point to this newly exported model. Usually this file is called `config_infer_primary.txt` for detection models and `config_infer_secondary_*.txt` for classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorRT engine (FP32)\n",
    "!tlt-converter -k $KEY \\\n",
    "               -d 3,384,1248 \\\n",
    "               -o BatchedNMS \\\n",
    "               -e $USER_EXPERIMENT_DIR/export/trt.engine \\\n",
    "               -m 16 \\\n",
    "               -t fp32 \\\n",
    "               -i nchw \\\n",
    "               $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt\n",
    "\n",
    "# Convert to TensorRT engine (FP16)\n",
    "# !tlt-converter -k $KEY \\\n",
    "#                -d 3,384,1248 \\\n",
    "#                -o BatchedNMS \\\n",
    "#                -e $USER_EXPERIMENT_DIR/export/trt.engine \\\n",
    "#                -m 16 \\\n",
    "#                -t fp16 \\\n",
    "#                -i nchw \\\n",
    "#                $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt\n",
    "\n",
    "# Convert to TensorRT engine (INT8).\n",
    "# !tlt-converter -k $KEY  \\\n",
    "#                -d 3,384,1248 \\\n",
    "#                -o BatchedNMS \\\n",
    "#                -c $USER_EXPERIMENT_DIR/export/cal.bin \\\n",
    "#                -e $USER_EXPERIMENT_DIR/export/trt.engine \\\n",
    "#                -b 8 \\\n",
    "#                -m 16 \\\n",
    "#                -t int8 \\\n",
    "#                -i nchw \\\n",
    "#                $USER_EXPERIMENT_DIR/export/yolov4_resnet18_epoch_$EPOCH.etlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exported engine:')\n",
    "print('------------')\n",
    "!ls -lh $USER_EXPERIMENT_DIR/export/trt.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify the deployed model <a class=\"anchor\" id=\"head-10\"></a>\n",
    "Verify the converted engine by visualizing TensorRT inferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer using TensorRT engine\n",
    "\n",
    "!yolo_v4 inference -m $USER_EXPERIMENT_DIR/export/trt.engine \\\n",
    "                   -e $SPECS_DIR/yolo_v4_retrain_resnet18_kitti.txt \\\n",
    "                   -i /workspace/examples/yolo_v4/test_samples \\\n",
    "                   -o $USER_EXPERIMENT_DIR/yolo_infer_images \\\n",
    "                   -t 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "OUTPUT_PATH = 'yolo_infer_images' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 3 # number of columns in the visualizer grid.\n",
    "IMAGES = 9 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
